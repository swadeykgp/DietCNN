{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b23d7e",
   "metadata": {},
   "source": [
    "## Section 1: First we try PyTorch Quantization.\n",
    "### This is the best available option as we will see, in terms of accuracy preservation.\n",
    "### However, the FBGEMM backend is not available of FPGA and other embedded systems. In the next section we will implement Quantization from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89927c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:06:53.669544Z",
     "start_time": "2022-11-03T08:06:53.665983Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import warnings\n",
    "from vgg_sym import *\n",
    "from vgg_symq import *\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed3f864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:07:22.240398Z",
     "start_time": "2022-11-03T08:07:21.554918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "bs= 32 \n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../../formal_pruning/dataset', train=True, download=False, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=False)\n",
    "testset = torchvision.datasets.CIFAR10(root='../../../formal_pruning/dataset', train=False, download=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False)\n",
    "num_classes = 10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647be2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:07:43.974565Z",
     "start_time": "2022-11-03T08:07:43.731014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x1600 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some images\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "figure = plt.figure(figsize = (24,16))\n",
    "num_of_images = 20\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].permute(1,2,0).numpy().squeeze().astype('uint8'), cmap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c00a200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:08:24.136042Z",
     "start_time": "2022-11-03T08:08:24.082545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGG Model - Defined in vgg_sym.py \n",
    "pretrained_model = \"./cifar_vgg_sym_v3.pt\"\n",
    "net = VGG('VGG11')\n",
    "sd = torch.load(pretrained_model, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(sd['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050f46df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:09:03.796355Z",
     "start_time": "2022-11-03T08:09:03.793669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for getting the model size\n",
    "def print_size_of_model(model):\n",
    "    \"\"\" Print the size of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: model whose size needs to be determined\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size of the model(MB):', round(os.path.getsize('temp.p')/(1024*1024),3))\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c11abcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:09:06.001312Z",
     "start_time": "2022-11-03T08:09:05.969960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model(MB): 35.198\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeff1556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:09:21.423601Z",
     "start_time": "2022-11-03T08:09:21.411458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main accuracy testing function\n",
    "\n",
    "def test(model, device, test_loader, train_loader, batch_size, quantize=False, fbgemm=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Testing with qauntization if quantize=True\n",
    "    if quantize:\n",
    "#         modules_to_fuse = [['conv1', 'bn1'],\n",
    "#                    ['layer1.0.conv1', 'layer1.0.bn1'],\n",
    "#                    ['layer1.0.conv2', 'layer1.0.bn2'],\n",
    "#                    ['layer1.1.conv1', 'layer1.1.bn1'],\n",
    "#                    ['layer1.1.conv2', 'layer1.1.bn2'],\n",
    "#                    ['layer2.0.conv1', 'layer2.0.bn1'],\n",
    "#                    ['layer2.0.conv2', 'layer2.0.bn2'],\n",
    "#                    ['layer2.0.downsample.0', 'layer2.0.downsample.1'],\n",
    "#                    ['layer2.1.conv1', 'layer2.1.bn1'],\n",
    "#                    ['layer2.1.conv2', 'layer2.1.bn2'],\n",
    "#                    ['layer3.0.conv1', 'layer3.0.bn1'],\n",
    "#                    ['layer3.0.conv2', 'layer3.0.bn2'],\n",
    "#                    ['layer3.0.downsample.0', 'layer3.0.downsample.1'],\n",
    "#                    ['layer3.1.conv1', 'layer3.1.bn1'],\n",
    "#                    ['layer3.1.conv2', 'layer3.1.bn2'],\n",
    "#                    ['layer4.0.conv1', 'layer4.0.bn1'],\n",
    "#                    ['layer4.0.conv2', 'layer4.0.bn2'],\n",
    "#                    ['layer4.0.downsample.0', 'layer4.0.downsample.1'],\n",
    "#                    ['layer4.1.conv1', 'layer4.1.bn1'],\n",
    "#                    ['layer4.1.conv2', 'layer4.1.bn2']]\n",
    "#         model = torch.quantization.fuse_modules(model, modules_to_fuse)\n",
    "        if fbgemm:\n",
    "            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        else:\n",
    "            model.qconfig = torch.quantization.default_qconfig\n",
    "        torch.quantization.prepare(model, inplace=True)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in train_loader:\n",
    "                model.forward(data)\n",
    "            torch.quantization.convert(model, inplace=True)\n",
    "            print(\"======= Quantization Done =====\")\n",
    "\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        st = time.time()\n",
    "        for data in test_loader:\n",
    "            X, y = data\n",
    "            #st = time.time()\n",
    "            output = model.forward(X)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                #if True:\n",
    "                    correct += 1\n",
    "            total += batch_size\n",
    "        et = time.time()    \n",
    "    acc = round(correct/total, 4)\n",
    "    print(\"========================================= PERFORMANCE =============================================\")\n",
    "    print_size_of_model(model)\n",
    "    print(\"PyTorch optimized model test accuracy :{}% \".format(100*round(correct/total, 4)))\n",
    "    print('Elapsed time = {:0.4f} milliseconds'.format((et - st) * 1000))\n",
    "    print(\"====================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a03eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:09:43.334424Z",
     "start_time": "2022-11-03T08:09:27.569750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 35.198\n",
      "PyTorch optimized model test accuracy :90.19% \n",
      "Elapsed time = 15731.9725 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Baseline performance - unquantized model\n",
    "device = 'cpu'\n",
    "test(model=net, device=device, test_loader=testloader, train_loader=trainloader, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14390483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:13:46.973499Z",
     "start_time": "2022-11-03T08:10:21.490134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Quantization Done =====\n",
      "VGGQ(\n",
      "  (features): Sequential(\n",
      "    (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.303924024105072, zero_point=64)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.3796601891517639, zero_point=61)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.24186919629573822, zero_point=66)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.200422003865242, zero_point=54)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.12747766077518463, zero_point=44)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.06178085505962372, zero_point=53)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07090853154659271, zero_point=42)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1341523677110672, zero_point=61)\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=512, out_features=10, scale=0.5953040719032288, zero_point=31, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 8.817\n",
      "PyTorch optimized model test accuracy :90.0% \n",
      "Elapsed time = 12707.7856 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Quantization Performance\n",
    "# Load the model to be quantized with Pytorch Quantization\n",
    "netq = VGGQ('VGG11Q')\n",
    "sdq = torch.load(pretrained_model, map_location=torch.device('cpu'))\n",
    "netq.load_state_dict(sdq['net'])\n",
    "device = 'cpu'\n",
    "test(model=netq, device=device, test_loader=testloader, train_loader=trainloader, batch_size=bs, quantize=True)\n",
    "# This performs the quantized inference. This is PyTorch static quantization. We only did activations quantization\n",
    "# and performed the calibration required post training. The ACC drop is 0.19% only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17468889",
   "metadata": {},
   "source": [
    "## Section 2: From scratch post training quantization - with calibration  \n",
    "### As the FBGEMM backend is not available of FPGA and other embedded systems, we implement a simple quantization from scratch. We will take this to FPGA in the FPGA implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f604ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:14:53.863198Z",
     "start_time": "2022-11-03T08:14:53.794749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Routines for post training quantization - with calibration from scratch    \n",
    "    \n",
    "# Simple implementation for FPGA\n",
    "# Routines for Quantization    \n",
    "\n",
    "# Routines for Quantization    \n",
    "    \n",
    "from collections import namedtuple\n",
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "nb = 8\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=nb):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale\n",
    "  \n",
    "  zero_point = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point = qmax\n",
    "  else:\n",
    "      zero_point = initial_zero_point\n",
    "\n",
    "  zero_point = int(zero_point)\n",
    "\n",
    "  return scale, zero_point\n",
    "\n",
    "def quantize_tensor(x, num_bits=nb, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    q_x = q_x.round().byte()\n",
    "    \n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=nb):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale_next = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale_next\n",
    "  \n",
    "  zero_point_next = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point_next = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point_next = qmax\n",
    "  else:\n",
    "      zero_point_next = initial_zero_point\n",
    "\n",
    "  zero_point_next = int(zero_point_next)\n",
    "\n",
    "  return scale_next, zero_point_next\n",
    "  \n",
    "def quantizeLayer(x, layer, stat, scale_x, zp_x, num_bits=nb):\n",
    "  # for both conv and linear layers\n",
    "  W = layer.weight.data\n",
    "  B = layer.bias.data\n",
    "\n",
    "  # scale_x = x.scale\n",
    "  # zp_x = x.zero_point\n",
    "  w = quantize_tensor(layer.weight.data,num_bits) \n",
    "  b = quantize_tensor(layer.bias.data,num_bits)\n",
    "\n",
    "  layer.weight.data = w.tensor.float()\n",
    "  layer.bias.data = b.tensor.float()\n",
    "\n",
    "  ####################################################################\n",
    "  # This is Quantisation !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "  scale_w = w.scale\n",
    "  zp_w = w.zero_point\n",
    "  \n",
    "  scale_b = b.scale\n",
    "  zp_b = b.zero_point\n",
    "  \n",
    "\n",
    "  scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
    "\n",
    "  # Perparing input by shifting\n",
    "  X = x.float() - zp_x\n",
    "  layer.weight.data = (scale_x * scale_w/scale_next)*(layer.weight.data - zp_w)\n",
    "  layer.bias.data = (scale_b/scale_next)*(layer.bias.data + zp_b)\n",
    "\n",
    "  # All int\n",
    "\n",
    "  #x = (layer(X)/ scale_next) + zero_point_next \n",
    "  x = layer(X) + zero_point_next   \n",
    "  \n",
    "  # Perform relu too\n",
    "  x = F.relu(x)\n",
    "    \n",
    "    \n",
    "  # Reset\n",
    "  layer.weight.data = W\n",
    "  layer.bias.data = B\n",
    "  \n",
    "  return x, scale_next, zero_point_next\n",
    "\n",
    "\n",
    "def quantForward(model, x, stats):\n",
    "  #print(x.shape)\n",
    "  # Quantise before inputting into incoming layers\n",
    "  x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'])\n",
    "  #print(model.features[0].weight.data.shape)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.features[0], stats['conv2'], x.scale, x.zero_point)\n",
    "  #x = model.features[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[2], stats['conv3'], scale_next, zero_point_next)\n",
    "  #x = model.features[3](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[4], stats['conv4'], scale_next, zero_point_next)\n",
    "  #x = model.features[5](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[6], stats['conv5'], scale_next, zero_point_next)\n",
    "  #x = model.features[7](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[8], stats['conv6'], scale_next, zero_point_next)\n",
    "  #x = model.features[9](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[10], stats['conv7'], scale_next, zero_point_next)\n",
    "  #x = model.features[11](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[12], stats['conv8'], scale_next, zero_point_next)\n",
    "  #x = model.features[13](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[14], stats['fc'], scale_next, zero_point_next)\n",
    "  #x = model.features[15](x)\n",
    "    \n",
    "  \n",
    "  #x = x.view(x.size(0), -1)  \n",
    "  x = x.view(-1, 512)   \n",
    "  \n",
    "  \n",
    "  # Back to dequant for final layer\n",
    "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "   \n",
    "  x = model.classifier(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "# Get Min and max of x tensor, and stores it\n",
    "def updateStats(x, stats, key):\n",
    "  max_val, _ = torch.max(x, dim=1)\n",
    "  min_val, _ = torch.min(x, dim=1)\n",
    "  \n",
    "  \n",
    "  if key not in stats:\n",
    "    stats[key] = {\"max\": max_val.sum(), \"min\": min_val.sum(), \"total\": 1}\n",
    "  else:\n",
    "    stats[key]['max'] += max_val.sum().item()\n",
    "    stats[key]['min'] += min_val.sum().item()\n",
    "    stats[key]['total'] += 1\n",
    "  \n",
    "  return stats\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "  x = model.features[1](model.features[0](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "  x =  model.features[3](model.features[2](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "  x = model.features[5](model.features[4](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "  x = model.features[7](model.features[6](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "  x = model.features[9](model.features[8](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "  x = model.features[11](model.features[10](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "  x = model.features[13](model.features[12](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "  x = model.features[15](model.features[14](x))\n",
    "\n",
    "  #x = x.view(x.size(0), -1)  \n",
    "  x = x.view(-1, 512) \n",
    "  \n",
    "  stats = updateStats(x, stats, 'fc')\n",
    "\n",
    "  x = model.classifier(x)\n",
    "\n",
    "  return stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    stats = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            stats = gatherActivationStats(model, data, stats)\n",
    "    \n",
    "    final_stats = {}\n",
    "    for key, value in stats.items():\n",
    "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"] }\n",
    "    return final_stats\n",
    "\n",
    "# Routines for performance testing\n",
    "\n",
    "def test_quant_scr(model, device, test_loader, train_loader, batch_size, quantize=False, fbgemm=False, stats=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        st = time.time()\n",
    "        for data in test_loader:\n",
    "            X, y = data\n",
    "            #st = time.time()\n",
    "            # Testing with qauntization if quantize=True\n",
    "            if quantize:\n",
    "                output = quantForward(model, X, stats)\n",
    "            else:    \n",
    "                output = model.forward(X)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                #if True:\n",
    "                    correct += 1\n",
    "            total += batch_size\n",
    "        et = time.time()    \n",
    "    acc = round(correct/total, 4)\n",
    "    print(\"========================================= PERFORMANCE =============================================\")\n",
    "    print_size_of_model(model)\n",
    "    print(\"PyTorch optimized model test accuracy :{}% \".format(100*round(correct/total, 2)))\n",
    "    print('Elapsed time = {:0.4f} milliseconds'.format((et - st) * 1000))\n",
    "    print(\"====================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2881a422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:19:07.030618Z",
     "start_time": "2022-11-03T08:16:02.561834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv1': {'max': tensor(75.4283), 'min': tensor(-77.5850)}, 'conv2': {'max': tensor(296.1228), 'min': tensor(0.)}, 'conv3': {'max': tensor(306.3843), 'min': tensor(0.)}, 'conv4': {'max': tensor(236.8686), 'min': tensor(0.)}, 'conv5': {'max': tensor(207.8151), 'min': tensor(0.)}, 'conv6': {'max': tensor(125.4137), 'min': tensor(0.)}, 'conv7': {'max': tensor(49.3179), 'min': tensor(0.)}, 'conv8': {'max': tensor(67.7475), 'min': tensor(0.)}, 'fc': {'max': tensor(91.8696), 'min': tensor(3.8462e-10)}}\n"
     ]
    }
   ],
   "source": [
    "# Quantized model performance\n",
    "# Copy from the original model for Q\n",
    "import copy\n",
    "netqq = copy.deepcopy(net)\n",
    "\n",
    "# one time stats gathering - we will keep this stored for MNIST FPGA implementation\n",
    "# This is doing the required calibration for finding the min, max of the activations\n",
    "stats = gatherStats(netqq, trainloader)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c3b44e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:20:40.293832Z",
     "start_time": "2022-11-03T08:19:56.051235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 35.198\n",
      "PyTorch optimized model test accuracy :18.0% \n",
      "Elapsed time = 44151.8731 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Quantized Inference with the normal model\n",
    "test_quant_scr(model=netqq, device=device, test_loader=testloader, train_loader=trainloader, batch_size=bs, quantize=True, stats=stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f88c4",
   "metadata": {},
   "source": [
    "### What is this! Bad performance after Quantization compared to PyTorch Post training quantization with calibration. \n",
    "### We will try out Quantization Aware Re-Training now to recover the accuracy. then we take this model to FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e15e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:26:02.090433Z",
     "start_time": "2022-11-03T08:26:02.053388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some more definitions required for the inference on the QAT model that we have trained offline\n",
    "\n",
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale\n",
    "  \n",
    "  zero_point = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point = qmax\n",
    "  else:\n",
    "      zero_point = initial_zero_point\n",
    "\n",
    "  zero_point = int(zero_point)\n",
    "\n",
    "  return scale, zero_point\n",
    "\n",
    "def calcScaleZeroPointSym(min_val, max_val,num_bits=8):\n",
    "  \n",
    "  # Calc Scale \n",
    "  max_val = max(abs(min_val), abs(max_val))\n",
    "  qmin = 0.\n",
    "  qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "  scale = max_val / qmax\n",
    "\n",
    "  return scale, 0\n",
    "\n",
    "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    q_x = q_x.round().byte()\n",
    "    \n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "\n",
    "def quantize_tensor_sym(x, num_bits=8, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    max_val = max(abs(min_val), abs(max_val))\n",
    "    qmin = 0.\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "    scale = max_val / qmax   \n",
    "\n",
    "    q_x = x/scale\n",
    "\n",
    "    q_x.clamp_(-qmax, qmax).round_()\n",
    "    q_x = q_x.round()\n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=0)\n",
    "\n",
    "def dequantize_tensor_sym(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float())\n",
    "\n",
    "def quantizeLayer(x, layer, stat, scale_x, zp_x, vis=False, axs=None, X=None, y=None, sym=False, num_bits=8):\n",
    "  # for both conv and linear layers\n",
    "\n",
    "  # cache old values\n",
    "  W = layer.weight.data\n",
    "  B = layer.bias.data\n",
    "\n",
    "  # WEIGHTS SIMULATED QUANTISED\n",
    "\n",
    "  # quantise weights, activations are already quantised\n",
    "  if sym:\n",
    "    w = quantize_tensor_sym(layer.weight.data,num_bits=num_bits) \n",
    "    b = quantize_tensor_sym(layer.bias.data,num_bits=num_bits)\n",
    "  else:\n",
    "    w = quantize_tensor(layer.weight.data, num_bits=num_bits) \n",
    "    b = quantize_tensor(layer.bias.data, num_bits=num_bits)\n",
    "\n",
    "  layer.weight.data = w.tensor.float()\n",
    "  layer.bias.data = b.tensor.float()\n",
    "\n",
    "  ## END WEIGHTS QUANTISED SIMULATION\n",
    "\n",
    "\n",
    "  if vis:\n",
    "    axs[X,y].set_xlabel(\"Visualising weights of layer: \")\n",
    "    visualise(layer.weight.data, axs[X,y])\n",
    "\n",
    "  # QUANTISED OP, USES SCALE AND ZERO POINT TO DO LAYER FORWARD PASS. (How does backprop change here ?)\n",
    "  # This is Quantisation Arithmetic\n",
    "  scale_w = w.scale\n",
    "  zp_w = w.zero_point\n",
    "  scale_b = b.scale\n",
    "  zp_b = b.zero_point\n",
    "  \n",
    "  if sym:\n",
    "    scale_next, zero_point_next = calcScaleZeroPointSym(min_val=stat['min'], max_val=stat['max'])\n",
    "  else:\n",
    "    scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
    "\n",
    "  # Preparing input by saturating range to num_bits range.\n",
    "  if sym:\n",
    "    X = x.float()\n",
    "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data)\n",
    "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data)\n",
    "  else:\n",
    "    X = x.float() - zp_x\n",
    "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data - zp_w)\n",
    "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data + zp_b)\n",
    "\n",
    "  # All int computation\n",
    "  if sym:  \n",
    "    x = (layer(X)) \n",
    "  else:\n",
    "    x = (layer(X)) + zero_point_next \n",
    "  \n",
    "  # cast to int\n",
    "  x.round_()\n",
    "\n",
    "  # Perform relu too\n",
    "  x = F.relu(x)\n",
    "\n",
    "  # Reset weights for next forward pass\n",
    "  layer.weight.data = W\n",
    "  layer.bias.data = B\n",
    "  \n",
    "  return x, scale_next, zero_point_next\n",
    "\n",
    "# Get Min and max of x tensor, and stores it\n",
    "def updateStats(x, stats, key):\n",
    "  max_val, _ = torch.max(x, dim=1)\n",
    "  min_val, _ = torch.min(x, dim=1)\n",
    "\n",
    "  # add ema calculation\n",
    "\n",
    "  if key not in stats:\n",
    "    stats[key] = {'max': max_val.sum(), 'min': min_val.sum(), 'total': 1}\n",
    "  else:\n",
    "    stats[key]['max'] += max_val.sum().item()\n",
    "    stats[key]['min'] += min_val.sum().item()\n",
    "    if 'total' in stats[key]:\n",
    "        stats[key]['total'] += 1\n",
    "    else:\n",
    "        stats[key]['total'] = 1\n",
    "  \n",
    "  weighting = 2.0 / (stats[key]['total']) + 1\n",
    "\n",
    "  if 'ema_min' in stats[key]:\n",
    "    stats[key]['ema_min'] = weighting*(min_val.mean().item()) + (1- weighting) * stats[key]['ema_min']\n",
    "  else:\n",
    "    stats[key]['ema_min'] = weighting*(min_val.mean().item())\n",
    "\n",
    "  if 'ema_max' in stats[key]:\n",
    "    stats[key]['ema_max'] = weighting*(max_val.mean().item()) + (1- weighting) * stats[key]['ema_max']\n",
    "  else: \n",
    "    stats[key]['ema_max'] = weighting*(max_val.mean().item())\n",
    "\n",
    "  stats[key]['min_val'] = stats[key]['min']/ stats[key]['total']\n",
    "  stats[key]['max_val'] = stats[key]['max']/ stats[key]['total']\n",
    "  \n",
    "  return stats\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "  x = model.features[1](model.features[0](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "  x =  model.features[3](model.features[2](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "  x = model.features[5](model.features[4](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "  x = model.features[7](model.features[6](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "  x = model.features[9](model.features[8](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "  x = model.features[11](model.features[10](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "  x = model.features[13](model.features[12](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "  x = model.features[15](model.features[14](x))\n",
    "\n",
    "  #x = x.view(x.size(0), -1)  \n",
    "  x = x.view(-1, 512) \n",
    "  \n",
    "  stats = updateStats(x, stats, 'fc')\n",
    "\n",
    "  x = model.classifier(x)\n",
    "\n",
    "  return stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader):\n",
    "    device = 'cpu'\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    stats = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            stats = gatherActivationStats(model, data, stats)\n",
    "    \n",
    "    final_stats = {}\n",
    "    for key, value in stats.items():\n",
    "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "    return final_stats\n",
    "\n",
    "def quantForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8):\n",
    "  X = 0\n",
    "  y = 0\n",
    "  # Quantise before inputting into incoming layers\n",
    "  if sym:\n",
    "    x = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "  else:\n",
    "    x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "\n",
    "    # Quantise before inputting into incoming layers\n",
    "  if sym:\n",
    "    x = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=nb)\n",
    "  else:\n",
    "    x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=nb)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.features[0], stats['conv2'], x.scale, x.zero_point)\n",
    "  #x = model.features[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[2], stats['conv3'], scale_next, zero_point_next)\n",
    "  #x = model.features[3](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[4], stats['conv4'], scale_next, zero_point_next)\n",
    "  #x = model.features[5](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[6], stats['conv5'], scale_next, zero_point_next)\n",
    "  #x = model.features[7](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[8], stats['conv6'], scale_next, zero_point_next)\n",
    "  #x = model.features[9](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[10], stats['conv7'], scale_next, zero_point_next)\n",
    "  #x = model.features[11](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[12], stats['conv8'], scale_next, zero_point_next)\n",
    "  #x = model.features[13](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[14], stats['fc'], scale_next, zero_point_next)\n",
    "  #x = model.features[15](x)\n",
    "    \n",
    "  \n",
    "  #x = x.view(x.size(0), -1)  \n",
    "  x = x.view(-1, 512)   \n",
    "  \n",
    "  \n",
    "  # Back to dequant for final layer\n",
    "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "   \n",
    "  x = model.classifier(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "import torch\n",
    "\n",
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, num_bits=8, min_val=None, max_val=None):\n",
    "        x = quantize_tensor(x,num_bits=num_bits, min_val=min_val, max_val=max_val)\n",
    "        x = dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None\n",
    "\n",
    "def quantAwareTrainingForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8, act_quant=False):\n",
    "\n",
    "  #print(x.shape)\n",
    "  #print(model.features[0].weight.data.shape) \n",
    "  #x = model.features[0](x)\n",
    "  #x = model.features[1](x)\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "  #x =  model.features[3](model.features[2](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "  #x = model.features[5](model.features[4](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "  #x = model.features[7](model.features[6](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "  #x = model.features[9](model.features[8](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "  #x = model.features[11](model.features[10](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "  #x = model.features[13](model.features[12](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "  #x = model.features[15](model.features[14](x))\n",
    "  ##x = x.view(x.size(0), -1)  \n",
    "  #x = x.view(-1, 512) \n",
    "  #stats = updateStats(x, stats, 'fc')\n",
    "  #x = model.classifier(x)\n",
    "\n",
    "  conv1weight = model.features[0].weight.data\n",
    "  model.features[0].weight.data = FakeQuantOp.apply(model.features[0].weight.data, num_bits)\n",
    "  x = model.features[1](model.features[0](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv1']['ema_min'], stats['conv1']['ema_max'])\n",
    "\n",
    "  conv2weight = model.features[2].weight.data\n",
    "  model.features[2].weight.data = FakeQuantOp.apply(model.features[2].weight.data, num_bits)\n",
    "  x = model.features[3](model.features[2](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv2']['ema_min'], stats['conv2']['ema_max'])\n",
    "\n",
    "  conv3weight = model.features[4].weight.data\n",
    "  model.features[4].weight.data = FakeQuantOp.apply(model.features[4].weight.data, num_bits)\n",
    "  x = model.features[5](model.features[4](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv3']['ema_min'], stats['conv3']['ema_max'])\n",
    "\n",
    "\n",
    "  conv4weight = model.features[6].weight.data\n",
    "  model.features[6].weight.data = FakeQuantOp.apply(model.features[6].weight.data, num_bits)\n",
    "  x = model.features[7](model.features[6](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv4']['ema_min'], stats['conv4']['ema_max'])\n",
    "\n",
    "\n",
    "  conv5weight = model.features[8].weight.data\n",
    "  model.features[8].weight.data = FakeQuantOp.apply(model.features[8].weight.data, num_bits)\n",
    "  x = model.features[9](model.features[8](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv5']['ema_min'], stats['conv5']['ema_max'])\n",
    "\n",
    "\n",
    "\n",
    "  conv6weight = model.features[10].weight.data\n",
    "  model.features[10].weight.data = FakeQuantOp.apply(model.features[10].weight.data, num_bits)\n",
    "  x = model.features[11](model.features[10](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv6']['ema_min'], stats['conv6']['ema_max'])\n",
    "\n",
    "\n",
    "  conv7weight = model.features[12].weight.data\n",
    "  model.features[12].weight.data = FakeQuantOp.apply(model.features[12].weight.data, num_bits)\n",
    "  x = model.features[13](model.features[12](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv7']['ema_min'], stats['conv7']['ema_max'])\n",
    "\n",
    "\n",
    "  conv8weight = model.features[14].weight.data\n",
    "  model.features[14].weight.data = FakeQuantOp.apply(model.features[14].weight.data, num_bits)\n",
    "  x = model.features[15](model.features[14](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv8']['ema_min'], stats['conv8']['ema_max'])\n",
    "\n",
    "  x = x.view(-1, 512) \n",
    "  x = model.classifier(x)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc')\n",
    "\n",
    "\n",
    "  return x, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, stats\n",
    "\n",
    "# Training\n",
    "def train(epoch, trainloader, optimizer, criterion, model, device, stats, act_quant=False, num_bits=8):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #outputs = net(inputs)\n",
    "        outputs, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, stats = quantAwareTrainingForward(model, inputs, stats, num_bits=num_bits, act_quant=act_quant)\n",
    "        model.features[0].weight.data   = conv1weight\n",
    "        model.features[2].weight.data   = conv2weight\n",
    "        model.features[4].weight.data   = conv3weight\n",
    "        model.features[6].weight.data   = conv4weight\n",
    "        model.features[8].weight.data   = conv5weight\n",
    "        model.features[10].weight.data  = conv6weight\n",
    "        model.features[12].weight.data  = conv7weight\n",
    "        model.features[14].weight.data  = conv8weight\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test_qat(epoch, testloader, criterion, model, device, stats, act_quant, num_bits=8):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #outputs = net(inputs)\n",
    "            outputs, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, stats = quantAwareTrainingForward(model, inputs, stats, num_bits=num_bits, act_quant=act_quant)\n",
    "            model.features[0].weight.data   = conv1weight\n",
    "            model.features[2].weight.data   = conv2weight\n",
    "            model.features[4].weight.data   = conv3weight\n",
    "            model.features[6].weight.data   = conv4weight\n",
    "            model.features[8].weight.data   = conv5weight\n",
    "            model.features[10].weight.data  = conv6weight\n",
    "            model.features[12].weight.data  = conv7weight\n",
    "            model.features[14].weight.data  = conv8weight\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96f8a205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:45:49.567080Z",
     "start_time": "2022-11-03T08:42:45.063198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv1': {'max': tensor(75.4060), 'min': tensor(-77.5925), 'ema_min': -2.429084955641284, 'ema_max': 2.2644749349894213}, 'conv2': {'max': tensor(289.6924), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 9.745374106466938}, 'conv3': {'max': tensor(311.6935), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 10.426215821130976}, 'conv4': {'max': tensor(234.2506), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 7.733187223847345}, 'conv5': {'max': tensor(221.5462), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 7.1292019954629575}, 'conv6': {'max': tensor(112.8982), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 3.626955604523778}, 'conv7': {'max': tensor(56.8567), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 1.961748730051182}, 'conv8': {'max': tensor(65.4364), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 2.0627339223770456}, 'fc': {'max': tensor(93.6161), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 2.7617917017409255}}\n"
     ]
    }
   ],
   "source": [
    "# now we test the QAT trained model's inference accuracy\n",
    "pretrained_modelqat = \"./cifar_qat.pt\"\n",
    "netqat = VGG('VGG11')\n",
    "sdqat = torch.load(pretrained_modelqat, map_location=torch.device('cpu'))\n",
    "netqat.load_state_dict(sdqat['net'])\n",
    "stats = gatherStats(netqat, trainloader)\n",
    "print(stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d90e8088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:37:30.288233Z",
     "start_time": "2022-11-03T08:37:09.762901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 313 Loss: 2.311 | Acc: 6.250% (2/32)\n",
      "1 313 Loss: 6.531 | Acc: 9.375% (6/64)\n",
      "2 313 Loss: 5.122 | Acc: 10.417% (10/96)\n",
      "3 313 Loss: 3.932 | Acc: 29.688% (38/128)\n",
      "4 313 Loss: 3.608 | Acc: 24.375% (39/160)\n",
      "5 313 Loss: 3.094 | Acc: 34.896% (67/192)\n",
      "6 313 Loss: 2.688 | Acc: 43.304% (97/224)\n",
      "7 313 Loss: 2.416 | Acc: 49.609% (127/256)\n",
      "8 313 Loss: 2.273 | Acc: 54.167% (156/288)\n",
      "9 313 Loss: 2.085 | Acc: 58.125% (186/320)\n",
      "10 313 Loss: 1.895 | Acc: 61.932% (218/352)\n",
      "11 313 Loss: 1.758 | Acc: 64.323% (247/384)\n",
      "12 313 Loss: 1.662 | Acc: 66.827% (278/416)\n",
      "13 313 Loss: 1.597 | Acc: 68.527% (307/448)\n",
      "14 313 Loss: 1.550 | Acc: 70.000% (336/480)\n",
      "15 313 Loss: 1.461 | Acc: 71.680% (367/512)\n",
      "16 313 Loss: 1.395 | Acc: 72.978% (397/544)\n",
      "17 313 Loss: 1.351 | Acc: 73.785% (425/576)\n",
      "18 313 Loss: 1.291 | Acc: 75.000% (456/608)\n",
      "19 313 Loss: 1.227 | Acc: 76.250% (488/640)\n",
      "20 313 Loss: 1.189 | Acc: 76.786% (516/672)\n",
      "21 313 Loss: 1.174 | Acc: 77.415% (545/704)\n",
      "22 313 Loss: 1.153 | Acc: 77.853% (573/736)\n",
      "23 313 Loss: 1.131 | Acc: 78.125% (600/768)\n",
      "24 313 Loss: 1.119 | Acc: 78.500% (628/800)\n",
      "25 313 Loss: 1.097 | Acc: 78.966% (657/832)\n",
      "26 313 Loss: 1.064 | Acc: 79.514% (687/864)\n",
      "27 313 Loss: 1.064 | Acc: 79.911% (716/896)\n",
      "28 313 Loss: 1.043 | Acc: 80.172% (744/928)\n",
      "29 313 Loss: 1.031 | Acc: 80.417% (772/960)\n",
      "30 313 Loss: 1.016 | Acc: 80.847% (802/992)\n",
      "31 313 Loss: 1.010 | Acc: 80.859% (828/1024)\n",
      "32 313 Loss: 1.004 | Acc: 81.061% (856/1056)\n",
      "33 313 Loss: 1.016 | Acc: 81.342% (885/1088)\n",
      "34 313 Loss: 0.996 | Acc: 81.607% (914/1120)\n",
      "35 313 Loss: 1.017 | Acc: 81.424% (938/1152)\n",
      "36 313 Loss: 0.999 | Acc: 81.672% (967/1184)\n",
      "37 313 Loss: 0.979 | Acc: 82.072% (998/1216)\n",
      "38 313 Loss: 0.973 | Acc: 82.212% (1026/1248)\n",
      "39 313 Loss: 0.961 | Acc: 82.500% (1056/1280)\n",
      "40 313 Loss: 0.943 | Acc: 82.698% (1085/1312)\n",
      "41 313 Loss: 0.935 | Acc: 82.887% (1114/1344)\n",
      "42 313 Loss: 0.928 | Acc: 83.067% (1143/1376)\n",
      "43 313 Loss: 0.922 | Acc: 83.310% (1173/1408)\n",
      "44 313 Loss: 0.902 | Acc: 83.681% (1205/1440)\n",
      "45 313 Loss: 0.914 | Acc: 83.832% (1234/1472)\n",
      "46 313 Loss: 0.911 | Acc: 83.710% (1259/1504)\n",
      "47 313 Loss: 0.912 | Acc: 83.919% (1289/1536)\n",
      "48 313 Loss: 0.900 | Acc: 84.120% (1319/1568)\n",
      "49 313 Loss: 0.896 | Acc: 84.188% (1347/1600)\n",
      "50 313 Loss: 0.885 | Acc: 84.314% (1376/1632)\n",
      "51 313 Loss: 0.872 | Acc: 84.495% (1406/1664)\n",
      "52 313 Loss: 0.868 | Acc: 84.611% (1435/1696)\n",
      "53 313 Loss: 0.856 | Acc: 84.780% (1465/1728)\n",
      "54 313 Loss: 0.850 | Acc: 84.773% (1492/1760)\n",
      "55 313 Loss: 0.861 | Acc: 84.766% (1519/1792)\n",
      "56 313 Loss: 0.859 | Acc: 84.814% (1547/1824)\n",
      "57 313 Loss: 0.852 | Acc: 84.914% (1576/1856)\n",
      "58 313 Loss: 0.851 | Acc: 84.958% (1604/1888)\n",
      "59 313 Loss: 0.841 | Acc: 85.104% (1634/1920)\n",
      "60 313 Loss: 0.853 | Acc: 84.990% (1659/1952)\n",
      "61 313 Loss: 0.860 | Acc: 85.030% (1687/1984)\n",
      "62 313 Loss: 0.854 | Acc: 85.169% (1717/2016)\n",
      "63 313 Loss: 0.852 | Acc: 85.205% (1745/2048)\n",
      "64 313 Loss: 0.845 | Acc: 85.288% (1774/2080)\n",
      "65 313 Loss: 0.840 | Acc: 85.417% (1804/2112)\n",
      "66 313 Loss: 0.844 | Acc: 85.494% (1833/2144)\n",
      "67 313 Loss: 0.837 | Acc: 85.570% (1862/2176)\n",
      "68 313 Loss: 0.827 | Acc: 85.688% (1892/2208)\n",
      "69 313 Loss: 0.824 | Acc: 85.714% (1920/2240)\n",
      "70 313 Loss: 0.828 | Acc: 85.695% (1947/2272)\n",
      "71 313 Loss: 0.830 | Acc: 85.764% (1976/2304)\n",
      "72 313 Loss: 0.826 | Acc: 85.745% (2003/2336)\n",
      "73 313 Loss: 0.830 | Acc: 85.769% (2031/2368)\n",
      "74 313 Loss: 0.819 | Acc: 85.917% (2062/2400)\n",
      "75 313 Loss: 0.823 | Acc: 86.020% (2092/2432)\n",
      "76 313 Loss: 0.822 | Acc: 85.998% (2119/2464)\n",
      "77 313 Loss: 0.829 | Acc: 85.978% (2146/2496)\n",
      "78 313 Loss: 0.830 | Acc: 86.036% (2175/2528)\n",
      "79 313 Loss: 0.836 | Acc: 86.094% (2204/2560)\n",
      "80 313 Loss: 0.829 | Acc: 86.188% (2234/2592)\n",
      "81 313 Loss: 0.832 | Acc: 86.242% (2263/2624)\n",
      "82 313 Loss: 0.831 | Acc: 86.258% (2291/2656)\n",
      "83 313 Loss: 0.821 | Acc: 86.421% (2323/2688)\n",
      "84 313 Loss: 0.814 | Acc: 86.507% (2353/2720)\n",
      "85 313 Loss: 0.809 | Acc: 86.555% (2382/2752)\n",
      "86 313 Loss: 0.804 | Acc: 86.638% (2412/2784)\n",
      "87 313 Loss: 0.806 | Acc: 86.648% (2440/2816)\n",
      "88 313 Loss: 0.809 | Acc: 86.657% (2468/2848)\n",
      "89 313 Loss: 0.802 | Acc: 86.771% (2499/2880)\n",
      "90 313 Loss: 0.796 | Acc: 86.848% (2529/2912)\n",
      "91 313 Loss: 0.794 | Acc: 86.889% (2558/2944)\n",
      "92 313 Loss: 0.795 | Acc: 86.895% (2586/2976)\n",
      "93 313 Loss: 0.793 | Acc: 86.935% (2615/3008)\n",
      "94 313 Loss: 0.793 | Acc: 86.941% (2643/3040)\n",
      "95 313 Loss: 0.793 | Acc: 86.947% (2671/3072)\n",
      "96 313 Loss: 0.796 | Acc: 86.952% (2699/3104)\n",
      "97 313 Loss: 0.792 | Acc: 86.990% (2728/3136)\n",
      "98 313 Loss: 0.793 | Acc: 86.995% (2756/3168)\n",
      "99 313 Loss: 0.791 | Acc: 87.031% (2785/3200)\n",
      "100 313 Loss: 0.787 | Acc: 87.036% (2813/3232)\n",
      "101 313 Loss: 0.782 | Acc: 87.102% (2843/3264)\n",
      "102 313 Loss: 0.781 | Acc: 87.136% (2872/3296)\n",
      "103 313 Loss: 0.778 | Acc: 87.169% (2901/3328)\n",
      "104 313 Loss: 0.780 | Acc: 87.113% (2927/3360)\n",
      "105 313 Loss: 0.777 | Acc: 87.117% (2955/3392)\n",
      "106 313 Loss: 0.775 | Acc: 87.120% (2983/3424)\n",
      "107 313 Loss: 0.779 | Acc: 87.124% (3011/3456)\n",
      "108 313 Loss: 0.777 | Acc: 87.185% (3041/3488)\n",
      "109 313 Loss: 0.780 | Acc: 87.159% (3068/3520)\n",
      "110 313 Loss: 0.777 | Acc: 87.218% (3098/3552)\n",
      "111 313 Loss: 0.775 | Acc: 87.193% (3125/3584)\n",
      "112 313 Loss: 0.788 | Acc: 87.168% (3152/3616)\n",
      "113 313 Loss: 0.787 | Acc: 87.171% (3180/3648)\n",
      "114 313 Loss: 0.784 | Acc: 87.147% (3207/3680)\n",
      "115 313 Loss: 0.781 | Acc: 87.177% (3236/3712)\n",
      "116 313 Loss: 0.779 | Acc: 87.153% (3263/3744)\n",
      "117 313 Loss: 0.777 | Acc: 87.209% (3293/3776)\n",
      "118 313 Loss: 0.781 | Acc: 87.159% (3319/3808)\n",
      "119 313 Loss: 0.782 | Acc: 87.161% (3347/3840)\n",
      "120 313 Loss: 0.777 | Acc: 87.242% (3378/3872)\n",
      "121 313 Loss: 0.774 | Acc: 87.295% (3408/3904)\n",
      "122 313 Loss: 0.771 | Acc: 87.322% (3437/3936)\n",
      "123 313 Loss: 0.768 | Acc: 87.374% (3467/3968)\n",
      "124 313 Loss: 0.762 | Acc: 87.475% (3499/4000)\n",
      "125 313 Loss: 0.764 | Acc: 87.475% (3527/4032)\n",
      "126 313 Loss: 0.768 | Acc: 87.475% (3555/4064)\n",
      "127 313 Loss: 0.766 | Acc: 87.524% (3585/4096)\n",
      "128 313 Loss: 0.770 | Acc: 87.524% (3613/4128)\n",
      "129 313 Loss: 0.770 | Acc: 87.524% (3641/4160)\n",
      "130 313 Loss: 0.767 | Acc: 87.595% (3672/4192)\n",
      "131 313 Loss: 0.768 | Acc: 87.571% (3699/4224)\n",
      "132 313 Loss: 0.763 | Acc: 87.641% (3730/4256)\n",
      "133 313 Loss: 0.758 | Acc: 87.733% (3762/4288)\n",
      "134 313 Loss: 0.759 | Acc: 87.755% (3791/4320)\n",
      "135 313 Loss: 0.756 | Acc: 87.799% (3821/4352)\n",
      "136 313 Loss: 0.758 | Acc: 87.819% (3850/4384)\n",
      "137 313 Loss: 0.761 | Acc: 87.840% (3879/4416)\n",
      "138 313 Loss: 0.756 | Acc: 87.905% (3910/4448)\n",
      "139 313 Loss: 0.757 | Acc: 87.924% (3939/4480)\n",
      "140 313 Loss: 0.756 | Acc: 87.965% (3969/4512)\n",
      "141 313 Loss: 0.750 | Acc: 88.050% (4001/4544)\n",
      "142 313 Loss: 0.749 | Acc: 88.068% (4030/4576)\n",
      "143 313 Loss: 0.747 | Acc: 88.108% (4060/4608)\n",
      "144 313 Loss: 0.751 | Acc: 87.996% (4083/4640)\n",
      "145 313 Loss: 0.747 | Acc: 88.014% (4112/4672)\n",
      "146 313 Loss: 0.745 | Acc: 88.010% (4140/4704)\n",
      "147 313 Loss: 0.747 | Acc: 88.007% (4168/4736)\n",
      "148 313 Loss: 0.750 | Acc: 87.940% (4193/4768)\n",
      "149 313 Loss: 0.748 | Acc: 87.958% (4222/4800)\n",
      "150 313 Loss: 0.743 | Acc: 88.038% (4254/4832)\n",
      "151 313 Loss: 0.740 | Acc: 88.076% (4284/4864)\n",
      "152 313 Loss: 0.741 | Acc: 88.113% (4314/4896)\n",
      "153 313 Loss: 0.736 | Acc: 88.190% (4346/4928)\n",
      "154 313 Loss: 0.739 | Acc: 88.165% (4373/4960)\n",
      "155 313 Loss: 0.742 | Acc: 88.161% (4401/4992)\n",
      "156 313 Loss: 0.740 | Acc: 88.177% (4430/5024)\n",
      "157 313 Loss: 0.735 | Acc: 88.252% (4462/5056)\n",
      "158 313 Loss: 0.735 | Acc: 88.247% (4490/5088)\n",
      "159 313 Loss: 0.735 | Acc: 88.262% (4519/5120)\n",
      "160 313 Loss: 0.735 | Acc: 88.257% (4547/5152)\n",
      "161 313 Loss: 0.733 | Acc: 88.252% (4575/5184)\n",
      "162 313 Loss: 0.732 | Acc: 88.229% (4602/5216)\n",
      "163 313 Loss: 0.729 | Acc: 88.224% (4630/5248)\n",
      "164 313 Loss: 0.726 | Acc: 88.239% (4659/5280)\n",
      "165 313 Loss: 0.724 | Acc: 88.291% (4690/5312)\n",
      "166 313 Loss: 0.723 | Acc: 88.286% (4718/5344)\n",
      "167 313 Loss: 0.721 | Acc: 88.281% (4746/5376)\n",
      "168 313 Loss: 0.719 | Acc: 88.332% (4777/5408)\n",
      "169 313 Loss: 0.720 | Acc: 88.346% (4806/5440)\n",
      "170 313 Loss: 0.721 | Acc: 88.322% (4833/5472)\n",
      "171 313 Loss: 0.718 | Acc: 88.372% (4864/5504)\n",
      "172 313 Loss: 0.719 | Acc: 88.367% (4892/5536)\n",
      "173 313 Loss: 0.723 | Acc: 88.362% (4920/5568)\n",
      "174 313 Loss: 0.719 | Acc: 88.411% (4951/5600)\n",
      "175 313 Loss: 0.720 | Acc: 88.423% (4980/5632)\n",
      "176 313 Loss: 0.720 | Acc: 88.436% (5009/5664)\n",
      "177 313 Loss: 0.718 | Acc: 88.430% (5037/5696)\n",
      "178 313 Loss: 0.716 | Acc: 88.478% (5068/5728)\n",
      "179 313 Loss: 0.716 | Acc: 88.507% (5098/5760)\n",
      "180 313 Loss: 0.712 | Acc: 88.570% (5130/5792)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 313 Loss: 0.710 | Acc: 88.582% (5159/5824)\n",
      "182 313 Loss: 0.710 | Acc: 88.576% (5187/5856)\n",
      "183 313 Loss: 0.711 | Acc: 88.587% (5216/5888)\n",
      "184 313 Loss: 0.712 | Acc: 88.547% (5242/5920)\n",
      "185 313 Loss: 0.712 | Acc: 88.575% (5272/5952)\n",
      "186 313 Loss: 0.708 | Acc: 88.636% (5304/5984)\n",
      "187 313 Loss: 0.708 | Acc: 88.614% (5331/6016)\n",
      "188 313 Loss: 0.711 | Acc: 88.608% (5359/6048)\n",
      "189 313 Loss: 0.709 | Acc: 88.618% (5388/6080)\n",
      "190 313 Loss: 0.708 | Acc: 88.645% (5418/6112)\n",
      "191 313 Loss: 0.704 | Acc: 88.688% (5449/6144)\n",
      "192 313 Loss: 0.706 | Acc: 88.633% (5474/6176)\n",
      "193 313 Loss: 0.707 | Acc: 88.628% (5502/6208)\n",
      "194 313 Loss: 0.707 | Acc: 88.654% (5532/6240)\n",
      "195 313 Loss: 0.709 | Acc: 88.632% (5559/6272)\n",
      "196 313 Loss: 0.706 | Acc: 88.690% (5591/6304)\n",
      "197 313 Loss: 0.703 | Acc: 88.731% (5622/6336)\n",
      "198 313 Loss: 0.704 | Acc: 88.725% (5650/6368)\n",
      "199 313 Loss: 0.702 | Acc: 88.750% (5680/6400)\n",
      "200 313 Loss: 0.704 | Acc: 88.728% (5707/6432)\n",
      "201 313 Loss: 0.707 | Acc: 88.707% (5734/6464)\n",
      "202 313 Loss: 0.704 | Acc: 88.747% (5765/6496)\n",
      "203 313 Loss: 0.702 | Acc: 88.787% (5796/6528)\n",
      "204 313 Loss: 0.700 | Acc: 88.796% (5825/6560)\n",
      "205 313 Loss: 0.698 | Acc: 88.820% (5855/6592)\n",
      "206 313 Loss: 0.695 | Acc: 88.844% (5885/6624)\n",
      "207 313 Loss: 0.695 | Acc: 88.822% (5912/6656)\n",
      "208 313 Loss: 0.693 | Acc: 88.846% (5942/6688)\n",
      "209 313 Loss: 0.691 | Acc: 88.854% (5971/6720)\n",
      "210 313 Loss: 0.692 | Acc: 88.833% (5998/6752)\n",
      "211 313 Loss: 0.694 | Acc: 88.812% (6025/6784)\n",
      "212 313 Loss: 0.691 | Acc: 88.850% (6056/6816)\n",
      "213 313 Loss: 0.689 | Acc: 88.873% (6086/6848)\n",
      "214 313 Loss: 0.690 | Acc: 88.837% (6112/6880)\n",
      "215 313 Loss: 0.687 | Acc: 88.860% (6142/6912)\n",
      "216 313 Loss: 0.687 | Acc: 88.868% (6171/6944)\n",
      "217 313 Loss: 0.684 | Acc: 88.905% (6202/6976)\n",
      "218 313 Loss: 0.688 | Acc: 88.856% (6227/7008)\n",
      "219 313 Loss: 0.689 | Acc: 88.849% (6255/7040)\n",
      "220 313 Loss: 0.691 | Acc: 88.843% (6283/7072)\n",
      "221 313 Loss: 0.692 | Acc: 88.823% (6310/7104)\n",
      "222 313 Loss: 0.692 | Acc: 88.817% (6338/7136)\n",
      "223 313 Loss: 0.689 | Acc: 88.867% (6370/7168)\n",
      "224 313 Loss: 0.690 | Acc: 88.861% (6398/7200)\n",
      "225 313 Loss: 0.689 | Acc: 88.869% (6427/7232)\n",
      "226 313 Loss: 0.686 | Acc: 88.918% (6459/7264)\n",
      "227 313 Loss: 0.684 | Acc: 88.939% (6489/7296)\n",
      "228 313 Loss: 0.682 | Acc: 88.974% (6520/7328)\n",
      "229 313 Loss: 0.679 | Acc: 89.022% (6552/7360)\n",
      "230 313 Loss: 0.677 | Acc: 89.015% (6580/7392)\n",
      "231 313 Loss: 0.681 | Acc: 88.982% (6606/7424)\n",
      "232 313 Loss: 0.679 | Acc: 89.002% (6636/7456)\n",
      "233 313 Loss: 0.677 | Acc: 89.009% (6665/7488)\n",
      "234 313 Loss: 0.676 | Acc: 88.976% (6691/7520)\n",
      "235 313 Loss: 0.676 | Acc: 88.996% (6721/7552)\n",
      "236 313 Loss: 0.678 | Acc: 88.977% (6748/7584)\n",
      "237 313 Loss: 0.679 | Acc: 88.971% (6776/7616)\n",
      "238 313 Loss: 0.677 | Acc: 88.991% (6806/7648)\n",
      "239 313 Loss: 0.676 | Acc: 88.997% (6835/7680)\n",
      "240 313 Loss: 0.677 | Acc: 89.004% (6864/7712)\n",
      "241 313 Loss: 0.677 | Acc: 88.998% (6892/7744)\n",
      "242 313 Loss: 0.679 | Acc: 88.992% (6920/7776)\n",
      "243 313 Loss: 0.680 | Acc: 88.986% (6948/7808)\n",
      "244 313 Loss: 0.681 | Acc: 88.992% (6977/7840)\n",
      "245 313 Loss: 0.680 | Acc: 88.986% (7005/7872)\n",
      "246 313 Loss: 0.681 | Acc: 89.006% (7035/7904)\n",
      "247 313 Loss: 0.680 | Acc: 89.012% (7064/7936)\n",
      "248 313 Loss: 0.681 | Acc: 88.993% (7091/7968)\n",
      "249 313 Loss: 0.679 | Acc: 89.037% (7123/8000)\n",
      "250 313 Loss: 0.679 | Acc: 89.031% (7151/8032)\n",
      "251 313 Loss: 0.679 | Acc: 89.025% (7179/8064)\n",
      "252 313 Loss: 0.677 | Acc: 89.056% (7210/8096)\n",
      "253 313 Loss: 0.680 | Acc: 89.038% (7237/8128)\n",
      "254 313 Loss: 0.678 | Acc: 89.069% (7268/8160)\n",
      "255 313 Loss: 0.679 | Acc: 89.038% (7294/8192)\n",
      "256 313 Loss: 0.678 | Acc: 89.032% (7322/8224)\n",
      "257 313 Loss: 0.677 | Acc: 89.062% (7353/8256)\n",
      "258 313 Loss: 0.677 | Acc: 89.032% (7379/8288)\n",
      "259 313 Loss: 0.678 | Acc: 89.002% (7405/8320)\n",
      "260 313 Loss: 0.677 | Acc: 88.985% (7432/8352)\n",
      "261 313 Loss: 0.677 | Acc: 88.955% (7458/8384)\n",
      "262 313 Loss: 0.676 | Acc: 88.962% (7487/8416)\n",
      "263 313 Loss: 0.674 | Acc: 88.968% (7516/8448)\n",
      "264 313 Loss: 0.675 | Acc: 88.950% (7543/8480)\n",
      "265 313 Loss: 0.674 | Acc: 88.957% (7572/8512)\n",
      "266 313 Loss: 0.673 | Acc: 88.975% (7602/8544)\n",
      "267 313 Loss: 0.674 | Acc: 88.993% (7632/8576)\n",
      "268 313 Loss: 0.675 | Acc: 88.987% (7660/8608)\n",
      "269 313 Loss: 0.675 | Acc: 88.981% (7688/8640)\n",
      "270 313 Loss: 0.674 | Acc: 88.988% (7717/8672)\n",
      "271 313 Loss: 0.672 | Acc: 88.994% (7746/8704)\n",
      "272 313 Loss: 0.673 | Acc: 89.000% (7775/8736)\n",
      "273 313 Loss: 0.672 | Acc: 89.017% (7805/8768)\n",
      "274 313 Loss: 0.673 | Acc: 89.011% (7833/8800)\n",
      "275 313 Loss: 0.675 | Acc: 88.995% (7860/8832)\n",
      "276 313 Loss: 0.672 | Acc: 89.034% (7892/8864)\n",
      "277 313 Loss: 0.671 | Acc: 89.040% (7921/8896)\n",
      "278 313 Loss: 0.669 | Acc: 89.057% (7951/8928)\n",
      "279 313 Loss: 0.670 | Acc: 89.051% (7979/8960)\n",
      "280 313 Loss: 0.669 | Acc: 89.057% (8008/8992)\n",
      "281 313 Loss: 0.667 | Acc: 89.096% (8040/9024)\n",
      "282 313 Loss: 0.667 | Acc: 89.090% (8068/9056)\n",
      "283 313 Loss: 0.665 | Acc: 89.118% (8099/9088)\n",
      "284 313 Loss: 0.664 | Acc: 89.145% (8130/9120)\n",
      "285 313 Loss: 0.662 | Acc: 89.172% (8161/9152)\n",
      "286 313 Loss: 0.660 | Acc: 89.177% (8190/9184)\n",
      "287 313 Loss: 0.658 | Acc: 89.204% (8221/9216)\n",
      "288 313 Loss: 0.661 | Acc: 89.165% (8246/9248)\n",
      "289 313 Loss: 0.661 | Acc: 89.159% (8274/9280)\n",
      "290 313 Loss: 0.659 | Acc: 89.175% (8304/9312)\n",
      "291 313 Loss: 0.659 | Acc: 89.170% (8332/9344)\n",
      "292 313 Loss: 0.661 | Acc: 89.142% (8358/9376)\n",
      "293 313 Loss: 0.663 | Acc: 89.116% (8384/9408)\n",
      "294 313 Loss: 0.663 | Acc: 89.100% (8411/9440)\n",
      "295 313 Loss: 0.663 | Acc: 89.115% (8441/9472)\n",
      "296 313 Loss: 0.663 | Acc: 89.099% (8468/9504)\n",
      "297 313 Loss: 0.664 | Acc: 89.083% (8495/9536)\n",
      "298 313 Loss: 0.663 | Acc: 89.078% (8523/9568)\n",
      "299 313 Loss: 0.661 | Acc: 89.083% (8552/9600)\n",
      "300 313 Loss: 0.661 | Acc: 89.088% (8581/9632)\n",
      "301 313 Loss: 0.661 | Acc: 89.094% (8610/9664)\n",
      "302 313 Loss: 0.660 | Acc: 89.109% (8640/9696)\n",
      "303 313 Loss: 0.660 | Acc: 89.134% (8671/9728)\n",
      "304 313 Loss: 0.660 | Acc: 89.119% (8698/9760)\n",
      "305 313 Loss: 0.661 | Acc: 89.124% (8727/9792)\n",
      "306 313 Loss: 0.664 | Acc: 89.108% (8754/9824)\n",
      "307 313 Loss: 0.665 | Acc: 89.093% (8781/9856)\n",
      "308 313 Loss: 0.666 | Acc: 89.068% (8807/9888)\n",
      "309 313 Loss: 0.666 | Acc: 89.062% (8835/9920)\n",
      "310 313 Loss: 0.666 | Acc: 89.078% (8865/9952)\n",
      "311 313 Loss: 0.667 | Acc: 89.073% (8893/9984)\n",
      "312 313 Loss: 0.669 | Acc: 89.080% (8908/10000)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 1\n",
    "act_quant = True \n",
    "test_qat(epoch, testloader, criterion, netqat, device, stats, act_quant, num_bits=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e0005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:38:22.983594Z",
     "start_time": "2022-11-03T08:38:22.979286Z"
    }
   },
   "source": [
    "###  Ah! Now we get a decent performance with both activation and weight quantization. We will take this model to FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus experiment - how about a 4-bit quantization?\n",
    "test_qat(epoch, testloader, criterion, netqat, device, stats, act_quant, num_bits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7922325",
   "metadata": {},
   "source": [
    "## Section 3: DietCNN Inference - Multiplication Free\n",
    "### The main efficacy of this is in the FPGA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d7c4af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:48:10.094791Z",
     "start_time": "2022-11-03T08:48:10.081832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Added set of imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# For training\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import faiss\n",
    "import sys\n",
    "sys.path.insert(1, '../core')\n",
    "from lut_utils_cifar import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from patchlib import *\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from vgg_sym import *\n",
    "from eval_vgg_cifar import *\n",
    "PARALLEL = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5884d042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T09:54:46.808193Z",
     "start_time": "2022-11-03T09:54:46.803491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test method for the DietCNN CIFAR-10 VGG Network. Stand alone Python file is also provided \n",
    "# with instrumentation functions for CIFAR-10 VGG DietCNN inference\n",
    "\n",
    "HOWDY = 20000000 \n",
    "\n",
    "# Test accuracy of symbolic inference\n",
    "def test_fullsym_acc(model, data_iter, bss=1):\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    for data in data_iter:\n",
    "        X, y = data\n",
    "        if counter > HOWDY:\n",
    "            break\n",
    "        output = model.forward(X)\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "            #if True:\n",
    "                correct += 1\n",
    "        \n",
    "        counter +=bss \n",
    "        total += bss\n",
    "        if(counter > 0 and counter % bss == 0):\n",
    "            print(\"Full symbolic model test accuracy DietCNN :{}% \".format(100*round(correct/total, 4)))\n",
    "    return round(correct/total, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0637c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-11-03T09:54:57.968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Symbolic model loading started...\n",
      "Symbolic model loading completed in: 10.904255056000238\n"
     ]
    }
   ],
   "source": [
    "# DietCNN HyperParameters - 3 main \n",
    "# 1. Image  and all activation symbols \n",
    "# 2 & 3. Symbols for CONV and FC layers dictionary \n",
    "bs=4\n",
    "index = faiss.read_index(\"./kmeans_vgg11_fullnet_cifar10_k1_512_v0.index\")\n",
    "#index = faiss.read_index(\"./kmeans_vgg11_fullnet_cifar10_k1_256_v0.index\")\n",
    "#index = faiss.read_index(\"./kmeans_vgg11_fullnet_cifar10_k1_64_v0.index\")\n",
    "n_clusters=512\n",
    "\n",
    "# using a single pixel patch as of now\n",
    "conv_patch_size = (1, 1)\n",
    "patch_size = (1, 1)\n",
    "all_patch_size = (1, 1)\n",
    "patch_stride = 1\n",
    "# Hyperparameters 2 & 3. Symbols for CONV and FC layers dictionary \n",
    "n_cluster_conv_filters = 256\n",
    "n_cluster_fc_filters = 128\n",
    "conv_stride = 1\n",
    "\n",
    "# this is the reverse dictionary - symbol to patch \n",
    "centroid_lut = index.reconstruct_n(0, n_clusters)\n",
    "\n",
    "import pickle\n",
    "# Load the CONV and FC dictionaries and the LUT that are created already\n",
    "with open('cifar10_conv_flt.index', \"rb\") as f:\n",
    "    filter_index_conv = pickle.load(f)\n",
    "with open('cifar10_fc_flt.index', \"rb\") as f:\n",
    "    filter_index_fc = pickle.load(f)\n",
    "fc_lut = np.genfromtxt('./cifar10_fc_lut.txt', delimiter=',',dtype=np.int16)\n",
    "conv_lut = np.genfromtxt('./cifar10_conv_lut.txt', delimiter=',',dtype=np.int16)\n",
    "add_lut = np.genfromtxt('./cifar10_add_lut.txt', delimiter=',',dtype=np.int16)\n",
    "relu_lut = np.genfromtxt('./cifar10_relu_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c1_bias_lut = np.genfromtxt('./cifar10_c1_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c2_bias_lut = np.genfromtxt('./cifar10_c2_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c3_bias_lut = np.genfromtxt('./cifar10_c3_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c4_bias_lut = np.genfromtxt('./cifar10_c4_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c5_bias_lut = np.genfromtxt('./cifar10_c5_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c6_bias_lut = np.genfromtxt('./cifar10_c6_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c7_bias_lut = np.genfromtxt('./cifar10_c7_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "c8_bias_lut = np.genfromtxt('./cifar10_c8_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "f1_bias_lut = np.genfromtxt('./cifar10_f1_bias_lut.txt', delimiter=',',dtype=np.int16)\n",
    "\n",
    "# this is the creation of symbolic model.\n",
    "# All these steps are need in the desktop implementation to sync with PyTorch inference\n",
    "# For FPGA implementation the DietCNN models are quite simple \n",
    "\n",
    "print(\" Symbolic model loading started...\")\n",
    "t = time.process_time()\n",
    "netsym = vgg_sym(net,sd, filter_index_conv, filter_index_fc, conv_lut, fc_lut, add_lut, \n",
    "              c1_bias_lut, c2_bias_lut, c3_bias_lut, c4_bias_lut, c5_bias_lut, c6_bias_lut, c7_bias_lut, c8_bias_lut, \n",
    "              f1_bias_lut, relu_lut, n_clusters, index, centroid_lut, patch_size, patch_stride)\n",
    "elapsed_time3 = time.process_time() - t\n",
    "print(\"Symbolic model loading completed in:\",elapsed_time3)\n",
    "netsym.eval()\n",
    "start_t = time.time()  \n",
    "acc = test_fullsym_acc(netsym, testloader)\n",
    "end = time.time()\n",
    "print(\"elapsed time for symbolic inference:\", end - start_t) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23c523",
   "metadata": {},
   "source": [
    "## Section 4: Quantization performance on standard model vs. quantization performance  on an already pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15f9fefb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T13:44:42.350140Z",
     "start_time": "2022-09-24T13:44:28.187482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 7.12%\n",
      "Sparsity in conv2.weight: 37.21%\n",
      "Sparsity in conv3.weight: 33.05%\n",
      "Sparsity in conv4.weight: 28.97%\n",
      "Sparsity in conv5.weight: 43.25%\n",
      "Sparsity in conv6.weight: 77.76%\n",
      "Sparsity in conv7.weight: 90.41%\n",
      "Sparsity in conv8.weight: 90.81%\n",
      "Sparsity in fc1.weight: 18.09%\n",
      "Global sparsity: 75.00%\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 70.383\n",
      "PyTorch optimized model test accuracy :89.52% \n",
      "Elapsed time = 13259.1751 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Now lets prune the model usin global pruning to get an estimate of the possible sparsity without accuracy loss\n",
    "# Load the model\n",
    "model = VGG('VGG11')\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=torch.device('cpu'))['net'])\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[2], 'weight'),\n",
    "    (model.features[4], 'weight'),\n",
    "    (model.features[6], 'weight'),\n",
    "    (model.features[8], 'weight'),\n",
    "    (model.features[10], 'weight'),\n",
    "    (model.features[12], 'weight'),\n",
    "    (model.features[14], 'weight'),\n",
    "    (model.classifier[0], 'weight'),\n",
    "    \n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.75,\n",
    ")\n",
    "check_sparsity(model)\n",
    "#Baseline performance - only pruning device = 'cpu'\n",
    "test(model=model, device=device, test_loader=testloader, train_loader=trainloader, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f662b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T08:13:46.993329Z",
     "start_time": "2022-11-03T08:13:46.976487Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_sparsity(model):\n",
    "    print(\n",
    "        \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[0].weight == 0))\n",
    "            / float(model.features[0].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[2].weight == 0))\n",
    "            / float(model.features[2].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv3.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[4].weight == 0))\n",
    "            / float(model.features[4].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv4.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[6].weight == 0))\n",
    "            / float(model.features[6].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv5.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[8].weight == 0))\n",
    "            / float(model.features[8].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv6.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[10].weight == 0))\n",
    "            / float(model.features[10].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv7.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[12].weight == 0))\n",
    "            / float(model.features[12].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv8.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[14].weight == 0))\n",
    "            / float(model.features[14].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.classifier[0].weight == 0))\n",
    "            / float(model.classifier[0].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Global sparsity: {:.2f}%\".format(\n",
    "            100. * float(\n",
    "                torch.sum(model.features[0].weight == 0)\n",
    "                + torch.sum(model.features[2].weight == 0)\n",
    "                + torch.sum(model.features[4].weight == 0)\n",
    "                + torch.sum(model.features[6].weight == 0)\n",
    "                + torch.sum(model.features[8].weight == 0)\n",
    "                + torch.sum(model.features[10].weight == 0)\n",
    "                + torch.sum(model.features[12].weight == 0)\n",
    "                + torch.sum(model.features[14].weight == 0)\n",
    "                + torch.sum(model.classifier[0].weight == 0)\n",
    "            )\n",
    "            / float(\n",
    "                model.features[0].weight.nelement()\n",
    "                + model.features[2].weight.nelement()\n",
    "                + model.features[4].weight.nelement()\n",
    "                + model.features[6].weight.nelement()\n",
    "                + model.features[8].weight.nelement()\n",
    "                + model.features[10].weight.nelement()\n",
    "                + model.features[12].weight.nelement()\n",
    "                + model.features[14].weight.nelement()\n",
    "                + model.classifier[0].weight.nelement()\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5a3efc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T18:18:41.047917Z",
     "start_time": "2022-09-23T18:18:25.909205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 7.12%\n",
      "Sparsity in conv2.weight: 37.21%\n",
      "Sparsity in conv3.weight: 33.05%\n",
      "Sparsity in conv4.weight: 28.97%\n",
      "Sparsity in conv5.weight: 43.25%\n",
      "Sparsity in conv6.weight: 77.76%\n",
      "Sparsity in conv7.weight: 90.41%\n",
      "Sparsity in conv8.weight: 90.81%\n",
      "Sparsity in fc1.weight: 18.09%\n",
      "Global sparsity: 75.00%\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 70.383\n",
      "PyTorch optimized model test accuracy :89.52% \n",
      "Elapsed time = 15051.9681 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd57c5da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T05:10:33.851741Z",
     "start_time": "2022-09-25T05:10:33.842963Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_sparsity_unst(model):\n",
    "    print(\n",
    "        \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[0].weight == 0))\n",
    "            / float(model.features[0].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[2].weight == 0))\n",
    "            / float(model.features[2].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv3.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[4].weight == 0))\n",
    "            / float(model.features[4].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv4.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[6].weight == 0))\n",
    "            / float(model.features[6].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv5.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[8].weight == 0))\n",
    "            / float(model.features[8].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv6.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[10].weight == 0))\n",
    "            / float(model.features[10].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv7.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[12].weight == 0))\n",
    "            / float(model.features[12].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in conv8.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.features[14].weight == 0))\n",
    "            / float(model.features[14].weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "            100. * float(torch.sum(model.classifier.weight == 0))\n",
    "            / float(model.classifier.weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Global sparsity: {:.2f}%\".format(\n",
    "            100. * float(\n",
    "                torch.sum(model.features[0].weight == 0)\n",
    "                + torch.sum(model.features[2].weight == 0)\n",
    "                + torch.sum(model.features[4].weight == 0)\n",
    "                + torch.sum(model.features[6].weight == 0)\n",
    "                + torch.sum(model.features[8].weight == 0)\n",
    "                + torch.sum(model.features[10].weight == 0)\n",
    "                + torch.sum(model.features[12].weight == 0)\n",
    "                + torch.sum(model.features[14].weight == 0)\n",
    "                + torch.sum(model.classifier.weight == 0)\n",
    "            )\n",
    "            / float(\n",
    "                model.features[0].weight.nelement()\n",
    "                + model.features[2].weight.nelement()\n",
    "                + model.features[4].weight.nelement()\n",
    "                + model.features[6].weight.nelement()\n",
    "                + model.features[8].weight.nelement()\n",
    "                + model.features[10].weight.nelement()\n",
    "                + model.features[12].weight.nelement()\n",
    "                + model.features[14].weight.nelement()\n",
    "                + model.classifier.weight.nelement()\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec218e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T05:10:38.344664Z",
     "start_time": "2022-09-25T05:10:38.337317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use this knowledge to prune the network locally\n",
    "def local_prune_and_test(sp, net, structured=True, quantize=True):\n",
    "    # prune the model layerwise\n",
    "    prune.l1_unstructured(net.features[0], name=\"weight\", amount=sp[0])\n",
    "    prune.l1_unstructured(net.features[2], name=\"weight\", amount=sp[1])\n",
    "    prune.l1_unstructured(net.features[4], name=\"weight\", amount=sp[2])\n",
    "    prune.l1_unstructured(net.features[6], name=\"weight\", amount=sp[3])\n",
    "    prune.l1_unstructured(net.features[8], name=\"weight\", amount=sp[4])\n",
    "    prune.l1_unstructured(net.features[10], name=\"weight\", amount=sp[5])\n",
    "    prune.l1_unstructured(net.features[12], name=\"weight\", amount=sp[6])\n",
    "    prune.l1_unstructured(net.features[14], name=\"weight\", amount=sp[7])\n",
    "    prune.l1_unstructured(net.classifier[0], name=\"weight\", amount=sp[8])\n",
    "\n",
    "    # Remove the layerwise data structures for pruning\n",
    "    net.features[0] = prune.remove(net.features[0], name='weight')\n",
    "    net.features[2] = prune.remove(net.features[2], name='weight')\n",
    "    net.features[4] = prune.remove(net.features[4], name='weight')\n",
    "    net.features[6] = prune.remove(net.features[6], name='weight')\n",
    "    net.features[8] = prune.remove(net.features[8], name='weight')\n",
    "    net.features[10] = prune.remove(net.features[10], name='weight')\n",
    "    net.features[12] = prune.remove(net.features[12], name='weight')\n",
    "    net.features[14] = prune.remove(net.features[14], name='weight')\n",
    "    net.classifier = prune.remove(net.classifier[0], name='weight')\n",
    "\n",
    "    # Check sparsity\n",
    "    if structured:\n",
    "        check_sparsity(net)\n",
    "    else:\n",
    "        check_sparsity_unst(net)\n",
    "\n",
    "    # test the baseline accuracy this is a trial & error phase\n",
    "    test(model=net, device=device, test_loader=testloader, train_loader=trainloader, batch_size=bs, quantize=quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c92534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T13:45:59.097363Z",
     "start_time": "2022-09-24T13:45:45.859254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 7.12%\n",
      "Sparsity in conv2.weight: 37.21%\n",
      "Sparsity in conv3.weight: 33.05%\n",
      "Sparsity in conv4.weight: 28.97%\n",
      "Sparsity in conv5.weight: 43.25%\n",
      "Sparsity in conv6.weight: 77.76%\n",
      "Sparsity in conv7.weight: 90.41%\n",
      "Sparsity in conv8.weight: 90.81%\n",
      "Sparsity in fc1.weight: 18.09%\n",
      "Global sparsity: 75.00%\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 35.198\n",
      "PyTorch optimized model test accuracy :89.55% \n",
      "Elapsed time = 12508.9352 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#sparsities = [0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "#sparsities = [0.3, 0.3, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "sparsities = [0.0712, 0.3721, 0.3305, 0.2897, 0.4325, 0.7776, 0.9041, 0.9081, 0.1809]\n",
    "\n",
    "netstd = VGG('VGG11')\n",
    "sdstd = torch.load(pretrained_model, map_location=torch.device('cpu'))\n",
    "netstd.load_state_dict(sdstd['net'])\n",
    "local_prune_and_test(sparsities, netstd, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8b692a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T05:12:38.338027Z",
     "start_time": "2022-09-25T05:11:26.478256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 7.12%\n",
      "Sparsity in conv2.weight: 37.21%\n",
      "Sparsity in conv3.weight: 33.05%\n",
      "Sparsity in conv4.weight: 28.97%\n",
      "Sparsity in conv5.weight: 43.25%\n",
      "Sparsity in conv6.weight: 77.76%\n",
      "Sparsity in conv7.weight: 90.41%\n",
      "Sparsity in conv8.weight: 90.81%\n",
      "Sparsity in fc1.weight: 18.09%\n",
      "Global sparsity: 75.00%\n",
      "======= Quantization Done =====\n",
      "VGGQ(\n",
      "  (features): Sequential(\n",
      "    (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.3012647330760956, zero_point=64)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.3513280153274536, zero_point=62)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.23424533009529114, zero_point=67)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.20147429406642914, zero_point=51)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1277512013912201, zero_point=42)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.061248283833265305, zero_point=49)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.06570837646722794, zero_point=38)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1376347839832306, zero_point=72)\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): QuantizedLinear(in_features=512, out_features=10, scale=0.46637365221977234, zero_point=36, qscheme=torch.per_tensor_affine)\n",
      "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 8.817\n",
      "PyTorch optimized model test accuracy :89.52% \n",
      "Elapsed time = 4604.2922 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#sparsities = [0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "#sparsities = [0.3, 0.3, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "sparsities = [0.0712, 0.3721, 0.3305, 0.2897, 0.4325, 0.7776, 0.9041, 0.9081, 0.1809]\n",
    "device='cpu'\n",
    "netq = VGGQ('VGG11Q')\n",
    "sdq = torch.load(pretrained_model, map_location=torch.device('cpu'))\n",
    "netq.load_state_dict(sdq['net'])\n",
    "local_prune_and_test(sparsities, netq, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b5405",
   "metadata": {},
   "source": [
    "| Pruning Ratios   | Accuracy (Std) | Accuracy (Q) |\n",
    "| ----------- | ----------- |----------- |\n",
    "| 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2  | 90.15        | 89.86   |\n",
    "| 0.3, 0.3, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5  | 89.22        |89.3     |\n",
    "|0.0712, 0.3721, 0.3305, 0.2897, 0.4325, 0.7776, 0.9041, 0.9081, 0.1809 |76.01|75.57|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04905827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T18:37:38.499467Z",
     "start_time": "2022-09-24T18:37:24.836554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 70.383\n",
      "PyTorch optimized model test accuracy :89.52% \n",
      "Elapsed time = 13604.8198 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "test(model=mq, device=device, test_loader=testloader, train_loader=trainloader, batch_size=bs, quantize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
