{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf409408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T09:38:48.847720Z",
     "start_time": "2022-10-31T09:38:48.842022Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from vgg_sym import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import models, datasets\n",
    "\n",
    "# For training\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "cudnn.benchmark = True\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60f2ae8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T09:39:30.268246Z",
     "start_time": "2022-10-31T09:39:30.240080Z"
    }
   },
   "outputs": [],
   "source": [
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale\n",
    "  \n",
    "  zero_point = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point = qmax\n",
    "  else:\n",
    "      zero_point = initial_zero_point\n",
    "\n",
    "  zero_point = int(zero_point)\n",
    "\n",
    "  return scale, zero_point\n",
    "\n",
    "def calcScaleZeroPointSym(min_val, max_val,num_bits=8):\n",
    "  \n",
    "  # Calc Scale \n",
    "  max_val = max(abs(min_val), abs(max_val))\n",
    "  qmin = 0.\n",
    "  qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "  scale = max_val / qmax\n",
    "\n",
    "  return scale, 0\n",
    "\n",
    "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    q_x = q_x.round().byte()\n",
    "    \n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "\n",
    "def quantize_tensor_sym(x, num_bits=8, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    max_val = max(abs(min_val), abs(max_val))\n",
    "    qmin = 0.\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "    scale = max_val / qmax   \n",
    "\n",
    "    q_x = x/scale\n",
    "\n",
    "    q_x.clamp_(-qmax, qmax).round_()\n",
    "    q_x = q_x.round()\n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=0)\n",
    "\n",
    "def dequantize_tensor_sym(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float())\n",
    "\n",
    "def quantizeLayer(x, layer, stat, scale_x, zp_x, vis=False, axs=None, X=None, y=None, sym=False, num_bits=8):\n",
    "  # for both conv and linear layers\n",
    "\n",
    "  # cache old values\n",
    "  W = layer.weight.data\n",
    "  B = layer.bias.data\n",
    "\n",
    "  # WEIGHTS SIMULATED QUANTISED\n",
    "\n",
    "  # quantise weights, activations are already quantised\n",
    "  if sym:\n",
    "    w = quantize_tensor_sym(layer.weight.data,num_bits=num_bits) \n",
    "    b = quantize_tensor_sym(layer.bias.data,num_bits=num_bits)\n",
    "  else:\n",
    "    w = quantize_tensor(layer.weight.data, num_bits=num_bits) \n",
    "    b = quantize_tensor(layer.bias.data, num_bits=num_bits)\n",
    "\n",
    "  layer.weight.data = w.tensor.float()\n",
    "  layer.bias.data = b.tensor.float()\n",
    "\n",
    "  ## END WEIGHTS QUANTISED SIMULATION\n",
    "\n",
    "\n",
    "  if vis:\n",
    "    axs[X,y].set_xlabel(\"Visualising weights of layer: \")\n",
    "    visualise(layer.weight.data, axs[X,y])\n",
    "\n",
    "  # QUANTISED OP, USES SCALE AND ZERO POINT TO DO LAYER FORWARD PASS. (How does backprop change here ?)\n",
    "  # This is Quantisation Arithmetic\n",
    "  scale_w = w.scale\n",
    "  zp_w = w.zero_point\n",
    "  scale_b = b.scale\n",
    "  zp_b = b.zero_point\n",
    "  \n",
    "  if sym:\n",
    "    scale_next, zero_point_next = calcScaleZeroPointSym(min_val=stat['min'], max_val=stat['max'])\n",
    "  else:\n",
    "    scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
    "\n",
    "  # Preparing input by saturating range to num_bits range.\n",
    "  if sym:\n",
    "    X = x.float()\n",
    "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data)\n",
    "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data)\n",
    "  else:\n",
    "    X = x.float() - zp_x\n",
    "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data - zp_w)\n",
    "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data + zp_b)\n",
    "\n",
    "  # All int computation\n",
    "  if sym:  \n",
    "    x = (layer(X)) \n",
    "  else:\n",
    "    x = (layer(X)) + zero_point_next \n",
    "  \n",
    "  # cast to int\n",
    "  x.round_()\n",
    "\n",
    "  # Perform relu too\n",
    "  x = F.relu(x)\n",
    "\n",
    "  # Reset weights for next forward pass\n",
    "  layer.weight.data = W\n",
    "  layer.bias.data = B\n",
    "  \n",
    "  return x, scale_next, zero_point_next\n",
    "\n",
    "# Get Min and max of x tensor, and stores it\n",
    "def updateStats(x, stats, key):\n",
    "  max_val, _ = torch.max(x, dim=1)\n",
    "  min_val, _ = torch.min(x, dim=1)\n",
    "\n",
    "  # add ema calculation\n",
    "\n",
    "  if key not in stats:\n",
    "    stats[key] = {'max': max_val.sum(), 'min': min_val.sum(), 'total': 1}\n",
    "  else:\n",
    "    stats[key]['max'] += max_val.sum().item()\n",
    "    stats[key]['min'] += min_val.sum().item()\n",
    "    if 'total' in stats[key]:\n",
    "        stats[key]['total'] += 1\n",
    "    else:\n",
    "        stats[key]['total'] = 1\n",
    "  \n",
    "  weighting = 2.0 / (stats[key]['total']) + 1\n",
    "\n",
    "  if 'ema_min' in stats[key]:\n",
    "    stats[key]['ema_min'] = weighting*(min_val.mean().item()) + (1- weighting) * stats[key]['ema_min']\n",
    "  else:\n",
    "    stats[key]['ema_min'] = weighting*(min_val.mean().item())\n",
    "\n",
    "  if 'ema_max' in stats[key]:\n",
    "    stats[key]['ema_max'] = weighting*(max_val.mean().item()) + (1- weighting) * stats[key]['ema_max']\n",
    "  else: \n",
    "    stats[key]['ema_max'] = weighting*(max_val.mean().item())\n",
    "\n",
    "  stats[key]['min_val'] = stats[key]['min']/ stats[key]['total']\n",
    "  stats[key]['max_val'] = stats[key]['max']/ stats[key]['total']\n",
    "  \n",
    "  return stats\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "  x = model.features[1](model.features[0](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "  x =  model.features[3](model.features[2](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "  x = model.features[5](model.features[4](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "  x = model.features[7](model.features[6](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "  x = model.features[9](model.features[8](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "  x = model.features[11](model.features[10](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "  x = model.features[13](model.features[12](x))\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "  x = model.features[15](model.features[14](x))\n",
    "\n",
    "  #x = x.view(x.size(0), -1)  \n",
    "  x = x.view(-1, 512) \n",
    "  \n",
    "  stats = updateStats(x, stats, 'fc')\n",
    "\n",
    "  x = model.classifier(x)\n",
    "\n",
    "  return stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader):\n",
    "    device = 'cpu'\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    stats = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            stats = gatherActivationStats(model, data, stats)\n",
    "    \n",
    "    final_stats = {}\n",
    "    for key, value in stats.items():\n",
    "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "    return final_stats\n",
    "\n",
    "def quantForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8):\n",
    "  X = 0\n",
    "  y = 0\n",
    "  # Quantise before inputting into incoming layers\n",
    "  if sym:\n",
    "    x = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "  else:\n",
    "    x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "\n",
    "    # Quantise before inputting into incoming layers\n",
    "  if sym:\n",
    "    x = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=nb)\n",
    "  else:\n",
    "    x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=nb)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.features[0], stats['conv2'], x.scale, x.zero_point)\n",
    "  #x = model.features[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[2], stats['conv3'], scale_next, zero_point_next)\n",
    "  #x = model.features[3](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[4], stats['conv4'], scale_next, zero_point_next)\n",
    "  #x = model.features[5](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[6], stats['conv5'], scale_next, zero_point_next)\n",
    "  #x = model.features[7](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[8], stats['conv6'], scale_next, zero_point_next)\n",
    "  #x = model.features[9](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[10], stats['conv7'], scale_next, zero_point_next)\n",
    "  #x = model.features[11](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[12], stats['conv8'], scale_next, zero_point_next)\n",
    "  #x = model.features[13](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.features[14], stats['fc'], scale_next, zero_point_next)\n",
    "  #x = model.features[15](x)\n",
    "    \n",
    "  \n",
    "  #x = x.view(x.size(0), -1)  \n",
    "  x = x.view(-1, 512)   \n",
    "  \n",
    "  \n",
    "  # Back to dequant for final layer\n",
    "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "   \n",
    "  x = model.classifier(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "import torch\n",
    "\n",
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, num_bits=8, min_val=None, max_val=None):\n",
    "        x = quantize_tensor(x,num_bits=num_bits, min_val=min_val, max_val=max_val)\n",
    "        x = dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None\n",
    "\n",
    "def quantAwareTrainingForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8, act_quant=False):\n",
    "\n",
    "  #print(x.shape)\n",
    "  #print(model.features[0].weight.data.shape) \n",
    "  #x = model.features[0](x)\n",
    "  #x = model.features[1](x)\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "  #x =  model.features[3](model.features[2](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "  #x = model.features[5](model.features[4](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "  #x = model.features[7](model.features[6](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "  #x = model.features[9](model.features[8](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "  #x = model.features[11](model.features[10](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "  #x = model.features[13](model.features[12](x))\n",
    "  #stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "  #x = model.features[15](model.features[14](x))\n",
    "  ##x = x.view(x.size(0), -1)  \n",
    "  #x = x.view(-1, 512) \n",
    "  #stats = updateStats(x, stats, 'fc')\n",
    "  #x = model.classifier(x)\n",
    "\n",
    "  conv1weight = model.features[0].weight.data\n",
    "  model.features[0].weight.data = FakeQuantOp.apply(model.features[0].weight.data, num_bits)\n",
    "  x = model.features[1](model.features[0](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv1']['ema_min'], stats['conv1']['ema_max'])\n",
    "\n",
    "  conv2weight = model.features[2].weight.data\n",
    "  model.features[2].weight.data = FakeQuantOp.apply(model.features[2].weight.data, num_bits)\n",
    "  x = model.features[3](model.features[2](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv2']['ema_min'], stats['conv2']['ema_max'])\n",
    "\n",
    "  conv3weight = model.features[4].weight.data\n",
    "  model.features[4].weight.data = FakeQuantOp.apply(model.features[4].weight.data, num_bits)\n",
    "  x = model.features[5](model.features[4](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv3')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv3']['ema_min'], stats['conv3']['ema_max'])\n",
    "\n",
    "\n",
    "  conv4weight = model.features[6].weight.data\n",
    "  model.features[6].weight.data = FakeQuantOp.apply(model.features[6].weight.data, num_bits)\n",
    "  x = model.features[7](model.features[6](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv4')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv4']['ema_min'], stats['conv4']['ema_max'])\n",
    "\n",
    "\n",
    "  conv5weight = model.features[8].weight.data\n",
    "  model.features[8].weight.data = FakeQuantOp.apply(model.features[8].weight.data, num_bits)\n",
    "  x = model.features[9](model.features[8](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv5')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv5']['ema_min'], stats['conv5']['ema_max'])\n",
    "\n",
    "\n",
    "\n",
    "  conv6weight = model.features[10].weight.data\n",
    "  model.features[10].weight.data = FakeQuantOp.apply(model.features[10].weight.data, num_bits)\n",
    "  x = model.features[11](model.features[10](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv6')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv6']['ema_min'], stats['conv6']['ema_max'])\n",
    "\n",
    "\n",
    "  conv7weight = model.features[12].weight.data\n",
    "  model.features[12].weight.data = FakeQuantOp.apply(model.features[12].weight.data, num_bits)\n",
    "  x = model.features[13](model.features[12](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv7')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv7']['ema_min'], stats['conv7']['ema_max'])\n",
    "\n",
    "\n",
    "  conv8weight = model.features[14].weight.data\n",
    "  model.features[14].weight.data = FakeQuantOp.apply(model.features[14].weight.data, num_bits)\n",
    "  x = model.features[15](model.features[14](x))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv8')\n",
    "\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv8']['ema_min'], stats['conv8']['ema_max'])\n",
    "\n",
    "  x = x.view(-1, 512) \n",
    "  x = model.classifier(x)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc')\n",
    "\n",
    "\n",
    "  return x, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, stats\n",
    "\n",
    "# Training\n",
    "def train(epoch, trainloader, optimizer, criterion, model, device, stats, act_quant=False, num_bits=8):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #outputs = net(inputs)\n",
    "        outputs, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, stats = quantAwareTrainingForward(model, inputs, stats, num_bits=num_bits, act_quant=act_quant)\n",
    "        model.features[0].weight.data   = conv1weight\n",
    "        model.features[2].weight.data   = conv2weight\n",
    "        model.features[4].weight.data   = conv3weight\n",
    "        model.features[6].weight.data   = conv4weight\n",
    "        model.features[8].weight.data   = conv5weight\n",
    "        model.features[10].weight.data  = conv6weight\n",
    "        model.features[12].weight.data  = conv7weight\n",
    "        model.features[14].weight.data  = conv8weight\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch, testloader, criterion, model, device, stats, act_quant, num_bits=8):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #outputs = net(inputs)\n",
    "            outputs, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, stats = quantAwareTrainingForward(model, inputs, stats, num_bits=num_bits, act_quant=act_quant)\n",
    "            model.features[0].weight.data   = conv1weight\n",
    "            model.features[2].weight.data   = conv2weight\n",
    "            model.features[4].weight.data   = conv3weight\n",
    "            model.features[6].weight.data   = conv4weight\n",
    "            model.features[8].weight.data   = conv5weight\n",
    "            model.features[10].weight.data  = conv6weight\n",
    "            model.features[12].weight.data  = conv7weight\n",
    "            model.features[14].weight.data  = conv8weight\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "#     # Save checkpoint.\n",
    "#     acc = 100.*correct/total\n",
    "#     if acc > best_acc:\n",
    "#         print('Saving model with validation acc, loss: ',acc,' ,',test_loss)\n",
    "#         state = {\n",
    "#             'net': net.state_dict(),\n",
    "#             'acc': acc,\n",
    "#             'epoch': epoch,\n",
    "#         }\n",
    "#         torch.save(state, './cifar_qat.pt')\n",
    "#         best_acc = acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "659c1d69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T09:41:12.920127Z",
     "start_time": "2022-10-31T09:39:36.646939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loading started...\n",
      "{'conv1': {'max': tensor(38.0929), 'min': tensor(-34.3413), 'ema_min': -2.2328857964704634, 'ema_max': 2.5693619505756042}, 'conv2': {'max': tensor(104.3430), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 7.106570094804752}, 'conv3': {'max': tensor(107.0603), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 6.664907399560724}, 'conv4': {'max': tensor(74.4338), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 5.193922243396578}, 'conv5': {'max': tensor(52.3959), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 3.8755614585320126}, 'conv6': {'max': tensor(24.7309), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 1.6148410296951774}, 'conv7': {'max': tensor(15.9820), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 0.9670984278935988}, 'conv8': {'max': tensor(31.2128), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 1.8251638992648405}, 'fc': {'max': tensor(62.8690), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 3.908302091814401}}\n",
      "0 625 Loss: 2.309 | Acc: 12.500% (2/16)\n",
      "1 625 Loss: 7.653 | Acc: 15.625% (5/32)\n",
      "2 625 Loss: 5.870 | Acc: 12.500% (6/48)\n",
      "3 625 Loss: 7.660 | Acc: 14.062% (9/64)\n",
      "4 625 Loss: 6.129 | Acc: 31.250% (25/80)\n",
      "5 625 Loss: 5.170 | Acc: 41.667% (40/96)\n",
      "6 625 Loss: 4.462 | Acc: 49.107% (55/112)\n",
      "7 625 Loss: 3.975 | Acc: 53.125% (68/128)\n",
      "8 625 Loss: 3.664 | Acc: 56.944% (82/144)\n",
      "9 625 Loss: 3.343 | Acc: 60.625% (97/160)\n",
      "10 625 Loss: 3.080 | Acc: 62.500% (110/176)\n",
      "11 625 Loss: 2.872 | Acc: 65.104% (125/192)\n",
      "12 625 Loss: 2.651 | Acc: 67.788% (141/208)\n",
      "13 625 Loss: 2.498 | Acc: 69.196% (155/224)\n",
      "14 625 Loss: 2.369 | Acc: 70.833% (170/240)\n",
      "15 625 Loss: 2.251 | Acc: 72.266% (185/256)\n",
      "16 625 Loss: 2.170 | Acc: 73.529% (200/272)\n",
      "17 625 Loss: 2.128 | Acc: 73.958% (213/288)\n",
      "18 625 Loss: 2.016 | Acc: 75.329% (229/304)\n",
      "19 625 Loss: 1.955 | Acc: 75.938% (243/320)\n",
      "20 625 Loss: 1.862 | Acc: 77.083% (259/336)\n",
      "21 625 Loss: 1.777 | Acc: 78.125% (275/352)\n",
      "22 625 Loss: 1.704 | Acc: 78.804% (290/368)\n",
      "23 625 Loss: 1.650 | Acc: 79.167% (304/384)\n",
      "24 625 Loss: 1.625 | Acc: 79.750% (319/400)\n",
      "25 625 Loss: 1.562 | Acc: 80.529% (335/416)\n",
      "26 625 Loss: 1.532 | Acc: 80.787% (349/432)\n",
      "27 625 Loss: 1.504 | Acc: 81.250% (364/448)\n",
      "28 625 Loss: 1.484 | Acc: 81.466% (378/464)\n",
      "29 625 Loss: 1.463 | Acc: 81.875% (393/480)\n",
      "30 625 Loss: 1.424 | Acc: 82.258% (408/496)\n",
      "31 625 Loss: 1.381 | Acc: 82.617% (423/512)\n",
      "32 625 Loss: 1.353 | Acc: 82.955% (438/528)\n",
      "33 625 Loss: 1.320 | Acc: 83.272% (453/544)\n",
      "34 625 Loss: 1.286 | Acc: 83.571% (468/560)\n",
      "35 625 Loss: 1.281 | Acc: 83.507% (481/576)\n",
      "36 625 Loss: 1.247 | Acc: 83.953% (497/592)\n",
      "37 625 Loss: 1.225 | Acc: 84.211% (512/608)\n",
      "38 625 Loss: 1.194 | Acc: 84.615% (528/624)\n",
      "39 625 Loss: 1.164 | Acc: 85.000% (544/640)\n",
      "40 625 Loss: 1.147 | Acc: 84.909% (557/656)\n",
      "41 625 Loss: 1.129 | Acc: 85.119% (572/672)\n",
      "42 625 Loss: 1.143 | Acc: 85.029% (585/688)\n",
      "43 625 Loss: 1.117 | Acc: 85.369% (601/704)\n",
      "44 625 Loss: 1.099 | Acc: 85.417% (615/720)\n",
      "45 625 Loss: 1.099 | Acc: 85.462% (629/736)\n",
      "46 625 Loss: 1.097 | Acc: 85.239% (641/752)\n",
      "47 625 Loss: 1.079 | Acc: 85.417% (656/768)\n",
      "48 625 Loss: 1.065 | Acc: 85.587% (671/784)\n",
      "49 625 Loss: 1.068 | Acc: 85.625% (685/800)\n",
      "50 625 Loss: 1.068 | Acc: 85.662% (699/816)\n",
      "51 625 Loss: 1.048 | Acc: 85.817% (714/832)\n",
      "52 625 Loss: 1.037 | Acc: 85.849% (728/848)\n",
      "53 625 Loss: 1.018 | Acc: 86.111% (744/864)\n",
      "54 625 Loss: 1.001 | Acc: 86.250% (759/880)\n",
      "55 625 Loss: 1.020 | Acc: 86.272% (773/896)\n",
      "56 625 Loss: 1.003 | Acc: 86.404% (788/912)\n",
      "57 625 Loss: 1.000 | Acc: 86.315% (801/928)\n",
      "58 625 Loss: 0.989 | Acc: 86.441% (816/944)\n",
      "59 625 Loss: 0.990 | Acc: 86.354% (829/960)\n",
      "60 625 Loss: 0.978 | Acc: 86.475% (844/976)\n",
      "61 625 Loss: 0.976 | Acc: 86.593% (859/992)\n",
      "62 625 Loss: 0.982 | Acc: 86.409% (871/1008)\n",
      "63 625 Loss: 0.972 | Acc: 86.426% (885/1024)\n",
      "64 625 Loss: 0.959 | Acc: 86.538% (900/1040)\n",
      "65 625 Loss: 0.968 | Acc: 86.458% (913/1056)\n",
      "66 625 Loss: 0.970 | Acc: 86.474% (927/1072)\n",
      "67 625 Loss: 0.980 | Acc: 86.581% (942/1088)\n",
      "68 625 Loss: 0.974 | Acc: 86.594% (956/1104)\n",
      "69 625 Loss: 0.962 | Acc: 86.696% (971/1120)\n",
      "70 625 Loss: 0.971 | Acc: 86.532% (983/1136)\n",
      "71 625 Loss: 0.983 | Acc: 86.372% (995/1152)\n",
      "72 625 Loss: 0.975 | Acc: 86.387% (1009/1168)\n",
      "73 625 Loss: 0.966 | Acc: 86.486% (1024/1184)\n",
      "74 625 Loss: 0.959 | Acc: 86.583% (1039/1200)\n",
      "75 625 Loss: 0.947 | Acc: 86.760% (1055/1216)\n",
      "76 625 Loss: 0.938 | Acc: 86.851% (1070/1232)\n",
      "77 625 Loss: 0.942 | Acc: 86.779% (1083/1248)\n",
      "78 625 Loss: 0.940 | Acc: 86.867% (1098/1264)\n",
      "79 625 Loss: 0.930 | Acc: 86.953% (1113/1280)\n",
      "80 625 Loss: 0.919 | Acc: 87.114% (1129/1296)\n",
      "81 625 Loss: 0.914 | Acc: 87.043% (1142/1312)\n",
      "82 625 Loss: 0.909 | Acc: 87.123% (1157/1328)\n",
      "83 625 Loss: 0.906 | Acc: 87.128% (1171/1344)\n",
      "84 625 Loss: 0.906 | Acc: 87.132% (1185/1360)\n",
      "85 625 Loss: 0.899 | Acc: 87.209% (1200/1376)\n",
      "86 625 Loss: 0.897 | Acc: 87.284% (1215/1392)\n",
      "87 625 Loss: 0.894 | Acc: 87.358% (1230/1408)\n",
      "88 625 Loss: 0.884 | Acc: 87.500% (1246/1424)\n",
      "89 625 Loss: 0.875 | Acc: 87.639% (1262/1440)\n",
      "90 625 Loss: 0.891 | Acc: 87.637% (1276/1456)\n",
      "91 625 Loss: 0.888 | Acc: 87.704% (1291/1472)\n",
      "92 625 Loss: 0.883 | Acc: 87.702% (1305/1488)\n",
      "93 625 Loss: 0.885 | Acc: 87.500% (1316/1504)\n",
      "94 625 Loss: 0.887 | Acc: 87.566% (1331/1520)\n",
      "95 625 Loss: 0.886 | Acc: 87.630% (1346/1536)\n",
      "96 625 Loss: 0.884 | Acc: 87.629% (1360/1552)\n",
      "97 625 Loss: 0.875 | Acc: 87.755% (1376/1568)\n",
      "98 625 Loss: 0.881 | Acc: 87.626% (1388/1584)\n",
      "99 625 Loss: 0.872 | Acc: 87.750% (1404/1600)\n",
      "100 625 Loss: 0.870 | Acc: 87.686% (1417/1616)\n",
      "101 625 Loss: 0.862 | Acc: 87.806% (1433/1632)\n",
      "102 625 Loss: 0.857 | Acc: 87.803% (1447/1648)\n",
      "103 625 Loss: 0.849 | Acc: 87.921% (1463/1664)\n",
      "104 625 Loss: 0.841 | Acc: 88.036% (1479/1680)\n",
      "105 625 Loss: 0.845 | Acc: 87.972% (1492/1696)\n",
      "106 625 Loss: 0.837 | Acc: 88.084% (1508/1712)\n",
      "107 625 Loss: 0.834 | Acc: 88.079% (1522/1728)\n",
      "108 625 Loss: 0.832 | Acc: 88.073% (1536/1744)\n",
      "109 625 Loss: 0.828 | Acc: 88.011% (1549/1760)\n",
      "110 625 Loss: 0.833 | Acc: 87.950% (1562/1776)\n",
      "111 625 Loss: 0.839 | Acc: 87.946% (1576/1792)\n",
      "112 625 Loss: 0.832 | Acc: 88.053% (1592/1808)\n",
      "113 625 Loss: 0.838 | Acc: 87.939% (1604/1824)\n",
      "114 625 Loss: 0.833 | Acc: 87.989% (1619/1840)\n",
      "115 625 Loss: 0.831 | Acc: 87.985% (1633/1856)\n",
      "116 625 Loss: 0.829 | Acc: 88.034% (1648/1872)\n",
      "117 625 Loss: 0.831 | Acc: 87.977% (1661/1888)\n",
      "118 625 Loss: 0.826 | Acc: 88.025% (1676/1904)\n",
      "119 625 Loss: 0.820 | Acc: 88.073% (1691/1920)\n",
      "120 625 Loss: 0.831 | Acc: 87.965% (1703/1936)\n",
      "121 625 Loss: 0.833 | Acc: 87.910% (1716/1952)\n",
      "122 625 Loss: 0.829 | Acc: 87.907% (1730/1968)\n",
      "123 625 Loss: 0.840 | Acc: 87.903% (1744/1984)\n",
      "124 625 Loss: 0.833 | Acc: 88.000% (1760/2000)\n",
      "125 625 Loss: 0.835 | Acc: 87.996% (1774/2016)\n",
      "126 625 Loss: 0.829 | Acc: 88.041% (1789/2032)\n",
      "127 625 Loss: 0.833 | Acc: 87.988% (1802/2048)\n",
      "128 625 Loss: 0.829 | Acc: 87.984% (1816/2064)\n",
      "129 625 Loss: 0.826 | Acc: 88.029% (1831/2080)\n",
      "130 625 Loss: 0.823 | Acc: 88.073% (1846/2096)\n",
      "131 625 Loss: 0.822 | Acc: 88.068% (1860/2112)\n",
      "132 625 Loss: 0.819 | Acc: 88.111% (1875/2128)\n",
      "133 625 Loss: 0.826 | Acc: 88.106% (1889/2144)\n",
      "134 625 Loss: 0.819 | Acc: 88.194% (1905/2160)\n",
      "135 625 Loss: 0.819 | Acc: 88.143% (1918/2176)\n",
      "136 625 Loss: 0.814 | Acc: 88.184% (1933/2192)\n",
      "137 625 Loss: 0.810 | Acc: 88.225% (1948/2208)\n",
      "138 625 Loss: 0.805 | Acc: 88.264% (1963/2224)\n",
      "139 625 Loss: 0.807 | Acc: 88.214% (1976/2240)\n",
      "140 625 Loss: 0.803 | Acc: 88.209% (1990/2256)\n",
      "141 625 Loss: 0.811 | Acc: 88.160% (2003/2272)\n",
      "142 625 Loss: 0.807 | Acc: 88.199% (2018/2288)\n",
      "143 625 Loss: 0.813 | Acc: 88.194% (2032/2304)\n",
      "144 625 Loss: 0.812 | Acc: 88.190% (2046/2320)\n",
      "145 625 Loss: 0.810 | Acc: 88.142% (2059/2336)\n",
      "146 625 Loss: 0.812 | Acc: 88.138% (2073/2352)\n",
      "147 625 Loss: 0.813 | Acc: 88.133% (2087/2368)\n",
      "148 625 Loss: 0.808 | Acc: 88.213% (2103/2384)\n",
      "149 625 Loss: 0.803 | Acc: 88.250% (2118/2400)\n",
      "150 625 Loss: 0.813 | Acc: 88.245% (2132/2416)\n",
      "151 625 Loss: 0.807 | Acc: 88.322% (2148/2432)\n",
      "152 625 Loss: 0.807 | Acc: 88.317% (2162/2448)\n",
      "153 625 Loss: 0.806 | Acc: 88.271% (2175/2464)\n",
      "154 625 Loss: 0.806 | Acc: 88.266% (2189/2480)\n",
      "155 625 Loss: 0.813 | Acc: 88.221% (2202/2496)\n",
      "156 625 Loss: 0.814 | Acc: 88.217% (2216/2512)\n",
      "157 625 Loss: 0.815 | Acc: 88.252% (2231/2528)\n",
      "158 625 Loss: 0.826 | Acc: 88.208% (2244/2544)\n",
      "159 625 Loss: 0.821 | Acc: 88.281% (2260/2560)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 625 Loss: 0.816 | Acc: 88.354% (2276/2576)\n",
      "161 625 Loss: 0.814 | Acc: 88.349% (2290/2592)\n",
      "162 625 Loss: 0.820 | Acc: 88.344% (2304/2608)\n",
      "163 625 Loss: 0.817 | Acc: 88.377% (2319/2624)\n",
      "164 625 Loss: 0.820 | Acc: 88.371% (2333/2640)\n",
      "165 625 Loss: 0.816 | Acc: 88.366% (2347/2656)\n",
      "166 625 Loss: 0.811 | Acc: 88.436% (2363/2672)\n",
      "167 625 Loss: 0.807 | Acc: 88.504% (2379/2688)\n",
      "168 625 Loss: 0.804 | Acc: 88.499% (2393/2704)\n",
      "169 625 Loss: 0.799 | Acc: 88.566% (2409/2720)\n",
      "170 625 Loss: 0.796 | Acc: 88.596% (2424/2736)\n",
      "171 625 Loss: 0.794 | Acc: 88.590% (2438/2752)\n",
      "172 625 Loss: 0.791 | Acc: 88.620% (2453/2768)\n",
      "173 625 Loss: 0.789 | Acc: 88.649% (2468/2784)\n",
      "174 625 Loss: 0.793 | Acc: 88.643% (2482/2800)\n",
      "175 625 Loss: 0.792 | Acc: 88.636% (2496/2816)\n",
      "176 625 Loss: 0.794 | Acc: 88.665% (2511/2832)\n",
      "177 625 Loss: 0.795 | Acc: 88.624% (2524/2848)\n",
      "178 625 Loss: 0.791 | Acc: 88.687% (2540/2864)\n",
      "179 625 Loss: 0.788 | Acc: 88.715% (2555/2880)\n",
      "180 625 Loss: 0.787 | Acc: 88.709% (2569/2896)\n",
      "181 625 Loss: 0.783 | Acc: 88.771% (2585/2912)\n",
      "182 625 Loss: 0.783 | Acc: 88.764% (2599/2928)\n",
      "183 625 Loss: 0.781 | Acc: 88.791% (2614/2944)\n",
      "184 625 Loss: 0.784 | Acc: 88.784% (2628/2960)\n",
      "185 625 Loss: 0.782 | Acc: 88.777% (2642/2976)\n",
      "186 625 Loss: 0.779 | Acc: 88.770% (2656/2992)\n",
      "187 625 Loss: 0.780 | Acc: 88.797% (2671/3008)\n",
      "188 625 Loss: 0.778 | Acc: 88.823% (2686/3024)\n",
      "189 625 Loss: 0.780 | Acc: 88.783% (2699/3040)\n",
      "190 625 Loss: 0.781 | Acc: 88.776% (2713/3056)\n",
      "191 625 Loss: 0.780 | Acc: 88.770% (2727/3072)\n",
      "192 625 Loss: 0.782 | Acc: 88.731% (2740/3088)\n",
      "193 625 Loss: 0.783 | Acc: 88.756% (2755/3104)\n",
      "194 625 Loss: 0.782 | Acc: 88.750% (2769/3120)\n",
      "195 625 Loss: 0.779 | Acc: 88.776% (2784/3136)\n",
      "196 625 Loss: 0.779 | Acc: 88.769% (2798/3152)\n",
      "197 625 Loss: 0.780 | Acc: 88.763% (2812/3168)\n",
      "198 625 Loss: 0.779 | Acc: 88.788% (2827/3184)\n",
      "199 625 Loss: 0.778 | Acc: 88.781% (2841/3200)\n",
      "200 625 Loss: 0.778 | Acc: 88.775% (2855/3216)\n",
      "201 625 Loss: 0.774 | Acc: 88.769% (2869/3232)\n",
      "202 625 Loss: 0.773 | Acc: 88.762% (2883/3248)\n",
      "203 625 Loss: 0.770 | Acc: 88.817% (2899/3264)\n",
      "204 625 Loss: 0.772 | Acc: 88.780% (2912/3280)\n",
      "205 625 Loss: 0.769 | Acc: 88.835% (2928/3296)\n",
      "206 625 Loss: 0.769 | Acc: 88.829% (2942/3312)\n",
      "207 625 Loss: 0.766 | Acc: 88.852% (2957/3328)\n",
      "208 625 Loss: 0.769 | Acc: 88.786% (2969/3344)\n",
      "209 625 Loss: 0.768 | Acc: 88.780% (2983/3360)\n",
      "210 625 Loss: 0.768 | Acc: 88.774% (2997/3376)\n",
      "211 625 Loss: 0.766 | Acc: 88.768% (3011/3392)\n",
      "212 625 Loss: 0.764 | Acc: 88.791% (3026/3408)\n",
      "213 625 Loss: 0.763 | Acc: 88.756% (3039/3424)\n",
      "214 625 Loss: 0.760 | Acc: 88.779% (3054/3440)\n",
      "215 625 Loss: 0.767 | Acc: 88.744% (3067/3456)\n",
      "216 625 Loss: 0.768 | Acc: 88.738% (3081/3472)\n",
      "217 625 Loss: 0.765 | Acc: 88.790% (3097/3488)\n",
      "218 625 Loss: 0.767 | Acc: 88.727% (3109/3504)\n",
      "219 625 Loss: 0.768 | Acc: 88.750% (3124/3520)\n",
      "220 625 Loss: 0.766 | Acc: 88.773% (3139/3536)\n",
      "221 625 Loss: 0.766 | Acc: 88.795% (3154/3552)\n",
      "222 625 Loss: 0.766 | Acc: 88.761% (3167/3568)\n",
      "223 625 Loss: 0.764 | Acc: 88.756% (3181/3584)\n",
      "224 625 Loss: 0.764 | Acc: 88.750% (3195/3600)\n",
      "225 625 Loss: 0.776 | Acc: 88.717% (3208/3616)\n",
      "226 625 Loss: 0.777 | Acc: 88.684% (3221/3632)\n",
      "227 625 Loss: 0.776 | Acc: 88.706% (3236/3648)\n",
      "228 625 Loss: 0.774 | Acc: 88.674% (3249/3664)\n",
      "229 625 Loss: 0.773 | Acc: 88.668% (3263/3680)\n",
      "230 625 Loss: 0.771 | Acc: 88.663% (3277/3696)\n",
      "231 625 Loss: 0.770 | Acc: 88.685% (3292/3712)\n",
      "232 625 Loss: 0.769 | Acc: 88.653% (3305/3728)\n",
      "233 625 Loss: 0.768 | Acc: 88.649% (3319/3744)\n",
      "234 625 Loss: 0.769 | Acc: 88.644% (3333/3760)\n",
      "235 625 Loss: 0.766 | Acc: 88.692% (3349/3776)\n",
      "236 625 Loss: 0.769 | Acc: 88.660% (3362/3792)\n",
      "237 625 Loss: 0.770 | Acc: 88.629% (3375/3808)\n",
      "238 625 Loss: 0.770 | Acc: 88.624% (3389/3824)\n",
      "239 625 Loss: 0.771 | Acc: 88.620% (3403/3840)\n",
      "240 625 Loss: 0.768 | Acc: 88.667% (3419/3856)\n",
      "241 625 Loss: 0.767 | Acc: 88.688% (3434/3872)\n",
      "242 625 Loss: 0.767 | Acc: 88.709% (3449/3888)\n",
      "243 625 Loss: 0.764 | Acc: 88.730% (3464/3904)\n",
      "244 625 Loss: 0.761 | Acc: 88.750% (3479/3920)\n",
      "245 625 Loss: 0.761 | Acc: 88.720% (3492/3936)\n",
      "246 625 Loss: 0.759 | Acc: 88.740% (3507/3952)\n",
      "247 625 Loss: 0.758 | Acc: 88.760% (3522/3968)\n",
      "248 625 Loss: 0.755 | Acc: 88.805% (3538/3984)\n",
      "249 625 Loss: 0.752 | Acc: 88.850% (3554/4000)\n",
      "250 625 Loss: 0.757 | Acc: 88.795% (3566/4016)\n",
      "251 625 Loss: 0.754 | Acc: 88.839% (3582/4032)\n",
      "252 625 Loss: 0.755 | Acc: 88.859% (3597/4048)\n",
      "253 625 Loss: 0.758 | Acc: 88.829% (3610/4064)\n",
      "254 625 Loss: 0.756 | Acc: 88.848% (3625/4080)\n",
      "255 625 Loss: 0.756 | Acc: 88.867% (3640/4096)\n",
      "256 625 Loss: 0.757 | Acc: 88.862% (3654/4112)\n",
      "257 625 Loss: 0.760 | Acc: 88.857% (3668/4128)\n",
      "258 625 Loss: 0.762 | Acc: 88.851% (3682/4144)\n",
      "259 625 Loss: 0.760 | Acc: 88.846% (3696/4160)\n",
      "260 625 Loss: 0.758 | Acc: 88.889% (3712/4176)\n",
      "261 625 Loss: 0.757 | Acc: 88.907% (3727/4192)\n",
      "262 625 Loss: 0.760 | Acc: 88.878% (3740/4208)\n",
      "263 625 Loss: 0.759 | Acc: 88.873% (3754/4224)\n",
      "264 625 Loss: 0.756 | Acc: 88.915% (3770/4240)\n",
      "265 625 Loss: 0.753 | Acc: 88.933% (3785/4256)\n",
      "266 625 Loss: 0.751 | Acc: 88.975% (3801/4272)\n",
      "267 625 Loss: 0.748 | Acc: 89.016% (3817/4288)\n",
      "268 625 Loss: 0.749 | Acc: 89.010% (3831/4304)\n",
      "269 625 Loss: 0.749 | Acc: 89.028% (3846/4320)\n",
      "270 625 Loss: 0.749 | Acc: 89.022% (3860/4336)\n",
      "271 625 Loss: 0.747 | Acc: 89.062% (3876/4352)\n",
      "272 625 Loss: 0.749 | Acc: 89.057% (3890/4368)\n",
      "273 625 Loss: 0.748 | Acc: 89.074% (3905/4384)\n",
      "274 625 Loss: 0.748 | Acc: 89.091% (3920/4400)\n",
      "275 625 Loss: 0.751 | Acc: 89.085% (3934/4416)\n",
      "276 625 Loss: 0.749 | Acc: 89.102% (3949/4432)\n",
      "277 625 Loss: 0.746 | Acc: 89.141% (3965/4448)\n",
      "278 625 Loss: 0.748 | Acc: 89.135% (3979/4464)\n",
      "279 625 Loss: 0.748 | Acc: 89.129% (3993/4480)\n",
      "280 625 Loss: 0.745 | Acc: 89.168% (4009/4496)\n",
      "281 625 Loss: 0.747 | Acc: 89.162% (4023/4512)\n",
      "282 625 Loss: 0.744 | Acc: 89.201% (4039/4528)\n",
      "283 625 Loss: 0.741 | Acc: 89.239% (4055/4544)\n",
      "284 625 Loss: 0.742 | Acc: 89.211% (4068/4560)\n",
      "285 625 Loss: 0.740 | Acc: 89.248% (4084/4576)\n",
      "286 625 Loss: 0.740 | Acc: 89.264% (4099/4592)\n",
      "287 625 Loss: 0.738 | Acc: 89.280% (4114/4608)\n",
      "288 625 Loss: 0.740 | Acc: 89.208% (4125/4624)\n",
      "289 625 Loss: 0.742 | Acc: 89.159% (4137/4640)\n",
      "290 625 Loss: 0.741 | Acc: 89.154% (4151/4656)\n",
      "291 625 Loss: 0.739 | Acc: 89.170% (4166/4672)\n",
      "292 625 Loss: 0.738 | Acc: 89.142% (4179/4688)\n",
      "293 625 Loss: 0.737 | Acc: 89.158% (4194/4704)\n",
      "294 625 Loss: 0.737 | Acc: 89.153% (4208/4720)\n",
      "295 625 Loss: 0.739 | Acc: 89.147% (4222/4736)\n",
      "296 625 Loss: 0.740 | Acc: 89.099% (4234/4752)\n",
      "297 625 Loss: 0.742 | Acc: 89.073% (4247/4768)\n",
      "298 625 Loss: 0.740 | Acc: 89.089% (4262/4784)\n",
      "299 625 Loss: 0.739 | Acc: 89.083% (4276/4800)\n",
      "300 625 Loss: 0.737 | Acc: 89.120% (4292/4816)\n",
      "301 625 Loss: 0.735 | Acc: 89.156% (4308/4832)\n",
      "302 625 Loss: 0.734 | Acc: 89.150% (4322/4848)\n",
      "303 625 Loss: 0.732 | Acc: 89.186% (4338/4864)\n",
      "304 625 Loss: 0.729 | Acc: 89.221% (4354/4880)\n",
      "305 625 Loss: 0.732 | Acc: 89.216% (4368/4896)\n",
      "306 625 Loss: 0.730 | Acc: 89.251% (4384/4912)\n",
      "307 625 Loss: 0.728 | Acc: 89.286% (4400/4928)\n",
      "308 625 Loss: 0.731 | Acc: 89.280% (4414/4944)\n",
      "309 625 Loss: 0.731 | Acc: 89.254% (4427/4960)\n",
      "310 625 Loss: 0.736 | Acc: 89.228% (4440/4976)\n",
      "311 625 Loss: 0.734 | Acc: 89.243% (4455/4992)\n",
      "312 625 Loss: 0.734 | Acc: 89.217% (4468/5008)\n",
      "313 625 Loss: 0.732 | Acc: 89.232% (4483/5024)\n",
      "314 625 Loss: 0.730 | Acc: 89.266% (4499/5040)\n",
      "315 625 Loss: 0.727 | Acc: 89.300% (4515/5056)\n",
      "316 625 Loss: 0.725 | Acc: 89.314% (4530/5072)\n",
      "317 625 Loss: 0.727 | Acc: 89.289% (4543/5088)\n",
      "318 625 Loss: 0.725 | Acc: 89.303% (4558/5104)\n",
      "319 625 Loss: 0.727 | Acc: 89.297% (4572/5120)\n",
      "320 625 Loss: 0.727 | Acc: 89.291% (4586/5136)\n",
      "321 625 Loss: 0.727 | Acc: 89.286% (4600/5152)\n",
      "322 625 Loss: 0.726 | Acc: 89.280% (4614/5168)\n",
      "323 625 Loss: 0.725 | Acc: 89.275% (4628/5184)\n",
      "324 625 Loss: 0.724 | Acc: 89.269% (4642/5200)\n",
      "325 625 Loss: 0.724 | Acc: 89.245% (4655/5216)\n",
      "326 625 Loss: 0.723 | Acc: 89.239% (4669/5232)\n",
      "327 625 Loss: 0.722 | Acc: 89.234% (4683/5248)\n",
      "328 625 Loss: 0.720 | Acc: 89.267% (4699/5264)\n",
      "329 625 Loss: 0.719 | Acc: 89.242% (4712/5280)\n",
      "330 625 Loss: 0.718 | Acc: 89.256% (4727/5296)\n",
      "331 625 Loss: 0.716 | Acc: 89.288% (4743/5312)\n",
      "332 625 Loss: 0.715 | Acc: 89.302% (4758/5328)\n",
      "333 625 Loss: 0.716 | Acc: 89.278% (4771/5344)\n",
      "334 625 Loss: 0.715 | Acc: 89.272% (4785/5360)\n",
      "335 625 Loss: 0.713 | Acc: 89.267% (4799/5376)\n",
      "336 625 Loss: 0.713 | Acc: 89.280% (4814/5392)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 625 Loss: 0.711 | Acc: 89.312% (4830/5408)\n",
      "338 625 Loss: 0.712 | Acc: 89.307% (4844/5424)\n",
      "339 625 Loss: 0.712 | Acc: 89.320% (4859/5440)\n",
      "340 625 Loss: 0.714 | Acc: 89.296% (4872/5456)\n",
      "341 625 Loss: 0.714 | Acc: 89.291% (4886/5472)\n",
      "342 625 Loss: 0.712 | Acc: 89.322% (4902/5488)\n",
      "343 625 Loss: 0.710 | Acc: 89.335% (4917/5504)\n",
      "344 625 Loss: 0.712 | Acc: 89.330% (4931/5520)\n",
      "345 625 Loss: 0.712 | Acc: 89.324% (4945/5536)\n",
      "346 625 Loss: 0.715 | Acc: 89.319% (4959/5552)\n",
      "347 625 Loss: 0.715 | Acc: 89.314% (4973/5568)\n",
      "348 625 Loss: 0.713 | Acc: 89.345% (4989/5584)\n",
      "349 625 Loss: 0.712 | Acc: 89.357% (5004/5600)\n",
      "350 625 Loss: 0.715 | Acc: 89.334% (5017/5616)\n",
      "351 625 Loss: 0.713 | Acc: 89.364% (5033/5632)\n",
      "352 625 Loss: 0.715 | Acc: 89.341% (5046/5648)\n",
      "353 625 Loss: 0.713 | Acc: 89.354% (5061/5664)\n",
      "354 625 Loss: 0.712 | Acc: 89.349% (5075/5680)\n",
      "355 625 Loss: 0.711 | Acc: 89.343% (5089/5696)\n",
      "356 625 Loss: 0.709 | Acc: 89.373% (5105/5712)\n",
      "357 625 Loss: 0.709 | Acc: 89.385% (5120/5728)\n",
      "358 625 Loss: 0.710 | Acc: 89.380% (5134/5744)\n",
      "359 625 Loss: 0.709 | Acc: 89.392% (5149/5760)\n",
      "360 625 Loss: 0.707 | Acc: 89.422% (5165/5776)\n",
      "361 625 Loss: 0.705 | Acc: 89.451% (5181/5792)\n",
      "362 625 Loss: 0.703 | Acc: 89.480% (5197/5808)\n",
      "363 625 Loss: 0.703 | Acc: 89.457% (5210/5824)\n",
      "364 625 Loss: 0.705 | Acc: 89.435% (5223/5840)\n",
      "365 625 Loss: 0.704 | Acc: 89.447% (5238/5856)\n",
      "366 625 Loss: 0.704 | Acc: 89.441% (5252/5872)\n",
      "367 625 Loss: 0.705 | Acc: 89.453% (5267/5888)\n",
      "368 625 Loss: 0.703 | Acc: 89.448% (5281/5904)\n",
      "369 625 Loss: 0.705 | Acc: 89.409% (5293/5920)\n",
      "370 625 Loss: 0.707 | Acc: 89.404% (5307/5936)\n",
      "371 625 Loss: 0.705 | Acc: 89.432% (5323/5952)\n",
      "372 625 Loss: 0.703 | Acc: 89.460% (5339/5968)\n",
      "373 625 Loss: 0.702 | Acc: 89.489% (5355/5984)\n",
      "374 625 Loss: 0.701 | Acc: 89.483% (5369/6000)\n",
      "375 625 Loss: 0.701 | Acc: 89.461% (5382/6016)\n",
      "376 625 Loss: 0.703 | Acc: 89.440% (5395/6032)\n",
      "377 625 Loss: 0.704 | Acc: 89.451% (5410/6048)\n",
      "378 625 Loss: 0.703 | Acc: 89.446% (5424/6064)\n",
      "379 625 Loss: 0.703 | Acc: 89.457% (5439/6080)\n",
      "380 625 Loss: 0.701 | Acc: 89.485% (5455/6096)\n",
      "381 625 Loss: 0.701 | Acc: 89.480% (5469/6112)\n",
      "382 625 Loss: 0.700 | Acc: 89.491% (5484/6128)\n",
      "383 625 Loss: 0.698 | Acc: 89.518% (5500/6144)\n",
      "384 625 Loss: 0.700 | Acc: 89.481% (5512/6160)\n",
      "385 625 Loss: 0.700 | Acc: 89.459% (5525/6176)\n",
      "386 625 Loss: 0.700 | Acc: 89.454% (5539/6192)\n",
      "387 625 Loss: 0.701 | Acc: 89.449% (5553/6208)\n",
      "388 625 Loss: 0.702 | Acc: 89.444% (5567/6224)\n",
      "389 625 Loss: 0.700 | Acc: 89.471% (5583/6240)\n",
      "390 625 Loss: 0.700 | Acc: 89.482% (5598/6256)\n",
      "391 625 Loss: 0.703 | Acc: 89.461% (5611/6272)\n",
      "392 625 Loss: 0.701 | Acc: 89.488% (5627/6288)\n",
      "393 625 Loss: 0.700 | Acc: 89.515% (5643/6304)\n",
      "394 625 Loss: 0.699 | Acc: 89.525% (5658/6320)\n",
      "395 625 Loss: 0.697 | Acc: 89.552% (5674/6336)\n",
      "396 625 Loss: 0.697 | Acc: 89.562% (5689/6352)\n",
      "397 625 Loss: 0.698 | Acc: 89.541% (5702/6368)\n",
      "398 625 Loss: 0.698 | Acc: 89.536% (5716/6384)\n",
      "399 625 Loss: 0.696 | Acc: 89.562% (5732/6400)\n",
      "400 625 Loss: 0.695 | Acc: 89.573% (5747/6416)\n",
      "401 625 Loss: 0.699 | Acc: 89.537% (5759/6432)\n",
      "402 625 Loss: 0.702 | Acc: 89.501% (5771/6448)\n",
      "403 625 Loss: 0.701 | Acc: 89.511% (5786/6464)\n",
      "404 625 Loss: 0.699 | Acc: 89.537% (5802/6480)\n",
      "405 625 Loss: 0.698 | Acc: 89.547% (5817/6496)\n",
      "406 625 Loss: 0.696 | Acc: 89.573% (5833/6512)\n",
      "407 625 Loss: 0.696 | Acc: 89.583% (5848/6528)\n",
      "408 625 Loss: 0.696 | Acc: 89.578% (5862/6544)\n",
      "409 625 Loss: 0.695 | Acc: 89.588% (5877/6560)\n",
      "410 625 Loss: 0.693 | Acc: 89.599% (5892/6576)\n",
      "411 625 Loss: 0.692 | Acc: 89.609% (5907/6592)\n",
      "412 625 Loss: 0.691 | Acc: 89.604% (5921/6608)\n",
      "413 625 Loss: 0.689 | Acc: 89.629% (5937/6624)\n",
      "414 625 Loss: 0.688 | Acc: 89.654% (5953/6640)\n",
      "415 625 Loss: 0.689 | Acc: 89.603% (5964/6656)\n",
      "416 625 Loss: 0.688 | Acc: 89.613% (5979/6672)\n",
      "417 625 Loss: 0.687 | Acc: 89.623% (5994/6688)\n",
      "418 625 Loss: 0.687 | Acc: 89.618% (6008/6704)\n",
      "419 625 Loss: 0.686 | Acc: 89.628% (6023/6720)\n",
      "420 625 Loss: 0.685 | Acc: 89.638% (6038/6736)\n",
      "421 625 Loss: 0.686 | Acc: 89.603% (6050/6752)\n",
      "422 625 Loss: 0.688 | Acc: 89.583% (6063/6768)\n",
      "423 625 Loss: 0.688 | Acc: 89.578% (6077/6784)\n",
      "424 625 Loss: 0.687 | Acc: 89.588% (6092/6800)\n",
      "425 625 Loss: 0.685 | Acc: 89.613% (6108/6816)\n",
      "426 625 Loss: 0.685 | Acc: 89.622% (6123/6832)\n",
      "427 625 Loss: 0.683 | Acc: 89.632% (6138/6848)\n",
      "428 625 Loss: 0.684 | Acc: 89.627% (6152/6864)\n",
      "429 625 Loss: 0.684 | Acc: 89.608% (6165/6880)\n",
      "430 625 Loss: 0.683 | Acc: 89.617% (6180/6896)\n",
      "431 625 Loss: 0.682 | Acc: 89.627% (6195/6912)\n",
      "432 625 Loss: 0.683 | Acc: 89.622% (6209/6928)\n",
      "433 625 Loss: 0.682 | Acc: 89.631% (6224/6944)\n",
      "434 625 Loss: 0.680 | Acc: 89.655% (6240/6960)\n",
      "435 625 Loss: 0.679 | Acc: 89.665% (6255/6976)\n",
      "436 625 Loss: 0.679 | Acc: 89.660% (6269/6992)\n",
      "437 625 Loss: 0.682 | Acc: 89.612% (6280/7008)\n",
      "438 625 Loss: 0.682 | Acc: 89.621% (6295/7024)\n",
      "439 625 Loss: 0.683 | Acc: 89.602% (6308/7040)\n",
      "440 625 Loss: 0.685 | Acc: 89.598% (6322/7056)\n",
      "441 625 Loss: 0.686 | Acc: 89.593% (6336/7072)\n",
      "442 625 Loss: 0.684 | Acc: 89.602% (6351/7088)\n",
      "443 625 Loss: 0.687 | Acc: 89.569% (6363/7104)\n",
      "444 625 Loss: 0.689 | Acc: 89.537% (6375/7120)\n",
      "445 625 Loss: 0.687 | Acc: 89.560% (6391/7136)\n",
      "446 625 Loss: 0.686 | Acc: 89.583% (6407/7152)\n",
      "447 625 Loss: 0.684 | Acc: 89.607% (6423/7168)\n",
      "448 625 Loss: 0.686 | Acc: 89.588% (6436/7184)\n",
      "449 625 Loss: 0.685 | Acc: 89.597% (6451/7200)\n",
      "450 625 Loss: 0.685 | Acc: 89.593% (6465/7216)\n",
      "451 625 Loss: 0.684 | Acc: 89.602% (6480/7232)\n",
      "452 625 Loss: 0.682 | Acc: 89.625% (6496/7248)\n",
      "453 625 Loss: 0.681 | Acc: 89.648% (6512/7264)\n",
      "454 625 Loss: 0.680 | Acc: 89.657% (6527/7280)\n",
      "455 625 Loss: 0.679 | Acc: 89.666% (6542/7296)\n",
      "456 625 Loss: 0.678 | Acc: 89.675% (6557/7312)\n",
      "457 625 Loss: 0.677 | Acc: 89.697% (6573/7328)\n",
      "458 625 Loss: 0.675 | Acc: 89.719% (6589/7344)\n",
      "459 625 Loss: 0.674 | Acc: 89.742% (6605/7360)\n",
      "460 625 Loss: 0.673 | Acc: 89.737% (6619/7376)\n",
      "461 625 Loss: 0.672 | Acc: 89.732% (6633/7392)\n",
      "462 625 Loss: 0.673 | Acc: 89.714% (6646/7408)\n",
      "463 625 Loss: 0.676 | Acc: 89.696% (6659/7424)\n",
      "464 625 Loss: 0.675 | Acc: 89.704% (6674/7440)\n",
      "465 625 Loss: 0.674 | Acc: 89.713% (6689/7456)\n",
      "466 625 Loss: 0.673 | Acc: 89.708% (6703/7472)\n",
      "467 625 Loss: 0.672 | Acc: 89.717% (6718/7488)\n",
      "468 625 Loss: 0.672 | Acc: 89.699% (6731/7504)\n",
      "469 625 Loss: 0.671 | Acc: 89.681% (6744/7520)\n",
      "470 625 Loss: 0.672 | Acc: 89.689% (6759/7536)\n",
      "471 625 Loss: 0.671 | Acc: 89.698% (6774/7552)\n",
      "472 625 Loss: 0.675 | Acc: 89.654% (6785/7568)\n",
      "473 625 Loss: 0.673 | Acc: 89.676% (6801/7584)\n",
      "474 625 Loss: 0.673 | Acc: 89.684% (6816/7600)\n",
      "475 625 Loss: 0.674 | Acc: 89.666% (6829/7616)\n",
      "476 625 Loss: 0.673 | Acc: 89.675% (6844/7632)\n",
      "477 625 Loss: 0.672 | Acc: 89.697% (6860/7648)\n",
      "478 625 Loss: 0.672 | Acc: 89.705% (6875/7664)\n",
      "479 625 Loss: 0.671 | Acc: 89.701% (6889/7680)\n",
      "480 625 Loss: 0.673 | Acc: 89.696% (6903/7696)\n",
      "481 625 Loss: 0.672 | Acc: 89.704% (6918/7712)\n",
      "482 625 Loss: 0.671 | Acc: 89.713% (6933/7728)\n",
      "483 625 Loss: 0.672 | Acc: 89.695% (6946/7744)\n",
      "484 625 Loss: 0.673 | Acc: 89.691% (6960/7760)\n",
      "485 625 Loss: 0.674 | Acc: 89.686% (6974/7776)\n",
      "486 625 Loss: 0.674 | Acc: 89.669% (6987/7792)\n",
      "487 625 Loss: 0.675 | Acc: 89.677% (7002/7808)\n",
      "488 625 Loss: 0.674 | Acc: 89.673% (7016/7824)\n",
      "489 625 Loss: 0.676 | Acc: 89.668% (7030/7840)\n",
      "490 625 Loss: 0.677 | Acc: 89.651% (7043/7856)\n",
      "491 625 Loss: 0.676 | Acc: 89.660% (7058/7872)\n",
      "492 625 Loss: 0.676 | Acc: 89.668% (7073/7888)\n",
      "493 625 Loss: 0.676 | Acc: 89.676% (7088/7904)\n",
      "494 625 Loss: 0.677 | Acc: 89.672% (7102/7920)\n",
      "495 625 Loss: 0.676 | Acc: 89.680% (7117/7936)\n",
      "496 625 Loss: 0.676 | Acc: 89.676% (7131/7952)\n",
      "497 625 Loss: 0.677 | Acc: 89.659% (7144/7968)\n",
      "498 625 Loss: 0.675 | Acc: 89.679% (7160/7984)\n",
      "499 625 Loss: 0.674 | Acc: 89.700% (7176/8000)\n",
      "500 625 Loss: 0.675 | Acc: 89.683% (7189/8016)\n",
      "501 625 Loss: 0.674 | Acc: 89.691% (7204/8032)\n",
      "502 625 Loss: 0.674 | Acc: 89.674% (7217/8048)\n",
      "503 625 Loss: 0.674 | Acc: 89.683% (7232/8064)\n",
      "504 625 Loss: 0.674 | Acc: 89.691% (7247/8080)\n",
      "505 625 Loss: 0.673 | Acc: 89.711% (7263/8096)\n",
      "506 625 Loss: 0.673 | Acc: 89.707% (7277/8112)\n",
      "507 625 Loss: 0.676 | Acc: 89.678% (7289/8128)\n",
      "508 625 Loss: 0.675 | Acc: 89.686% (7304/8144)\n",
      "509 625 Loss: 0.673 | Acc: 89.706% (7320/8160)\n",
      "510 625 Loss: 0.675 | Acc: 89.689% (7333/8176)\n",
      "511 625 Loss: 0.674 | Acc: 89.673% (7346/8192)\n",
      "512 625 Loss: 0.674 | Acc: 89.681% (7361/8208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 625 Loss: 0.673 | Acc: 89.664% (7374/8224)\n",
      "514 625 Loss: 0.673 | Acc: 89.672% (7389/8240)\n",
      "515 625 Loss: 0.672 | Acc: 89.692% (7405/8256)\n",
      "516 625 Loss: 0.671 | Acc: 89.688% (7419/8272)\n",
      "517 625 Loss: 0.673 | Acc: 89.660% (7431/8288)\n",
      "518 625 Loss: 0.674 | Acc: 89.632% (7443/8304)\n",
      "519 625 Loss: 0.673 | Acc: 89.627% (7457/8320)\n",
      "520 625 Loss: 0.674 | Acc: 89.599% (7469/8336)\n",
      "521 625 Loss: 0.673 | Acc: 89.607% (7484/8352)\n",
      "522 625 Loss: 0.674 | Acc: 89.579% (7496/8368)\n",
      "523 625 Loss: 0.673 | Acc: 89.575% (7510/8384)\n",
      "524 625 Loss: 0.672 | Acc: 89.583% (7525/8400)\n",
      "525 625 Loss: 0.671 | Acc: 89.579% (7539/8416)\n",
      "526 625 Loss: 0.671 | Acc: 89.575% (7553/8432)\n",
      "527 625 Loss: 0.670 | Acc: 89.583% (7568/8448)\n",
      "528 625 Loss: 0.671 | Acc: 89.579% (7582/8464)\n",
      "529 625 Loss: 0.671 | Acc: 89.564% (7595/8480)\n",
      "530 625 Loss: 0.671 | Acc: 89.560% (7609/8496)\n",
      "531 625 Loss: 0.670 | Acc: 89.579% (7625/8512)\n",
      "532 625 Loss: 0.670 | Acc: 89.587% (7640/8528)\n",
      "533 625 Loss: 0.669 | Acc: 89.595% (7655/8544)\n",
      "534 625 Loss: 0.668 | Acc: 89.603% (7670/8560)\n",
      "535 625 Loss: 0.669 | Acc: 89.611% (7685/8576)\n",
      "536 625 Loss: 0.670 | Acc: 89.583% (7697/8592)\n",
      "537 625 Loss: 0.671 | Acc: 89.591% (7712/8608)\n",
      "538 625 Loss: 0.670 | Acc: 89.587% (7726/8624)\n",
      "539 625 Loss: 0.671 | Acc: 89.583% (7740/8640)\n",
      "540 625 Loss: 0.670 | Acc: 89.579% (7754/8656)\n",
      "541 625 Loss: 0.670 | Acc: 89.587% (7769/8672)\n",
      "542 625 Loss: 0.669 | Acc: 89.583% (7783/8688)\n",
      "543 625 Loss: 0.668 | Acc: 89.591% (7798/8704)\n",
      "544 625 Loss: 0.667 | Acc: 89.610% (7814/8720)\n",
      "545 625 Loss: 0.669 | Acc: 89.595% (7827/8736)\n",
      "546 625 Loss: 0.668 | Acc: 89.602% (7842/8752)\n",
      "547 625 Loss: 0.668 | Acc: 89.610% (7857/8768)\n",
      "548 625 Loss: 0.669 | Acc: 89.595% (7870/8784)\n",
      "549 625 Loss: 0.669 | Acc: 89.602% (7885/8800)\n",
      "550 625 Loss: 0.672 | Acc: 89.576% (7897/8816)\n",
      "551 625 Loss: 0.671 | Acc: 89.583% (7912/8832)\n",
      "552 625 Loss: 0.670 | Acc: 89.602% (7928/8848)\n",
      "553 625 Loss: 0.668 | Acc: 89.621% (7944/8864)\n",
      "554 625 Loss: 0.668 | Acc: 89.617% (7958/8880)\n",
      "555 625 Loss: 0.667 | Acc: 89.625% (7973/8896)\n",
      "556 625 Loss: 0.666 | Acc: 89.632% (7988/8912)\n",
      "557 625 Loss: 0.665 | Acc: 89.639% (8003/8928)\n",
      "558 625 Loss: 0.666 | Acc: 89.624% (8016/8944)\n",
      "559 625 Loss: 0.666 | Acc: 89.632% (8031/8960)\n",
      "560 625 Loss: 0.665 | Acc: 89.639% (8046/8976)\n",
      "561 625 Loss: 0.665 | Acc: 89.635% (8060/8992)\n",
      "562 625 Loss: 0.664 | Acc: 89.654% (8076/9008)\n",
      "563 625 Loss: 0.663 | Acc: 89.672% (8092/9024)\n",
      "564 625 Loss: 0.663 | Acc: 89.668% (8106/9040)\n",
      "565 625 Loss: 0.663 | Acc: 89.664% (8120/9056)\n",
      "566 625 Loss: 0.662 | Acc: 89.683% (8136/9072)\n",
      "567 625 Loss: 0.662 | Acc: 89.690% (8151/9088)\n",
      "568 625 Loss: 0.660 | Acc: 89.708% (8167/9104)\n",
      "569 625 Loss: 0.660 | Acc: 89.715% (8182/9120)\n",
      "570 625 Loss: 0.659 | Acc: 89.733% (8198/9136)\n",
      "571 625 Loss: 0.658 | Acc: 89.740% (8213/9152)\n",
      "572 625 Loss: 0.657 | Acc: 89.747% (8228/9168)\n",
      "573 625 Loss: 0.657 | Acc: 89.743% (8242/9184)\n",
      "574 625 Loss: 0.656 | Acc: 89.750% (8257/9200)\n",
      "575 625 Loss: 0.655 | Acc: 89.768% (8273/9216)\n",
      "576 625 Loss: 0.658 | Acc: 89.721% (8283/9232)\n",
      "577 625 Loss: 0.657 | Acc: 89.728% (8298/9248)\n",
      "578 625 Loss: 0.658 | Acc: 89.724% (8312/9264)\n",
      "579 625 Loss: 0.657 | Acc: 89.720% (8326/9280)\n",
      "580 625 Loss: 0.656 | Acc: 89.727% (8341/9296)\n",
      "581 625 Loss: 0.655 | Acc: 89.734% (8356/9312)\n",
      "582 625 Loss: 0.655 | Acc: 89.730% (8370/9328)\n",
      "583 625 Loss: 0.656 | Acc: 89.726% (8384/9344)\n",
      "584 625 Loss: 0.657 | Acc: 89.701% (8396/9360)\n",
      "585 625 Loss: 0.657 | Acc: 89.697% (8410/9376)\n",
      "586 625 Loss: 0.660 | Acc: 89.672% (8422/9392)\n",
      "587 625 Loss: 0.659 | Acc: 89.668% (8436/9408)\n",
      "588 625 Loss: 0.659 | Acc: 89.675% (8451/9424)\n",
      "589 625 Loss: 0.660 | Acc: 89.650% (8463/9440)\n",
      "590 625 Loss: 0.659 | Acc: 89.657% (8478/9456)\n",
      "591 625 Loss: 0.659 | Acc: 89.664% (8493/9472)\n",
      "592 625 Loss: 0.658 | Acc: 89.671% (8508/9488)\n",
      "593 625 Loss: 0.659 | Acc: 89.646% (8520/9504)\n",
      "594 625 Loss: 0.661 | Acc: 89.632% (8533/9520)\n",
      "595 625 Loss: 0.660 | Acc: 89.629% (8547/9536)\n",
      "596 625 Loss: 0.660 | Acc: 89.625% (8561/9552)\n",
      "597 625 Loss: 0.660 | Acc: 89.622% (8575/9568)\n",
      "598 625 Loss: 0.659 | Acc: 89.629% (8590/9584)\n",
      "599 625 Loss: 0.658 | Acc: 89.625% (8604/9600)\n",
      "600 625 Loss: 0.657 | Acc: 89.621% (8618/9616)\n",
      "601 625 Loss: 0.658 | Acc: 89.628% (8633/9632)\n",
      "602 625 Loss: 0.658 | Acc: 89.625% (8647/9648)\n",
      "603 625 Loss: 0.658 | Acc: 89.632% (8662/9664)\n",
      "604 625 Loss: 0.657 | Acc: 89.638% (8677/9680)\n",
      "605 625 Loss: 0.656 | Acc: 89.635% (8691/9696)\n",
      "606 625 Loss: 0.658 | Acc: 89.642% (8706/9712)\n",
      "607 625 Loss: 0.657 | Acc: 89.659% (8722/9728)\n",
      "608 625 Loss: 0.656 | Acc: 89.645% (8735/9744)\n",
      "609 625 Loss: 0.657 | Acc: 89.641% (8749/9760)\n",
      "610 625 Loss: 0.656 | Acc: 89.658% (8765/9776)\n",
      "611 625 Loss: 0.657 | Acc: 89.645% (8778/9792)\n",
      "612 625 Loss: 0.658 | Acc: 89.641% (8792/9808)\n",
      "613 625 Loss: 0.660 | Acc: 89.627% (8805/9824)\n",
      "614 625 Loss: 0.661 | Acc: 89.624% (8819/9840)\n",
      "615 625 Loss: 0.662 | Acc: 89.610% (8832/9856)\n",
      "616 625 Loss: 0.661 | Acc: 89.607% (8846/9872)\n",
      "617 625 Loss: 0.663 | Acc: 89.583% (8858/9888)\n",
      "618 625 Loss: 0.664 | Acc: 89.570% (8871/9904)\n",
      "619 625 Loss: 0.663 | Acc: 89.577% (8886/9920)\n",
      "620 625 Loss: 0.663 | Acc: 89.573% (8900/9936)\n",
      "621 625 Loss: 0.662 | Acc: 89.590% (8916/9952)\n",
      "622 625 Loss: 0.663 | Acc: 89.597% (8931/9968)\n",
      "623 625 Loss: 0.664 | Acc: 89.583% (8944/9984)\n",
      "624 625 Loss: 0.665 | Acc: 89.590% (8959/10000)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Data loading started...\")\n",
    "bs = 16\n",
    "num_bits=8\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../../formal_pruning/dataset', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='../../../formal_pruning/dataset', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False)\n",
    "num_classes = 10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#pretrained_modelqat = \"./cifar_vgg_sym_v3.pt\"\n",
    "pretrained_modelqat = \"./cifar_qat.pt\"\n",
    "netqat = VGG('VGG11')\n",
    "sdqat = torch.load(pretrained_modelqat, map_location=torch.device('cpu'))\n",
    "netqat.load_state_dict(sdqat['net'])\n",
    "stats = gatherStats(netqat, test_loader)\n",
    "print(stats) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 1\n",
    "act_quant = True \n",
    "test(epoch, test_loader, criterion, netqat, device, stats, act_quant, num_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f2634f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T09:43:13.185625Z",
     "start_time": "2022-10-31T09:41:39.933421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loading started...\n",
      "{'conv1': {'max': tensor(38.0929), 'min': tensor(-34.3413), 'ema_min': -2.2328857964704634, 'ema_max': 2.5693619505756042}, 'conv2': {'max': tensor(104.3430), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 7.106570094804752}, 'conv3': {'max': tensor(107.0603), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 6.664907399560724}, 'conv4': {'max': tensor(74.4338), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 5.193922243396578}, 'conv5': {'max': tensor(52.3959), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 3.8755614585320126}, 'conv6': {'max': tensor(24.7309), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 1.6148410296951774}, 'conv7': {'max': tensor(15.9820), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 0.9670984278935988}, 'conv8': {'max': tensor(31.2128), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 1.8251638992648405}, 'fc': {'max': tensor(62.8690), 'min': tensor(0.), 'ema_min': 0.0, 'ema_max': 3.908302091814401}}\n",
      "0 625 Loss: 2.309 | Acc: 12.500% (2/16)\n",
      "1 625 Loss: 4.624 | Acc: 15.625% (5/32)\n",
      "2 625 Loss: 3.851 | Acc: 12.500% (6/48)\n",
      "3 625 Loss: 5.789 | Acc: 10.938% (7/64)\n",
      "4 625 Loss: 4.830 | Acc: 25.000% (20/80)\n",
      "5 625 Loss: 4.250 | Acc: 35.417% (34/96)\n",
      "6 625 Loss: 3.782 | Acc: 41.964% (47/112)\n",
      "7 625 Loss: 3.483 | Acc: 44.531% (57/128)\n",
      "8 625 Loss: 3.311 | Acc: 47.917% (69/144)\n",
      "9 625 Loss: 3.012 | Acc: 52.500% (84/160)\n",
      "10 625 Loss: 2.775 | Acc: 55.682% (98/176)\n",
      "11 625 Loss: 2.609 | Acc: 57.292% (110/192)\n",
      "12 625 Loss: 2.440 | Acc: 59.615% (124/208)\n",
      "13 625 Loss: 2.344 | Acc: 61.161% (137/224)\n",
      "14 625 Loss: 2.328 | Acc: 62.083% (149/240)\n",
      "15 625 Loss: 2.243 | Acc: 63.281% (162/256)\n",
      "16 625 Loss: 2.160 | Acc: 64.338% (175/272)\n",
      "17 625 Loss: 2.179 | Acc: 64.236% (185/288)\n",
      "18 625 Loss: 2.117 | Acc: 65.132% (198/304)\n",
      "19 625 Loss: 2.074 | Acc: 65.938% (211/320)\n",
      "20 625 Loss: 2.006 | Acc: 66.964% (225/336)\n",
      "21 625 Loss: 1.931 | Acc: 67.898% (239/352)\n",
      "22 625 Loss: 1.888 | Acc: 68.750% (253/368)\n",
      "23 625 Loss: 1.821 | Acc: 69.792% (268/384)\n",
      "24 625 Loss: 1.782 | Acc: 70.000% (280/400)\n",
      "25 625 Loss: 1.727 | Acc: 70.673% (294/416)\n",
      "26 625 Loss: 1.748 | Acc: 70.833% (306/432)\n",
      "27 625 Loss: 1.720 | Acc: 70.982% (318/448)\n",
      "28 625 Loss: 1.702 | Acc: 71.121% (330/464)\n",
      "29 625 Loss: 1.721 | Acc: 71.042% (341/480)\n",
      "30 625 Loss: 1.693 | Acc: 71.371% (354/496)\n",
      "31 625 Loss: 1.641 | Acc: 72.266% (370/512)\n",
      "32 625 Loss: 1.616 | Acc: 72.727% (384/528)\n",
      "33 625 Loss: 1.571 | Acc: 73.529% (400/544)\n",
      "34 625 Loss: 1.546 | Acc: 73.929% (414/560)\n",
      "35 625 Loss: 1.507 | Acc: 74.479% (429/576)\n",
      "36 625 Loss: 1.474 | Acc: 75.000% (444/592)\n",
      "37 625 Loss: 1.492 | Acc: 75.164% (457/608)\n",
      "38 625 Loss: 1.463 | Acc: 75.481% (471/624)\n",
      "39 625 Loss: 1.458 | Acc: 75.312% (482/640)\n",
      "40 625 Loss: 1.464 | Acc: 74.848% (491/656)\n",
      "41 625 Loss: 1.438 | Acc: 75.298% (506/672)\n",
      "42 625 Loss: 1.446 | Acc: 75.145% (517/688)\n",
      "43 625 Loss: 1.417 | Acc: 75.426% (531/704)\n",
      "44 625 Loss: 1.406 | Acc: 75.694% (545/720)\n",
      "45 625 Loss: 1.400 | Acc: 75.543% (556/736)\n",
      "46 625 Loss: 1.408 | Acc: 75.532% (568/752)\n",
      "47 625 Loss: 1.392 | Acc: 75.781% (582/768)\n",
      "48 625 Loss: 1.377 | Acc: 76.020% (596/784)\n",
      "49 625 Loss: 1.393 | Acc: 75.750% (606/800)\n",
      "50 625 Loss: 1.407 | Acc: 75.735% (618/816)\n",
      "51 625 Loss: 1.412 | Acc: 75.721% (630/832)\n",
      "52 625 Loss: 1.413 | Acc: 75.590% (641/848)\n",
      "53 625 Loss: 1.400 | Acc: 75.810% (655/864)\n",
      "54 625 Loss: 1.390 | Acc: 75.795% (667/880)\n",
      "55 625 Loss: 1.410 | Acc: 75.781% (679/896)\n",
      "56 625 Loss: 1.401 | Acc: 76.096% (694/912)\n",
      "57 625 Loss: 1.413 | Acc: 75.970% (705/928)\n",
      "58 625 Loss: 1.403 | Acc: 76.165% (719/944)\n",
      "59 625 Loss: 1.405 | Acc: 76.146% (731/960)\n",
      "60 625 Loss: 1.385 | Acc: 76.434% (746/976)\n",
      "61 625 Loss: 1.375 | Acc: 76.613% (760/992)\n",
      "62 625 Loss: 1.385 | Acc: 76.587% (772/1008)\n",
      "63 625 Loss: 1.372 | Acc: 76.758% (786/1024)\n",
      "64 625 Loss: 1.355 | Acc: 76.923% (800/1040)\n",
      "65 625 Loss: 1.373 | Acc: 76.894% (812/1056)\n",
      "66 625 Loss: 1.393 | Acc: 76.772% (823/1072)\n",
      "67 625 Loss: 1.391 | Acc: 76.930% (837/1088)\n",
      "68 625 Loss: 1.376 | Acc: 77.083% (851/1104)\n",
      "69 625 Loss: 1.362 | Acc: 77.232% (865/1120)\n",
      "70 625 Loss: 1.383 | Acc: 77.025% (875/1136)\n",
      "71 625 Loss: 1.392 | Acc: 77.083% (888/1152)\n",
      "72 625 Loss: 1.397 | Acc: 77.140% (901/1168)\n",
      "73 625 Loss: 1.403 | Acc: 77.027% (912/1184)\n",
      "74 625 Loss: 1.396 | Acc: 77.083% (925/1200)\n",
      "75 625 Loss: 1.382 | Acc: 77.303% (940/1216)\n",
      "76 625 Loss: 1.381 | Acc: 77.273% (952/1232)\n",
      "77 625 Loss: 1.386 | Acc: 77.324% (965/1248)\n",
      "78 625 Loss: 1.376 | Acc: 77.373% (978/1264)\n",
      "79 625 Loss: 1.372 | Acc: 77.422% (991/1280)\n",
      "80 625 Loss: 1.358 | Acc: 77.546% (1005/1296)\n",
      "81 625 Loss: 1.356 | Acc: 77.439% (1016/1312)\n",
      "82 625 Loss: 1.346 | Acc: 77.485% (1029/1328)\n",
      "83 625 Loss: 1.348 | Acc: 77.455% (1041/1344)\n",
      "84 625 Loss: 1.361 | Acc: 77.500% (1054/1360)\n",
      "85 625 Loss: 1.349 | Acc: 77.544% (1067/1376)\n",
      "86 625 Loss: 1.340 | Acc: 77.586% (1080/1392)\n",
      "87 625 Loss: 1.341 | Acc: 77.557% (1092/1408)\n",
      "88 625 Loss: 1.338 | Acc: 77.669% (1106/1424)\n",
      "89 625 Loss: 1.327 | Acc: 77.708% (1119/1440)\n",
      "90 625 Loss: 1.345 | Acc: 77.747% (1132/1456)\n",
      "91 625 Loss: 1.332 | Acc: 77.921% (1147/1472)\n",
      "92 625 Loss: 1.325 | Acc: 77.957% (1160/1488)\n",
      "93 625 Loss: 1.333 | Acc: 77.726% (1169/1504)\n",
      "94 625 Loss: 1.333 | Acc: 77.763% (1182/1520)\n",
      "95 625 Loss: 1.336 | Acc: 77.734% (1194/1536)\n",
      "96 625 Loss: 1.358 | Acc: 77.577% (1204/1552)\n",
      "97 625 Loss: 1.347 | Acc: 77.679% (1218/1568)\n",
      "98 625 Loss: 1.361 | Acc: 77.525% (1228/1584)\n",
      "99 625 Loss: 1.354 | Acc: 77.562% (1241/1600)\n",
      "100 625 Loss: 1.359 | Acc: 77.599% (1254/1616)\n",
      "101 625 Loss: 1.354 | Acc: 77.696% (1268/1632)\n",
      "102 625 Loss: 1.343 | Acc: 77.791% (1282/1648)\n",
      "103 625 Loss: 1.342 | Acc: 77.885% (1296/1664)\n",
      "104 625 Loss: 1.333 | Acc: 78.036% (1311/1680)\n",
      "105 625 Loss: 1.333 | Acc: 77.889% (1321/1696)\n",
      "106 625 Loss: 1.321 | Acc: 78.096% (1337/1712)\n",
      "107 625 Loss: 1.318 | Acc: 78.125% (1350/1728)\n",
      "108 625 Loss: 1.315 | Acc: 78.154% (1363/1744)\n",
      "109 625 Loss: 1.306 | Acc: 78.239% (1377/1760)\n",
      "110 625 Loss: 1.304 | Acc: 78.209% (1389/1776)\n",
      "111 625 Loss: 1.312 | Acc: 78.237% (1402/1792)\n",
      "112 625 Loss: 1.309 | Acc: 78.319% (1416/1808)\n",
      "113 625 Loss: 1.318 | Acc: 78.289% (1428/1824)\n",
      "114 625 Loss: 1.316 | Acc: 78.370% (1442/1840)\n",
      "115 625 Loss: 1.310 | Acc: 78.448% (1456/1856)\n",
      "116 625 Loss: 1.310 | Acc: 78.472% (1469/1872)\n",
      "117 625 Loss: 1.308 | Acc: 78.496% (1482/1888)\n",
      "118 625 Loss: 1.302 | Acc: 78.571% (1496/1904)\n",
      "119 625 Loss: 1.300 | Acc: 78.594% (1509/1920)\n",
      "120 625 Loss: 1.296 | Acc: 78.564% (1521/1936)\n",
      "121 625 Loss: 1.303 | Acc: 78.484% (1532/1952)\n",
      "122 625 Loss: 1.301 | Acc: 78.557% (1546/1968)\n",
      "123 625 Loss: 1.302 | Acc: 78.528% (1558/1984)\n",
      "124 625 Loss: 1.298 | Acc: 78.500% (1570/2000)\n",
      "125 625 Loss: 1.299 | Acc: 78.423% (1581/2016)\n",
      "126 625 Loss: 1.305 | Acc: 78.445% (1594/2032)\n",
      "127 625 Loss: 1.311 | Acc: 78.369% (1605/2048)\n",
      "128 625 Loss: 1.306 | Acc: 78.391% (1618/2064)\n",
      "129 625 Loss: 1.300 | Acc: 78.462% (1632/2080)\n",
      "130 625 Loss: 1.293 | Acc: 78.483% (1645/2096)\n",
      "131 625 Loss: 1.288 | Acc: 78.551% (1659/2112)\n",
      "132 625 Loss: 1.284 | Acc: 78.618% (1673/2128)\n",
      "133 625 Loss: 1.289 | Acc: 78.638% (1686/2144)\n",
      "134 625 Loss: 1.282 | Acc: 78.657% (1699/2160)\n",
      "135 625 Loss: 1.292 | Acc: 78.585% (1710/2176)\n",
      "136 625 Loss: 1.298 | Acc: 78.558% (1722/2192)\n",
      "137 625 Loss: 1.290 | Acc: 78.668% (1737/2208)\n",
      "138 625 Loss: 1.282 | Acc: 78.777% (1752/2224)\n",
      "139 625 Loss: 1.289 | Acc: 78.795% (1765/2240)\n",
      "140 625 Loss: 1.290 | Acc: 78.856% (1779/2256)\n",
      "141 625 Loss: 1.296 | Acc: 78.829% (1791/2272)\n",
      "142 625 Loss: 1.300 | Acc: 78.846% (1804/2288)\n",
      "143 625 Loss: 1.302 | Acc: 78.819% (1816/2304)\n",
      "144 625 Loss: 1.301 | Acc: 78.836% (1829/2320)\n",
      "145 625 Loss: 1.308 | Acc: 78.724% (1839/2336)\n",
      "146 625 Loss: 1.308 | Acc: 78.699% (1851/2352)\n",
      "147 625 Loss: 1.307 | Acc: 78.674% (1863/2368)\n",
      "148 625 Loss: 1.301 | Acc: 78.775% (1878/2384)\n",
      "149 625 Loss: 1.301 | Acc: 78.792% (1891/2400)\n",
      "150 625 Loss: 1.308 | Acc: 78.767% (1903/2416)\n",
      "151 625 Loss: 1.305 | Acc: 78.824% (1917/2432)\n",
      "152 625 Loss: 1.303 | Acc: 78.840% (1930/2448)\n",
      "153 625 Loss: 1.303 | Acc: 78.815% (1942/2464)\n",
      "154 625 Loss: 1.303 | Acc: 78.790% (1954/2480)\n",
      "155 625 Loss: 1.309 | Acc: 78.726% (1965/2496)\n",
      "156 625 Loss: 1.310 | Acc: 78.742% (1978/2512)\n",
      "157 625 Loss: 1.315 | Acc: 78.718% (1990/2528)\n",
      "158 625 Loss: 1.319 | Acc: 78.695% (2002/2544)\n",
      "159 625 Loss: 1.312 | Acc: 78.789% (2017/2560)\n",
      "160 625 Loss: 1.312 | Acc: 78.804% (2030/2576)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 625 Loss: 1.313 | Acc: 78.858% (2044/2592)\n",
      "162 625 Loss: 1.320 | Acc: 78.758% (2054/2608)\n",
      "163 625 Loss: 1.319 | Acc: 78.735% (2066/2624)\n",
      "164 625 Loss: 1.319 | Acc: 78.750% (2079/2640)\n",
      "165 625 Loss: 1.325 | Acc: 78.727% (2091/2656)\n",
      "166 625 Loss: 1.317 | Acc: 78.855% (2107/2672)\n",
      "167 625 Loss: 1.311 | Acc: 78.869% (2120/2688)\n",
      "168 625 Loss: 1.305 | Acc: 78.957% (2135/2704)\n",
      "169 625 Loss: 1.300 | Acc: 79.044% (2150/2720)\n",
      "170 625 Loss: 1.305 | Acc: 79.094% (2164/2736)\n",
      "171 625 Loss: 1.302 | Acc: 79.142% (2178/2752)\n",
      "172 625 Loss: 1.302 | Acc: 79.118% (2190/2768)\n",
      "173 625 Loss: 1.307 | Acc: 79.059% (2201/2784)\n",
      "174 625 Loss: 1.313 | Acc: 79.036% (2213/2800)\n",
      "175 625 Loss: 1.314 | Acc: 79.048% (2226/2816)\n",
      "176 625 Loss: 1.322 | Acc: 79.061% (2239/2832)\n",
      "177 625 Loss: 1.330 | Acc: 78.968% (2249/2848)\n",
      "178 625 Loss: 1.331 | Acc: 78.911% (2260/2864)\n",
      "179 625 Loss: 1.325 | Acc: 78.958% (2274/2880)\n",
      "180 625 Loss: 1.321 | Acc: 79.006% (2288/2896)\n",
      "181 625 Loss: 1.317 | Acc: 79.087% (2303/2912)\n",
      "182 625 Loss: 1.320 | Acc: 79.133% (2317/2928)\n",
      "183 625 Loss: 1.320 | Acc: 79.178% (2331/2944)\n",
      "184 625 Loss: 1.328 | Acc: 79.122% (2342/2960)\n",
      "185 625 Loss: 1.327 | Acc: 79.133% (2355/2976)\n",
      "186 625 Loss: 1.326 | Acc: 79.078% (2366/2992)\n",
      "187 625 Loss: 1.324 | Acc: 79.122% (2380/3008)\n",
      "188 625 Loss: 1.332 | Acc: 79.134% (2393/3024)\n",
      "189 625 Loss: 1.333 | Acc: 79.112% (2405/3040)\n",
      "190 625 Loss: 1.333 | Acc: 79.188% (2420/3056)\n",
      "191 625 Loss: 1.335 | Acc: 79.199% (2433/3072)\n",
      "192 625 Loss: 1.338 | Acc: 79.145% (2444/3088)\n",
      "193 625 Loss: 1.334 | Acc: 79.188% (2458/3104)\n",
      "194 625 Loss: 1.334 | Acc: 79.167% (2470/3120)\n",
      "195 625 Loss: 1.327 | Acc: 79.241% (2485/3136)\n",
      "196 625 Loss: 1.328 | Acc: 79.251% (2498/3152)\n",
      "197 625 Loss: 1.324 | Acc: 79.261% (2511/3168)\n",
      "198 625 Loss: 1.328 | Acc: 79.271% (2524/3184)\n",
      "199 625 Loss: 1.324 | Acc: 79.312% (2538/3200)\n",
      "200 625 Loss: 1.321 | Acc: 79.353% (2552/3216)\n",
      "201 625 Loss: 1.325 | Acc: 79.332% (2564/3232)\n",
      "202 625 Loss: 1.322 | Acc: 79.372% (2578/3248)\n",
      "203 625 Loss: 1.320 | Acc: 79.412% (2592/3264)\n",
      "204 625 Loss: 1.322 | Acc: 79.451% (2606/3280)\n",
      "205 625 Loss: 1.317 | Acc: 79.490% (2620/3296)\n",
      "206 625 Loss: 1.318 | Acc: 79.408% (2630/3312)\n",
      "207 625 Loss: 1.312 | Acc: 79.477% (2645/3328)\n",
      "208 625 Loss: 1.318 | Acc: 79.426% (2656/3344)\n",
      "209 625 Loss: 1.318 | Acc: 79.375% (2667/3360)\n",
      "210 625 Loss: 1.314 | Acc: 79.384% (2680/3376)\n",
      "211 625 Loss: 1.317 | Acc: 79.334% (2691/3392)\n",
      "212 625 Loss: 1.314 | Acc: 79.372% (2705/3408)\n",
      "213 625 Loss: 1.313 | Acc: 79.381% (2718/3424)\n",
      "214 625 Loss: 1.308 | Acc: 79.390% (2731/3440)\n",
      "215 625 Loss: 1.308 | Acc: 79.369% (2743/3456)\n",
      "216 625 Loss: 1.308 | Acc: 79.407% (2757/3472)\n",
      "217 625 Loss: 1.304 | Acc: 79.472% (2772/3488)\n",
      "218 625 Loss: 1.304 | Acc: 79.452% (2784/3504)\n",
      "219 625 Loss: 1.306 | Acc: 79.460% (2797/3520)\n",
      "220 625 Loss: 1.307 | Acc: 79.468% (2810/3536)\n",
      "221 625 Loss: 1.305 | Acc: 79.505% (2824/3552)\n",
      "222 625 Loss: 1.301 | Acc: 79.540% (2838/3568)\n",
      "223 625 Loss: 1.302 | Acc: 79.520% (2850/3584)\n",
      "224 625 Loss: 1.302 | Acc: 79.528% (2863/3600)\n",
      "225 625 Loss: 1.315 | Acc: 79.480% (2874/3616)\n",
      "226 625 Loss: 1.315 | Acc: 79.460% (2886/3632)\n",
      "227 625 Loss: 1.314 | Acc: 79.468% (2899/3648)\n",
      "228 625 Loss: 1.312 | Acc: 79.476% (2912/3664)\n",
      "229 625 Loss: 1.311 | Acc: 79.457% (2924/3680)\n",
      "230 625 Loss: 1.312 | Acc: 79.437% (2936/3696)\n",
      "231 625 Loss: 1.308 | Acc: 79.499% (2951/3712)\n",
      "232 625 Loss: 1.310 | Acc: 79.480% (2963/3728)\n",
      "233 625 Loss: 1.309 | Acc: 79.460% (2975/3744)\n",
      "234 625 Loss: 1.311 | Acc: 79.495% (2989/3760)\n",
      "235 625 Loss: 1.308 | Acc: 79.529% (3003/3776)\n",
      "236 625 Loss: 1.311 | Acc: 79.509% (3015/3792)\n",
      "237 625 Loss: 1.312 | Acc: 79.517% (3028/3808)\n",
      "238 625 Loss: 1.308 | Acc: 79.498% (3040/3824)\n",
      "239 625 Loss: 1.309 | Acc: 79.531% (3054/3840)\n",
      "240 625 Loss: 1.305 | Acc: 79.538% (3067/3856)\n",
      "241 625 Loss: 1.307 | Acc: 79.520% (3079/3872)\n",
      "242 625 Loss: 1.306 | Acc: 79.527% (3092/3888)\n",
      "243 625 Loss: 1.305 | Acc: 79.559% (3106/3904)\n",
      "244 625 Loss: 1.304 | Acc: 79.592% (3120/3920)\n",
      "245 625 Loss: 1.302 | Acc: 79.548% (3131/3936)\n",
      "246 625 Loss: 1.300 | Acc: 79.504% (3142/3952)\n",
      "247 625 Loss: 1.302 | Acc: 79.461% (3153/3968)\n",
      "248 625 Loss: 1.301 | Acc: 79.468% (3166/3984)\n",
      "249 625 Loss: 1.297 | Acc: 79.500% (3180/4000)\n",
      "250 625 Loss: 1.297 | Acc: 79.507% (3193/4016)\n",
      "251 625 Loss: 1.292 | Acc: 79.588% (3209/4032)\n",
      "252 625 Loss: 1.289 | Acc: 79.620% (3223/4048)\n",
      "253 625 Loss: 1.288 | Acc: 79.552% (3233/4064)\n",
      "254 625 Loss: 1.287 | Acc: 79.583% (3247/4080)\n",
      "255 625 Loss: 1.285 | Acc: 79.590% (3260/4096)\n",
      "256 625 Loss: 1.291 | Acc: 79.572% (3272/4112)\n",
      "257 625 Loss: 1.293 | Acc: 79.578% (3285/4128)\n",
      "258 625 Loss: 1.296 | Acc: 79.561% (3297/4144)\n",
      "259 625 Loss: 1.297 | Acc: 79.567% (3310/4160)\n",
      "260 625 Loss: 1.297 | Acc: 79.574% (3323/4176)\n",
      "261 625 Loss: 1.296 | Acc: 79.604% (3337/4192)\n",
      "262 625 Loss: 1.295 | Acc: 79.610% (3350/4208)\n",
      "263 625 Loss: 1.294 | Acc: 79.616% (3363/4224)\n",
      "264 625 Loss: 1.293 | Acc: 79.646% (3377/4240)\n",
      "265 625 Loss: 1.290 | Acc: 79.676% (3391/4256)\n",
      "266 625 Loss: 1.286 | Acc: 79.728% (3406/4272)\n",
      "267 625 Loss: 1.283 | Acc: 79.757% (3420/4288)\n",
      "268 625 Loss: 1.282 | Acc: 79.763% (3433/4304)\n",
      "269 625 Loss: 1.279 | Acc: 79.769% (3446/4320)\n",
      "270 625 Loss: 1.277 | Acc: 79.774% (3459/4336)\n",
      "271 625 Loss: 1.273 | Acc: 79.848% (3475/4352)\n",
      "272 625 Loss: 1.270 | Acc: 79.899% (3490/4368)\n",
      "273 625 Loss: 1.268 | Acc: 79.950% (3505/4384)\n",
      "274 625 Loss: 1.268 | Acc: 79.955% (3518/4400)\n",
      "275 625 Loss: 1.269 | Acc: 79.937% (3530/4416)\n",
      "276 625 Loss: 1.267 | Acc: 79.941% (3543/4432)\n",
      "277 625 Loss: 1.265 | Acc: 79.946% (3556/4448)\n",
      "278 625 Loss: 1.266 | Acc: 79.973% (3570/4464)\n",
      "279 625 Loss: 1.263 | Acc: 79.978% (3583/4480)\n",
      "280 625 Loss: 1.267 | Acc: 79.915% (3593/4496)\n",
      "281 625 Loss: 1.266 | Acc: 79.920% (3606/4512)\n",
      "282 625 Loss: 1.265 | Acc: 79.903% (3618/4528)\n",
      "283 625 Loss: 1.265 | Acc: 79.908% (3631/4544)\n",
      "284 625 Loss: 1.278 | Acc: 79.803% (3639/4560)\n",
      "285 625 Loss: 1.280 | Acc: 79.764% (3650/4576)\n",
      "286 625 Loss: 1.279 | Acc: 79.791% (3664/4592)\n",
      "287 625 Loss: 1.278 | Acc: 79.818% (3678/4608)\n",
      "288 625 Loss: 1.284 | Acc: 79.801% (3690/4624)\n",
      "289 625 Loss: 1.284 | Acc: 79.806% (3703/4640)\n",
      "290 625 Loss: 1.283 | Acc: 79.790% (3715/4656)\n",
      "291 625 Loss: 1.279 | Acc: 79.816% (3729/4672)\n",
      "292 625 Loss: 1.278 | Acc: 79.842% (3743/4688)\n",
      "293 625 Loss: 1.277 | Acc: 79.868% (3757/4704)\n",
      "294 625 Loss: 1.280 | Acc: 79.873% (3770/4720)\n",
      "295 625 Loss: 1.278 | Acc: 79.856% (3782/4736)\n",
      "296 625 Loss: 1.279 | Acc: 79.819% (3793/4752)\n",
      "297 625 Loss: 1.281 | Acc: 79.803% (3805/4768)\n",
      "298 625 Loss: 1.281 | Acc: 79.808% (3818/4784)\n",
      "299 625 Loss: 1.279 | Acc: 79.812% (3831/4800)\n",
      "300 625 Loss: 1.277 | Acc: 79.838% (3845/4816)\n",
      "301 625 Loss: 1.279 | Acc: 79.822% (3857/4832)\n",
      "302 625 Loss: 1.276 | Acc: 79.847% (3871/4848)\n",
      "303 625 Loss: 1.275 | Acc: 79.873% (3885/4864)\n",
      "304 625 Loss: 1.273 | Acc: 79.877% (3898/4880)\n",
      "305 625 Loss: 1.272 | Acc: 79.882% (3911/4896)\n",
      "306 625 Loss: 1.270 | Acc: 79.906% (3925/4912)\n",
      "307 625 Loss: 1.270 | Acc: 79.911% (3938/4928)\n",
      "308 625 Loss: 1.277 | Acc: 79.854% (3948/4944)\n",
      "309 625 Loss: 1.275 | Acc: 79.819% (3959/4960)\n",
      "310 625 Loss: 1.282 | Acc: 79.763% (3969/4976)\n",
      "311 625 Loss: 1.282 | Acc: 79.768% (3982/4992)\n",
      "312 625 Loss: 1.281 | Acc: 79.792% (3996/5008)\n",
      "313 625 Loss: 1.279 | Acc: 79.817% (4010/5024)\n",
      "314 625 Loss: 1.275 | Acc: 79.861% (4025/5040)\n",
      "315 625 Loss: 1.273 | Acc: 79.885% (4039/5056)\n",
      "316 625 Loss: 1.270 | Acc: 79.909% (4053/5072)\n",
      "317 625 Loss: 1.271 | Acc: 79.855% (4063/5088)\n",
      "318 625 Loss: 1.273 | Acc: 79.839% (4075/5104)\n",
      "319 625 Loss: 1.275 | Acc: 79.824% (4087/5120)\n",
      "320 625 Loss: 1.276 | Acc: 79.829% (4100/5136)\n",
      "321 625 Loss: 1.275 | Acc: 79.833% (4113/5152)\n",
      "322 625 Loss: 1.272 | Acc: 79.837% (4126/5168)\n",
      "323 625 Loss: 1.272 | Acc: 79.842% (4139/5184)\n",
      "324 625 Loss: 1.276 | Acc: 79.808% (4150/5200)\n",
      "325 625 Loss: 1.278 | Acc: 79.812% (4163/5216)\n",
      "326 625 Loss: 1.279 | Acc: 79.817% (4176/5232)\n",
      "327 625 Loss: 1.278 | Acc: 79.783% (4187/5248)\n",
      "328 625 Loss: 1.274 | Acc: 79.825% (4202/5264)\n",
      "329 625 Loss: 1.274 | Acc: 79.773% (4212/5280)\n",
      "330 625 Loss: 1.276 | Acc: 79.777% (4225/5296)\n",
      "331 625 Loss: 1.272 | Acc: 79.819% (4240/5312)\n",
      "332 625 Loss: 1.273 | Acc: 79.824% (4253/5328)\n",
      "333 625 Loss: 1.274 | Acc: 79.828% (4266/5344)\n",
      "334 625 Loss: 1.274 | Acc: 79.813% (4278/5360)\n",
      "335 625 Loss: 1.274 | Acc: 79.818% (4291/5376)\n",
      "336 625 Loss: 1.275 | Acc: 79.822% (4304/5392)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 625 Loss: 1.272 | Acc: 79.882% (4320/5408)\n",
      "338 625 Loss: 1.270 | Acc: 79.904% (4334/5424)\n",
      "339 625 Loss: 1.267 | Acc: 79.945% (4349/5440)\n",
      "340 625 Loss: 1.266 | Acc: 79.930% (4361/5456)\n",
      "341 625 Loss: 1.266 | Acc: 79.916% (4373/5472)\n",
      "342 625 Loss: 1.264 | Acc: 79.920% (4386/5488)\n",
      "343 625 Loss: 1.265 | Acc: 79.906% (4398/5504)\n",
      "344 625 Loss: 1.268 | Acc: 79.873% (4409/5520)\n",
      "345 625 Loss: 1.274 | Acc: 79.805% (4418/5536)\n",
      "346 625 Loss: 1.275 | Acc: 79.827% (4432/5552)\n",
      "347 625 Loss: 1.275 | Acc: 79.813% (4444/5568)\n",
      "348 625 Loss: 1.273 | Acc: 79.782% (4455/5584)\n",
      "349 625 Loss: 1.273 | Acc: 79.786% (4468/5600)\n",
      "350 625 Loss: 1.276 | Acc: 79.754% (4479/5616)\n",
      "351 625 Loss: 1.276 | Acc: 79.741% (4491/5632)\n",
      "352 625 Loss: 1.281 | Acc: 79.674% (4500/5648)\n",
      "353 625 Loss: 1.282 | Acc: 79.643% (4511/5664)\n",
      "354 625 Loss: 1.282 | Acc: 79.613% (4522/5680)\n",
      "355 625 Loss: 1.283 | Acc: 79.635% (4536/5696)\n",
      "356 625 Loss: 1.280 | Acc: 79.657% (4550/5712)\n",
      "357 625 Loss: 1.280 | Acc: 79.644% (4562/5728)\n",
      "358 625 Loss: 1.282 | Acc: 79.648% (4575/5744)\n",
      "359 625 Loss: 1.284 | Acc: 79.635% (4587/5760)\n",
      "360 625 Loss: 1.281 | Acc: 79.657% (4601/5776)\n",
      "361 625 Loss: 1.278 | Acc: 79.696% (4616/5792)\n",
      "362 625 Loss: 1.277 | Acc: 79.700% (4629/5808)\n",
      "363 625 Loss: 1.280 | Acc: 79.688% (4641/5824)\n",
      "364 625 Loss: 1.285 | Acc: 79.640% (4651/5840)\n",
      "365 625 Loss: 1.283 | Acc: 79.662% (4665/5856)\n",
      "366 625 Loss: 1.285 | Acc: 79.632% (4676/5872)\n",
      "367 625 Loss: 1.286 | Acc: 79.637% (4689/5888)\n",
      "368 625 Loss: 1.285 | Acc: 79.641% (4702/5904)\n",
      "369 625 Loss: 1.289 | Acc: 79.611% (4713/5920)\n",
      "370 625 Loss: 1.292 | Acc: 79.582% (4724/5936)\n",
      "371 625 Loss: 1.290 | Acc: 79.587% (4737/5952)\n",
      "372 625 Loss: 1.287 | Acc: 79.625% (4752/5968)\n",
      "373 625 Loss: 1.288 | Acc: 79.612% (4764/5984)\n",
      "374 625 Loss: 1.288 | Acc: 79.617% (4777/6000)\n",
      "375 625 Loss: 1.288 | Acc: 79.604% (4789/6016)\n",
      "376 625 Loss: 1.289 | Acc: 79.592% (4801/6032)\n",
      "377 625 Loss: 1.289 | Acc: 79.597% (4814/6048)\n",
      "378 625 Loss: 1.290 | Acc: 79.551% (4824/6064)\n",
      "379 625 Loss: 1.291 | Acc: 79.523% (4835/6080)\n",
      "380 625 Loss: 1.289 | Acc: 79.544% (4849/6096)\n",
      "381 625 Loss: 1.291 | Acc: 79.565% (4863/6112)\n",
      "382 625 Loss: 1.292 | Acc: 79.553% (4875/6128)\n",
      "383 625 Loss: 1.293 | Acc: 79.525% (4886/6144)\n",
      "384 625 Loss: 1.293 | Acc: 79.513% (4898/6160)\n",
      "385 625 Loss: 1.294 | Acc: 79.485% (4909/6176)\n",
      "386 625 Loss: 1.295 | Acc: 79.490% (4922/6192)\n",
      "387 625 Loss: 1.298 | Acc: 79.462% (4933/6208)\n",
      "388 625 Loss: 1.300 | Acc: 79.467% (4946/6224)\n",
      "389 625 Loss: 1.296 | Acc: 79.519% (4962/6240)\n",
      "390 625 Loss: 1.296 | Acc: 79.524% (4975/6256)\n",
      "391 625 Loss: 1.296 | Acc: 79.544% (4989/6272)\n",
      "392 625 Loss: 1.294 | Acc: 79.548% (5002/6288)\n",
      "393 625 Loss: 1.293 | Acc: 79.569% (5016/6304)\n",
      "394 625 Loss: 1.292 | Acc: 79.589% (5030/6320)\n",
      "395 625 Loss: 1.289 | Acc: 79.640% (5046/6336)\n",
      "396 625 Loss: 1.288 | Acc: 79.628% (5058/6352)\n",
      "397 625 Loss: 1.288 | Acc: 79.633% (5071/6368)\n",
      "398 625 Loss: 1.288 | Acc: 79.621% (5083/6384)\n",
      "399 625 Loss: 1.287 | Acc: 79.625% (5096/6400)\n",
      "400 625 Loss: 1.287 | Acc: 79.629% (5109/6416)\n",
      "401 625 Loss: 1.290 | Acc: 79.618% (5121/6432)\n",
      "402 625 Loss: 1.291 | Acc: 79.606% (5133/6448)\n",
      "403 625 Loss: 1.289 | Acc: 79.641% (5148/6464)\n",
      "404 625 Loss: 1.286 | Acc: 79.676% (5163/6480)\n",
      "405 625 Loss: 1.283 | Acc: 79.695% (5177/6496)\n",
      "406 625 Loss: 1.281 | Acc: 79.699% (5190/6512)\n",
      "407 625 Loss: 1.280 | Acc: 79.718% (5204/6528)\n",
      "408 625 Loss: 1.281 | Acc: 79.707% (5216/6544)\n",
      "409 625 Loss: 1.281 | Acc: 79.680% (5227/6560)\n",
      "410 625 Loss: 1.281 | Acc: 79.699% (5241/6576)\n",
      "411 625 Loss: 1.282 | Acc: 79.688% (5253/6592)\n",
      "412 625 Loss: 1.280 | Acc: 79.706% (5267/6608)\n",
      "413 625 Loss: 1.277 | Acc: 79.755% (5283/6624)\n",
      "414 625 Loss: 1.275 | Acc: 79.789% (5298/6640)\n",
      "415 625 Loss: 1.278 | Acc: 79.763% (5309/6656)\n",
      "416 625 Loss: 1.277 | Acc: 79.781% (5323/6672)\n",
      "417 625 Loss: 1.277 | Acc: 79.770% (5335/6688)\n",
      "418 625 Loss: 1.277 | Acc: 79.758% (5347/6704)\n",
      "419 625 Loss: 1.278 | Acc: 79.747% (5359/6720)\n",
      "420 625 Loss: 1.279 | Acc: 79.721% (5370/6736)\n",
      "421 625 Loss: 1.279 | Acc: 79.710% (5382/6752)\n",
      "422 625 Loss: 1.282 | Acc: 79.684% (5393/6768)\n",
      "423 625 Loss: 1.285 | Acc: 79.658% (5404/6784)\n",
      "424 625 Loss: 1.288 | Acc: 79.647% (5416/6800)\n",
      "425 625 Loss: 1.285 | Acc: 79.695% (5432/6816)\n",
      "426 625 Loss: 1.284 | Acc: 79.698% (5445/6832)\n",
      "427 625 Loss: 1.283 | Acc: 79.688% (5457/6848)\n",
      "428 625 Loss: 1.285 | Acc: 79.691% (5470/6864)\n",
      "429 625 Loss: 1.285 | Acc: 79.651% (5480/6880)\n",
      "430 625 Loss: 1.283 | Acc: 79.669% (5494/6896)\n",
      "431 625 Loss: 1.284 | Acc: 79.644% (5505/6912)\n",
      "432 625 Loss: 1.288 | Acc: 79.633% (5517/6928)\n",
      "433 625 Loss: 1.286 | Acc: 79.637% (5530/6944)\n",
      "434 625 Loss: 1.284 | Acc: 79.655% (5544/6960)\n",
      "435 625 Loss: 1.285 | Acc: 79.630% (5555/6976)\n",
      "436 625 Loss: 1.286 | Acc: 79.620% (5567/6992)\n",
      "437 625 Loss: 1.289 | Acc: 79.566% (5576/7008)\n",
      "438 625 Loss: 1.289 | Acc: 79.556% (5588/7024)\n",
      "439 625 Loss: 1.291 | Acc: 79.545% (5600/7040)\n",
      "440 625 Loss: 1.290 | Acc: 79.549% (5613/7056)\n",
      "441 625 Loss: 1.289 | Acc: 79.553% (5626/7072)\n",
      "442 625 Loss: 1.288 | Acc: 79.585% (5641/7088)\n",
      "443 625 Loss: 1.288 | Acc: 79.589% (5654/7104)\n",
      "444 625 Loss: 1.290 | Acc: 79.522% (5662/7120)\n",
      "445 625 Loss: 1.289 | Acc: 79.512% (5674/7136)\n",
      "446 625 Loss: 1.289 | Acc: 79.488% (5685/7152)\n",
      "447 625 Loss: 1.288 | Acc: 79.506% (5699/7168)\n",
      "448 625 Loss: 1.291 | Acc: 79.496% (5711/7184)\n",
      "449 625 Loss: 1.289 | Acc: 79.528% (5726/7200)\n",
      "450 625 Loss: 1.291 | Acc: 79.504% (5737/7216)\n",
      "451 625 Loss: 1.292 | Acc: 79.508% (5750/7232)\n",
      "452 625 Loss: 1.290 | Acc: 79.512% (5763/7248)\n",
      "453 625 Loss: 1.288 | Acc: 79.529% (5777/7264)\n",
      "454 625 Loss: 1.288 | Acc: 79.533% (5790/7280)\n",
      "455 625 Loss: 1.288 | Acc: 79.509% (5801/7296)\n",
      "456 625 Loss: 1.287 | Acc: 79.513% (5814/7312)\n",
      "457 625 Loss: 1.286 | Acc: 79.517% (5827/7328)\n",
      "458 625 Loss: 1.284 | Acc: 79.548% (5842/7344)\n",
      "459 625 Loss: 1.283 | Acc: 79.552% (5855/7360)\n",
      "460 625 Loss: 1.283 | Acc: 79.542% (5867/7376)\n",
      "461 625 Loss: 1.284 | Acc: 79.545% (5880/7392)\n",
      "462 625 Loss: 1.285 | Acc: 79.536% (5892/7408)\n",
      "463 625 Loss: 1.288 | Acc: 79.499% (5902/7424)\n",
      "464 625 Loss: 1.288 | Acc: 79.489% (5914/7440)\n",
      "465 625 Loss: 1.286 | Acc: 79.506% (5928/7456)\n",
      "466 625 Loss: 1.285 | Acc: 79.524% (5942/7472)\n",
      "467 625 Loss: 1.284 | Acc: 79.541% (5956/7488)\n",
      "468 625 Loss: 1.286 | Acc: 79.518% (5967/7504)\n",
      "469 625 Loss: 1.287 | Acc: 79.508% (5979/7520)\n",
      "470 625 Loss: 1.287 | Acc: 79.512% (5992/7536)\n",
      "471 625 Loss: 1.287 | Acc: 79.515% (6005/7552)\n",
      "472 625 Loss: 1.288 | Acc: 79.506% (6017/7568)\n",
      "473 625 Loss: 1.287 | Acc: 79.483% (6028/7584)\n",
      "474 625 Loss: 1.285 | Acc: 79.500% (6042/7600)\n",
      "475 625 Loss: 1.284 | Acc: 79.504% (6055/7616)\n",
      "476 625 Loss: 1.285 | Acc: 79.481% (6066/7632)\n",
      "477 625 Loss: 1.283 | Acc: 79.498% (6080/7648)\n",
      "478 625 Loss: 1.282 | Acc: 79.528% (6095/7664)\n",
      "479 625 Loss: 1.280 | Acc: 79.544% (6109/7680)\n",
      "480 625 Loss: 1.279 | Acc: 79.548% (6122/7696)\n",
      "481 625 Loss: 1.279 | Acc: 79.564% (6136/7712)\n",
      "482 625 Loss: 1.279 | Acc: 79.542% (6147/7728)\n",
      "483 625 Loss: 1.280 | Acc: 79.545% (6160/7744)\n",
      "484 625 Loss: 1.281 | Acc: 79.549% (6173/7760)\n",
      "485 625 Loss: 1.281 | Acc: 79.565% (6187/7776)\n",
      "486 625 Loss: 1.281 | Acc: 79.569% (6200/7792)\n",
      "487 625 Loss: 1.281 | Acc: 79.559% (6212/7808)\n",
      "488 625 Loss: 1.280 | Acc: 79.563% (6225/7824)\n",
      "489 625 Loss: 1.282 | Acc: 79.579% (6239/7840)\n",
      "490 625 Loss: 1.283 | Acc: 79.582% (6252/7856)\n",
      "491 625 Loss: 1.281 | Acc: 79.624% (6268/7872)\n",
      "492 625 Loss: 1.280 | Acc: 79.640% (6282/7888)\n",
      "493 625 Loss: 1.279 | Acc: 79.669% (6297/7904)\n",
      "494 625 Loss: 1.279 | Acc: 79.659% (6309/7920)\n",
      "495 625 Loss: 1.279 | Acc: 79.662% (6322/7936)\n",
      "496 625 Loss: 1.279 | Acc: 79.665% (6335/7952)\n",
      "497 625 Loss: 1.281 | Acc: 79.631% (6345/7968)\n",
      "498 625 Loss: 1.280 | Acc: 79.647% (6359/7984)\n",
      "499 625 Loss: 1.278 | Acc: 79.688% (6375/8000)\n",
      "500 625 Loss: 1.277 | Acc: 79.691% (6388/8016)\n",
      "501 625 Loss: 1.276 | Acc: 79.706% (6402/8032)\n",
      "502 625 Loss: 1.274 | Acc: 79.697% (6414/8048)\n",
      "503 625 Loss: 1.273 | Acc: 79.712% (6428/8064)\n",
      "504 625 Loss: 1.272 | Acc: 79.728% (6442/8080)\n",
      "505 625 Loss: 1.270 | Acc: 79.743% (6456/8096)\n",
      "506 625 Loss: 1.272 | Acc: 79.734% (6468/8112)\n",
      "507 625 Loss: 1.272 | Acc: 79.724% (6480/8128)\n",
      "508 625 Loss: 1.272 | Acc: 79.703% (6491/8144)\n",
      "509 625 Loss: 1.270 | Acc: 79.730% (6506/8160)\n",
      "510 625 Loss: 1.274 | Acc: 79.684% (6515/8176)\n",
      "511 625 Loss: 1.273 | Acc: 79.688% (6528/8192)\n",
      "512 625 Loss: 1.273 | Acc: 79.678% (6540/8208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 625 Loss: 1.274 | Acc: 79.669% (6552/8224)\n",
      "514 625 Loss: 1.273 | Acc: 79.684% (6566/8240)\n",
      "515 625 Loss: 1.272 | Acc: 79.675% (6578/8256)\n",
      "516 625 Loss: 1.271 | Acc: 79.691% (6592/8272)\n",
      "517 625 Loss: 1.274 | Acc: 79.657% (6602/8288)\n",
      "518 625 Loss: 1.276 | Acc: 79.612% (6611/8304)\n",
      "519 625 Loss: 1.275 | Acc: 79.615% (6624/8320)\n",
      "520 625 Loss: 1.275 | Acc: 79.619% (6637/8336)\n",
      "521 625 Loss: 1.277 | Acc: 79.586% (6647/8352)\n",
      "522 625 Loss: 1.279 | Acc: 79.565% (6658/8368)\n",
      "523 625 Loss: 1.279 | Acc: 79.556% (6670/8384)\n",
      "524 625 Loss: 1.281 | Acc: 79.512% (6679/8400)\n",
      "525 625 Loss: 1.279 | Acc: 79.515% (6692/8416)\n",
      "526 625 Loss: 1.279 | Acc: 79.530% (6706/8432)\n",
      "527 625 Loss: 1.281 | Acc: 79.522% (6718/8448)\n",
      "528 625 Loss: 1.281 | Acc: 79.537% (6732/8464)\n",
      "529 625 Loss: 1.281 | Acc: 79.540% (6745/8480)\n",
      "530 625 Loss: 1.284 | Acc: 79.508% (6755/8496)\n",
      "531 625 Loss: 1.282 | Acc: 79.535% (6770/8512)\n",
      "532 625 Loss: 1.283 | Acc: 79.538% (6783/8528)\n",
      "533 625 Loss: 1.282 | Acc: 79.553% (6797/8544)\n",
      "534 625 Loss: 1.281 | Acc: 79.568% (6811/8560)\n",
      "535 625 Loss: 1.281 | Acc: 79.583% (6825/8576)\n",
      "536 625 Loss: 1.286 | Acc: 79.562% (6836/8592)\n",
      "537 625 Loss: 1.288 | Acc: 79.554% (6848/8608)\n",
      "538 625 Loss: 1.289 | Acc: 79.545% (6860/8624)\n",
      "539 625 Loss: 1.289 | Acc: 79.549% (6873/8640)\n",
      "540 625 Loss: 1.289 | Acc: 79.552% (6886/8656)\n",
      "541 625 Loss: 1.287 | Acc: 79.566% (6900/8672)\n",
      "542 625 Loss: 1.286 | Acc: 79.570% (6913/8688)\n",
      "543 625 Loss: 1.285 | Acc: 79.561% (6925/8704)\n",
      "544 625 Loss: 1.283 | Acc: 79.599% (6941/8720)\n",
      "545 625 Loss: 1.285 | Acc: 79.602% (6954/8736)\n",
      "546 625 Loss: 1.284 | Acc: 79.605% (6967/8752)\n",
      "547 625 Loss: 1.284 | Acc: 79.619% (6981/8768)\n",
      "548 625 Loss: 1.286 | Acc: 79.599% (6992/8784)\n",
      "549 625 Loss: 1.285 | Acc: 79.602% (7005/8800)\n",
      "550 625 Loss: 1.287 | Acc: 79.571% (7015/8816)\n",
      "551 625 Loss: 1.287 | Acc: 79.563% (7027/8832)\n",
      "552 625 Loss: 1.285 | Acc: 79.577% (7041/8848)\n",
      "553 625 Loss: 1.284 | Acc: 79.569% (7053/8864)\n",
      "554 625 Loss: 1.284 | Acc: 79.572% (7066/8880)\n",
      "555 625 Loss: 1.283 | Acc: 79.586% (7080/8896)\n",
      "556 625 Loss: 1.283 | Acc: 79.589% (7093/8912)\n",
      "557 625 Loss: 1.281 | Acc: 79.603% (7107/8928)\n",
      "558 625 Loss: 1.281 | Acc: 79.584% (7118/8944)\n",
      "559 625 Loss: 1.280 | Acc: 79.576% (7130/8960)\n",
      "560 625 Loss: 1.280 | Acc: 79.590% (7144/8976)\n",
      "561 625 Loss: 1.280 | Acc: 79.593% (7157/8992)\n",
      "562 625 Loss: 1.280 | Acc: 79.607% (7171/9008)\n",
      "563 625 Loss: 1.278 | Acc: 79.632% (7186/9024)\n",
      "564 625 Loss: 1.278 | Acc: 79.646% (7200/9040)\n",
      "565 625 Loss: 1.278 | Acc: 79.638% (7212/9056)\n",
      "566 625 Loss: 1.276 | Acc: 79.674% (7228/9072)\n",
      "567 625 Loss: 1.276 | Acc: 79.676% (7241/9088)\n",
      "568 625 Loss: 1.274 | Acc: 79.701% (7256/9104)\n",
      "569 625 Loss: 1.274 | Acc: 79.715% (7270/9120)\n",
      "570 625 Loss: 1.273 | Acc: 79.739% (7285/9136)\n",
      "571 625 Loss: 1.272 | Acc: 79.731% (7297/9152)\n",
      "572 625 Loss: 1.270 | Acc: 79.756% (7312/9168)\n",
      "573 625 Loss: 1.269 | Acc: 79.780% (7327/9184)\n",
      "574 625 Loss: 1.269 | Acc: 79.772% (7339/9200)\n",
      "575 625 Loss: 1.267 | Acc: 79.796% (7354/9216)\n",
      "576 625 Loss: 1.271 | Acc: 79.723% (7360/9232)\n",
      "577 625 Loss: 1.270 | Acc: 79.725% (7373/9248)\n",
      "578 625 Loss: 1.271 | Acc: 79.717% (7385/9264)\n",
      "579 625 Loss: 1.270 | Acc: 79.709% (7397/9280)\n",
      "580 625 Loss: 1.271 | Acc: 79.722% (7411/9296)\n",
      "581 625 Loss: 1.271 | Acc: 79.714% (7423/9312)\n",
      "582 625 Loss: 1.271 | Acc: 79.728% (7437/9328)\n",
      "583 625 Loss: 1.270 | Acc: 79.730% (7450/9344)\n",
      "584 625 Loss: 1.271 | Acc: 79.701% (7460/9360)\n",
      "585 625 Loss: 1.273 | Acc: 79.693% (7472/9376)\n",
      "586 625 Loss: 1.273 | Acc: 79.685% (7484/9392)\n",
      "587 625 Loss: 1.272 | Acc: 79.698% (7498/9408)\n",
      "588 625 Loss: 1.271 | Acc: 79.722% (7513/9424)\n",
      "589 625 Loss: 1.271 | Acc: 79.703% (7524/9440)\n",
      "590 625 Loss: 1.272 | Acc: 79.695% (7536/9456)\n",
      "591 625 Loss: 1.271 | Acc: 79.688% (7548/9472)\n",
      "592 625 Loss: 1.270 | Acc: 79.711% (7563/9488)\n",
      "593 625 Loss: 1.271 | Acc: 79.682% (7573/9504)\n",
      "594 625 Loss: 1.272 | Acc: 79.664% (7584/9520)\n",
      "595 625 Loss: 1.271 | Acc: 79.667% (7597/9536)\n",
      "596 625 Loss: 1.272 | Acc: 79.659% (7609/9552)\n",
      "597 625 Loss: 1.272 | Acc: 79.651% (7621/9568)\n",
      "598 625 Loss: 1.271 | Acc: 79.654% (7634/9584)\n",
      "599 625 Loss: 1.270 | Acc: 79.656% (7647/9600)\n",
      "600 625 Loss: 1.269 | Acc: 79.669% (7661/9616)\n",
      "601 625 Loss: 1.269 | Acc: 79.682% (7675/9632)\n",
      "602 625 Loss: 1.268 | Acc: 79.685% (7688/9648)\n",
      "603 625 Loss: 1.268 | Acc: 79.667% (7699/9664)\n",
      "604 625 Loss: 1.268 | Acc: 79.659% (7711/9680)\n",
      "605 625 Loss: 1.267 | Acc: 79.662% (7724/9696)\n",
      "606 625 Loss: 1.268 | Acc: 79.664% (7737/9712)\n",
      "607 625 Loss: 1.266 | Acc: 79.677% (7751/9728)\n",
      "608 625 Loss: 1.267 | Acc: 79.659% (7762/9744)\n",
      "609 625 Loss: 1.268 | Acc: 79.641% (7773/9760)\n",
      "610 625 Loss: 1.267 | Acc: 79.644% (7786/9776)\n",
      "611 625 Loss: 1.267 | Acc: 79.647% (7799/9792)\n",
      "612 625 Loss: 1.267 | Acc: 79.639% (7811/9808)\n",
      "613 625 Loss: 1.270 | Acc: 79.601% (7820/9824)\n",
      "614 625 Loss: 1.270 | Acc: 79.604% (7833/9840)\n",
      "615 625 Loss: 1.272 | Acc: 79.606% (7846/9856)\n",
      "616 625 Loss: 1.272 | Acc: 79.609% (7859/9872)\n",
      "617 625 Loss: 1.272 | Acc: 79.591% (7870/9888)\n",
      "618 625 Loss: 1.274 | Acc: 79.584% (7882/9904)\n",
      "619 625 Loss: 1.274 | Acc: 79.597% (7896/9920)\n",
      "620 625 Loss: 1.274 | Acc: 79.599% (7909/9936)\n",
      "621 625 Loss: 1.273 | Acc: 79.602% (7922/9952)\n",
      "622 625 Loss: 1.274 | Acc: 79.585% (7933/9968)\n",
      "623 625 Loss: 1.274 | Acc: 79.567% (7944/9984)\n",
      "624 625 Loss: 1.274 | Acc: 79.580% (7958/10000)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Data loading started...\")\n",
    "bs = 16\n",
    "num_bits=8\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../../formal_pruning/dataset', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='../../../formal_pruning/dataset', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False)\n",
    "num_classes = 10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#pretrained_modelqat = \"./cifar_vgg_sym_v3.pt\"\n",
    "pretrained_modelqat = \"./cifar_qat.pt\"\n",
    "netqat = VGG('VGG11')\n",
    "sdqat = torch.load(pretrained_modelqat, map_location=torch.device('cpu'))\n",
    "netqat.load_state_dict(sdqat['net'])\n",
    "stats = gatherStats(netqat, test_loader)\n",
    "print(stats) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 1\n",
    "act_quant = True \n",
    "test(epoch, test_loader, criterion, netqat, device, stats, act_quant, num_bits=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
