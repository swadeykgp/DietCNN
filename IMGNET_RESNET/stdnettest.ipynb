{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b16ef9d",
   "metadata": {},
   "source": [
    "## Section 1: First we try PyTorch Quantization.\n",
    "### This is the best available option as we will see, in terms of accuracy preservation.\n",
    "### However, the FBGEMM backend is not available of FPGA and other embedded systems. In the next section we will implement Quantization from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc0f407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:17:40.940000Z",
     "start_time": "2022-11-09T03:17:39.841890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside main  forward shape:  torch.Size([1, 3, 64, 64])\n",
      "Inside main  forward conv1 shape:  torch.Size([1, 64, 32, 32])\n",
      "Inside main  forward pool shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv2 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer out shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv2 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer out shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside main  forward layer 1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv2 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer out shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv1 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv2 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer out shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside main  forward layer 2 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv1 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv2 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer out shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv1 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv2 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer out shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside main  forward layer 3 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv1 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv2 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer out shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer conv1 shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer conv2 shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer out shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside main  forward layer 4 shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside main  forward avgpool shape:  torch.Size([1, 512, 1, 1])\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n",
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import warnings\n",
    "from resnet_18 import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff39655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:17:54.779465Z",
     "start_time": "2022-11-09T03:17:54.217405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and display samples from Tiny ImageNet  dataset\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "directory = \"../../dataset/tiny-imagenet-200/\"\n",
    "num_classes = 200\n",
    "# the magic normalization parameters come from the example\n",
    "transform_mean = np.array([ 0.485, 0.456, 0.406 ])\n",
    "transform_std = np.array([ 0.229, 0.224, 0.225 ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.Resize(74),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
    "])\n",
    "\n",
    "\n",
    "##### Related to trainset , need only for label ids ##############\n",
    "traindir = os.path.join(directory, \"train\")\n",
    "bs = 1\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
    "])\n",
    "train = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=bs, shuffle=True)\n",
    "assert num_classes == len(train_loader.dataset.classes)\n",
    "small_labels = {}\n",
    "with open(os.path.join(directory, \"words.txt\"), \"r\") as dictionary_file:\n",
    "    line = dictionary_file.readline()\n",
    "    while line:\n",
    "        label_id, label = line.strip().split(\"\\t\")\n",
    "        small_labels[label_id] = label\n",
    "        line = dictionary_file.readline()\n",
    "labels = {}\n",
    "label_ids = {}\n",
    "for label_index, label_id in enumerate(train_loader.dataset.classes):\n",
    "    label = small_labels[label_id]\n",
    "    labels[label_index] = label\n",
    "    label_ids[label_id] = label_index\n",
    "############# All these just to get the label ids ############################\n",
    "\n",
    "valdir = os.path.join(directory, \"val\")\n",
    "\n",
    "val = datasets.ImageFolder(valdir, val_transform)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=bs, shuffle=True)\n",
    "\n",
    "small_labels = {}\n",
    "with open(os.path.join(directory, \"words.txt\"), \"r\") as dictionary_file:\n",
    "    line = dictionary_file.readline()\n",
    "    while line:\n",
    "        label_id, label = line.strip().split(\"\\t\")\n",
    "        small_labels[label_id] = label\n",
    "        line = dictionary_file.readline()\n",
    "\n",
    "\n",
    "val_label_map = {}\n",
    "with open(os.path.join(directory, \"val/val_annotations.txt\"), \"r\") as val_label_file:\n",
    "    line = val_label_file.readline()\n",
    "    while line:\n",
    "        file_name, label_id, _, _, _, _ = line.strip().split(\"\\t\")\n",
    "        val_label_map[file_name] = label_id\n",
    "        line = val_label_file.readline()\n",
    "\n",
    "\n",
    "for i in range(len(val_loader.dataset.imgs)):\n",
    "    file_path = val_loader.dataset.imgs[i][0]\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "    label_id = val_label_map[file_name]\n",
    "\n",
    "    val_loader.dataset.imgs[i] = (file_path, label_ids[label_id])\n",
    "\n",
    "# images, labels = next(iter(val_loader))\n",
    "\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "\n",
    "# figure = plt.figure(figsize = (24,16))\n",
    "# num_of_images = 8\n",
    "# for index in range(1, num_of_images + 1):\n",
    "#     plt.subplot(6, 10, index)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(images[index].permute(1,2,0).numpy().squeeze().astype('uint8'), cmap='summer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb9977f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:17:58.028193Z",
     "start_time": "2022-11-09T03:17:57.887090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet Model - Defined in resnet_18_sym.py \n",
    "pretrained_model = \"./tinyimg_resnet_std.pt\"\n",
    "net = resnet18() \n",
    "sd = torch.load(pretrained_model, map_location=torch.device('cpu'))\n",
    "net.load_state_dict(sd['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae46434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:19:20.910558Z",
     "start_time": "2022-11-09T03:19:20.903133Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model for our help\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffbb0a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:18:02.050301Z",
     "start_time": "2022-11-09T03:18:02.048109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for getting the model size\n",
    "def print_size_of_model(model):\n",
    "    \"\"\" Print the size of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: model whose size needs to be determined\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size of the model(MB):', round(os.path.getsize('temp.p')/(1024*1024),3))\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dad927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T06:39:29.026275Z",
     "start_time": "2022-11-05T06:39:28.977260Z"
    }
   },
   "outputs": [],
   "source": [
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1954d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:18:44.850614Z",
     "start_time": "2022-11-09T03:18:44.839844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main accuracy testing function\n",
    "\n",
    "def test(model, device, test_loader, train_loader, batch_size, quantize=False, fbgemm=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Testing with qauntization if quantize=True\n",
    "    if quantize:\n",
    "#         modules_to_fuse = [['conv1', 'bn1'],\n",
    "#                    ['layer1.0.conv1', 'layer1.0.bn1'],\n",
    "#                    ['layer1.0.conv2', 'layer1.0.bn2'],\n",
    "#                    ['layer1.1.conv1', 'layer1.1.bn1'],\n",
    "#                    ['layer1.1.conv2', 'layer1.1.bn2'],\n",
    "#                    ['layer2.0.conv1', 'layer2.0.bn1'],\n",
    "#                    ['layer2.0.conv2', 'layer2.0.bn2'],\n",
    "#                    ['layer2.0.downsample.0', 'layer2.0.downsample.1'],\n",
    "#                    ['layer2.1.conv1', 'layer2.1.bn1'],\n",
    "#                    ['layer2.1.conv2', 'layer2.1.bn2'],\n",
    "#                    ['layer3.0.conv1', 'layer3.0.bn1'],\n",
    "#                    ['layer3.0.conv2', 'layer3.0.bn2'],\n",
    "#                    ['layer3.0.downsample.0', 'layer3.0.downsample.1'],\n",
    "#                    ['layer3.1.conv1', 'layer3.1.bn1'],\n",
    "#                    ['layer3.1.conv2', 'layer3.1.bn2'],\n",
    "#                    ['layer4.0.conv1', 'layer4.0.bn1'],\n",
    "#                    ['layer4.0.conv2', 'layer4.0.bn2'],\n",
    "#                    ['layer4.0.downsample.0', 'layer4.0.downsample.1'],\n",
    "#                    ['layer4.1.conv1', 'layer4.1.bn1'],\n",
    "#                    ['layer4.1.conv2', 'layer4.1.bn2']]\n",
    "#         model = torch.quantization.fuse_modules(model, modules_to_fuse)\n",
    "        if fbgemm:\n",
    "            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        else:\n",
    "            model.qconfig = torch.quantization.default_qconfig\n",
    "        torch.quantization.prepare(model, inplace=True)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in train_loader:\n",
    "                model.forward(data)\n",
    "            torch.quantization.convert(model, inplace=True)\n",
    "            print(\"======= Quantization Done =====\")\n",
    "\n",
    "\n",
    "    #print(model)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        st = time.time()\n",
    "        for data in test_loader:\n",
    "            X, y = data\n",
    "            #st = time.time()\n",
    "            output = model.forward(X)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                #if True:\n",
    "                    correct += 1\n",
    "            total += batch_size\n",
    "            break\n",
    "        et = time.time()    \n",
    "    acc = round(correct/total, 4)\n",
    "    print(\"========================================= PERFORMANCE =============================================\")\n",
    "    print_size_of_model(model)\n",
    "    print(\"PyTorch optimized model test accuracy :{}% \".format(100*round(correct/total, 4)))\n",
    "    print('Elapsed time = {:0.4f} milliseconds'.format((et - st) * 1000))\n",
    "    print(\"====================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "266e688e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-09T03:18:47.248896Z",
     "start_time": "2022-11-09T03:18:47.180922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside main  forward shape:  torch.Size([1, 3, 64, 64])\n",
      "Inside main  forward conv1 shape:  torch.Size([1, 64, 32, 32])\n",
      "Inside main  forward pool shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv2 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer out shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv2 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer out shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside main  forward layer 1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv1 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer conv2 shape:  torch.Size([1, 64, 16, 16])\n",
      "Inside layer out shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv1 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv2 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer out shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside main  forward layer 2 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv1 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer conv2 shape:  torch.Size([1, 128, 8, 8])\n",
      "Inside layer out shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv1 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv2 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer out shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside main  forward layer 3 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv1 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer conv2 shape:  torch.Size([1, 256, 4, 4])\n",
      "Inside layer out shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer conv1 shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer conv2 shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside layer out shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside main  forward layer 4 shape:  torch.Size([1, 512, 2, 2])\n",
      "Inside main  forward avgpool shape:  torch.Size([1, 512, 1, 1])\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 43.099\n",
      "PyTorch optimized model test accuracy :100.0% \n",
      "Elapsed time = 15.9674 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Baseline performance - unquantized model\n",
    "device = 'cpu'\n",
    "test(model=net, device=device, test_loader=val_loader, train_loader=train_loader, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f44ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T12:35:33.485971Z",
     "start_time": "2022-11-03T12:30:02.395639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quantization Performance\n",
    "# Load the model to be quantized with Pytorch Quantization - Unfortunately this will fail\n",
    "device = 'cpu'\n",
    "import copy\n",
    "netq = copy.deepcopy(net)\n",
    "test(model=netq, device=device, test_loader=val_loader, train_loader=train_loader, batch_size=bs, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941af6a5",
   "metadata": {},
   "source": [
    "### Huh! PyTorch Quantization fails for ResNet - in the QuantizedCPU implementations.\n",
    "### Even if it worked, it would not help for FPGA implementation. The FBGEMM backend is not available of FPGA and other embedded systems. In the next section we will implement Quantization from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec915e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T12:46:46.056213Z",
     "start_time": "2022-11-03T12:46:46.036192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Routines for post training quantization - with calibration from scratch    \n",
    "    \n",
    "# Simple implementation for FPGA\n",
    "# Routines for Quantization    \n",
    "\n",
    "# Routines for Quantization \n",
    "\n",
    "from collections import namedtuple\n",
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "nb = 8\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=nb):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale\n",
    "  \n",
    "  zero_point = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point = qmax\n",
    "  else:\n",
    "      zero_point = initial_zero_point\n",
    "\n",
    "  zero_point = int(zero_point)\n",
    "\n",
    "  return scale, zero_point\n",
    "\n",
    "def quantize_tensor(x, num_bits=nb, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    q_x = q_x.round().byte()\n",
    "    \n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=nb):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale_next = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale_next\n",
    "  \n",
    "  zero_point_next = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point_next = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point_next = qmax\n",
    "  else:\n",
    "      zero_point_next = initial_zero_point\n",
    "\n",
    "  zero_point_next = int(zero_point_next)\n",
    "\n",
    "  return scale_next, zero_point_next\n",
    "  \n",
    "def quantizeLayer(x, layer, stat, scale_x, zp_x, num_bits=nb):\n",
    "  # for both conv and linear layers\n",
    "  W = layer.weight.data\n",
    "  #B = layer.bias.data\n",
    "\n",
    "  # scale_x = x.scale\n",
    "  # zp_x = x.zero_point\n",
    "  w = quantize_tensor(layer.weight.data,num_bits) \n",
    "  #b = quantize_tensor(layer.bias.data,num_bits)\n",
    "\n",
    "  layer.weight.data = w.tensor.float()\n",
    "  #layer.bias.data = b.tensor.float()\n",
    "\n",
    "  ####################################################################\n",
    "  # This is Quantisation !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "  scale_w = w.scale\n",
    "  zp_w = w.zero_point\n",
    "  \n",
    "  #scale_b = b.scale\n",
    "  #zp_b = b.zero_point\n",
    "  \n",
    "\n",
    "  scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
    "\n",
    "  # Perparing input by shifting\n",
    "  X = x.float() - zp_x\n",
    "  layer.weight.data = (scale_x * scale_w/scale_next)*(layer.weight.data - zp_w)\n",
    "  #layer.bias.data = (scale_b/scale_next)*(layer.bias.data + zp_b)\n",
    "\n",
    "  # All int\n",
    "\n",
    "  x = (layer(X)/ scale_next) + zero_point_next \n",
    "    \n",
    "  # Reset\n",
    "  layer.weight.data = W\n",
    "  #layer.bias.data = B\n",
    "  \n",
    "  return x, scale_next, zero_point_next\n",
    "\n",
    "\n",
    "def quantForward(model, x, stats):\n",
    "    \n",
    "  # Quantise before inputting into incoming layers\n",
    "  x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'])\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.conv1, stats['l10c1'], x.scale, x.zero_point)\n",
    "  x = model.bn1(x)\n",
    "  x = model.relu(x)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].conv1, stats['l10c2'], scale_next, zero_point_next)\n",
    "  x = model.layer1[0].bn1(x)\n",
    "  x = model.layer1[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].conv2, stats['l11c1'], scale_next, zero_point_next)\n",
    "  x = model.layer1[0].bn2(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].conv1, stats['l11c2'], scale_next, zero_point_next)\n",
    "  x = model.layer1[1].bn1(x)\n",
    "  x = model.layer1[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].conv2, stats['l20c1'], scale_next, zero_point_next)\n",
    "  x = model.layer1[1].bn2(x)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].conv1, stats['l20c2'], scale_next, zero_point_next)\n",
    "  x = model.layer2[0].bn1(x)\n",
    "  x = model.layer2[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].conv2, stats['l21c1'], scale_next, zero_point_next)\n",
    "  x = model.layer2[0].bn2(x)\n",
    "  #x = model.layer2[0].downsample[0](x)\n",
    "  #x = model.layer2[0].downsample[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].conv1, stats['l21c2'], scale_next, zero_point_next)\n",
    "  x = model.layer2[1].bn1(x)\n",
    "  x = model.layer2[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].conv2, stats['l30c1'], scale_next, zero_point_next)\n",
    "  x = model.layer2[1].bn2(x)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].conv1, stats['l30c2'], scale_next, zero_point_next)\n",
    "  x = model.layer3[0].bn1(x)\n",
    "  x = model.layer3[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].conv2, stats['l31c1'], scale_next, zero_point_next)\n",
    "  x = model.layer3[0].bn2(x)\n",
    "  #x = model.layer3[0].downsample[0](x)\n",
    "  #x = model.layer3[0].downsample[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].conv1, stats['l31c2'], scale_next, zero_point_next)\n",
    "  x = model.layer3[1].bn1(x)\n",
    "  x = model.layer3[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].conv2, stats['l40c1'], scale_next, zero_point_next)\n",
    "  x = model.layer3[1].bn2(x)  \n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[0].conv1, stats['l40c2'], scale_next, zero_point_next)\n",
    "  x = model.layer4[0].bn1(x)\n",
    "  x = model.layer4[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[0].conv2, stats['l41c1'], scale_next, zero_point_next)\n",
    "  x = model.layer4[0].bn2(x)\n",
    "  #x = model.layer4[0].downsample[0](x)\n",
    "  #x = model.layer4[0].downsample[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[1].conv1, stats['l41c2'], scale_next, zero_point_next)\n",
    "  x = model.layer4[1].bn1(x)\n",
    "  x = model.layer4[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[1].conv2, stats['fc'], scale_next, zero_point_next)\n",
    "  x = model.layer4[1].bn2(x)  \n",
    "\n",
    "  x = x.view(-1, 512)   \n",
    "  \n",
    "  # Back to dequant for final layer\n",
    "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "   \n",
    "  x = model.fc(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "# Get Min and max of x tensor, and stores it\n",
    "def updateStats(x, stats, key):\n",
    "  max_val, _ = torch.max(x, dim=1)\n",
    "  min_val, _ = torch.min(x, dim=1)\n",
    "  \n",
    "  \n",
    "  if key not in stats:\n",
    "    stats[key] = {\"max\": max_val.sum(), \"min\": min_val.sum(), \"total\": 1}\n",
    "  else:\n",
    "    stats[key]['max'] += max_val.sum().item()\n",
    "    stats[key]['min'] += min_val.sum().item()\n",
    "    stats[key]['total'] += 1\n",
    "  \n",
    "  return stats\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "  x = model.conv1(x)\n",
    "  x = model.bn1(x)\n",
    "  x = model.relu(x)\n",
    "\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l10c1')\n",
    "  x = model.layer1[0].conv1(x)\n",
    "  x = model.layer1[0].bn1(x)\n",
    "  x = model.layer1[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l10c2')\n",
    "  x = model.layer1[0].conv2(x)\n",
    "  x = model.layer1[0].bn2(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l11c1')\n",
    "  x = model.layer1[1].conv1(x)\n",
    "  x = model.layer1[1].bn1(x)\n",
    "  x = model.layer1[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l11c2')\n",
    "  x = model.layer1[1].conv2(x)\n",
    "  x = model.layer1[1].bn2(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l20c1')\n",
    "  x = model.layer2[0].conv1(x)\n",
    "  x = model.layer2[0].bn1(x)\n",
    "  x = model.layer2[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l20c2')\n",
    "  x = model.layer2[0].conv2(x)\n",
    "  x = model.layer2[0].bn2(x)\n",
    "  #x = model.layer2[0].downsample[0](x)\n",
    "  #x = model.layer2[0].downsample[1](x)  \n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l21c1')\n",
    "  x = model.layer2[1].conv1(x)  \n",
    "  x = model.layer2[1].bn1(x)\n",
    "  x = model.layer2[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l21c2')\n",
    "  x = model.layer2[1].conv2(x)  \n",
    "  x = model.layer2[1].bn2(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l30c1')\n",
    "  x = model.layer3[0].conv1(x)\n",
    "  x = model.layer3[0].bn1(x)\n",
    "  x = model.layer3[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l30c2')\n",
    "  x = model.layer3[0].conv2(x)\n",
    "  x = model.layer3[0].bn2(x)\n",
    "  #x = model.layer3[0].downsample[0](x)\n",
    "  #x = model.layer3[0].downsample[1](x) \n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l31c1')\n",
    "  x = model.layer3[1].conv1(x)\n",
    "  x = model.layer3[1].bn1(x)\n",
    "  x = model.layer3[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l31c2')\n",
    "  x = model.layer3[1].conv2(x)   \n",
    "  x = model.layer3[1].bn2(x) \n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l40c1')\n",
    "  x = model.layer4[0].conv1(x)\n",
    "  x = model.layer4[0].bn1(x)\n",
    "  x = model.layer4[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l40c2')\n",
    "  x = model.layer4[0].conv2(x)\n",
    "  x = model.layer4[0].bn2(x)\n",
    "  #x = model.layer4[0].downsample[0](x)\n",
    "  #x = model.layer4[0].downsample[1](x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l41c1')\n",
    "  x = model.layer4[1].conv1(x)  \n",
    "  x = model.layer4[1].bn1(x)\n",
    "  x = model.layer4[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l41c2')\n",
    "  x = model.layer4[1].conv2(x) \n",
    "  x = model.layer4[1].bn2(x) \n",
    "  x = x.view(-1, 512) \n",
    "  \n",
    "  stats = updateStats(x, stats, 'fc')\n",
    "\n",
    "  x = model.fc(x)\n",
    "\n",
    "  return stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    stats = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            stats = gatherActivationStats(model, data, stats)\n",
    "    \n",
    "    final_stats = {}\n",
    "    for key, value in stats.items():\n",
    "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"] }\n",
    "    return final_stats\n",
    "\n",
    "# Routines for performance testing\n",
    "\n",
    "def test(model, device, test_loader, train_loader, batch_size, quantize=False, fbgemm=False, stats=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    #print(model)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        st = time.time()\n",
    "        for data in test_loader:\n",
    "            X, y = data\n",
    "            #st = time.time()\n",
    "            # Testing with qauntization if quantize=True\n",
    "            if quantize:\n",
    "                output = quantForward(model, X, stats)\n",
    "            else:    \n",
    "                output = model.forward(X)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                #if True:\n",
    "                    correct += 1\n",
    "            total += batch_size\n",
    "        et = time.time()    \n",
    "    acc = round(correct/total, 4)\n",
    "    print(\"========================================= PERFORMANCE =============================================\")\n",
    "    print_size_of_model(model)\n",
    "    print(\"PyTorch optimized model test accuracy :{}% \".format(100*round(correct/total, 2)))\n",
    "    print('Elapsed time = {:0.4f} milliseconds'.format((et - st) * 1000))\n",
    "    print(\"====================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8e392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T12:55:21.354439Z",
     "start_time": "2022-11-03T12:50:22.222178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quantized model performance\n",
    "import copy\n",
    "netqq = copy.deepcopy(net)\n",
    "\n",
    "# one time stats gathering - we will keep this stored for CIFAR for the FPGA implementation\n",
    "stats = gatherStats(netqq, train_loader)\n",
    "#stats = gatherStats(netqq, val_loader)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002c560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T13:03:49.677891Z",
     "start_time": "2022-11-03T13:03:05.944853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quantized Inference\n",
    "test(model=netqq, device=device, test_loader=val_loader, train_loader=train_loader, batch_size=bs, quantize=True, stats=stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6566e8",
   "metadata": {},
   "source": [
    "### What 1% !!\n",
    "### We must do a QAT to mitigate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f1dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T06:56:05.306764Z",
     "start_time": "2022-11-05T06:56:05.242961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some more definitions required for the inference on the QAT model that we have trained offline\n",
    "from collections import namedtuple\n",
    "\n",
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "\n",
    "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
    "  # Calc Scale and zero point of next \n",
    "  qmin = 0.\n",
    "  qmax = 2.**num_bits - 1.\n",
    "\n",
    "  scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale\n",
    "  \n",
    "  zero_point = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point = qmax\n",
    "  else:\n",
    "      zero_point = initial_zero_point\n",
    "\n",
    "  zero_point = int(zero_point)\n",
    "\n",
    "  return scale, zero_point\n",
    "\n",
    "def calcScaleZeroPointSym(min_val, max_val,num_bits=8):\n",
    "  \n",
    "  # Calc Scale \n",
    "  max_val = max(abs(min_val), abs(max_val))\n",
    "  qmin = 0.\n",
    "  qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "  scale = max_val / qmax\n",
    "\n",
    "  return scale, 0\n",
    "\n",
    "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    q_x = q_x.round().byte()\n",
    "    \n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "\n",
    "def quantize_tensor_sym(x, num_bits=8, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    max_val = max(abs(min_val), abs(max_val))\n",
    "    qmin = 0.\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "    scale = max_val / qmax   \n",
    "\n",
    "    q_x = x/scale\n",
    "\n",
    "    q_x.clamp_(-qmax, qmax).round_()\n",
    "    q_x = q_x.round()\n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=0)\n",
    "\n",
    "def dequantize_tensor_sym(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float())\n",
    "\n",
    "def quantizeLayer(x, layer, stat, scale_x, zp_x, vis=False, axs=None, X=None, y=None, sym=False, num_bits=8):\n",
    "  # for both conv and linear layers\n",
    "\n",
    "  # cache old values\n",
    "  W = layer.weight.data\n",
    "  B = layer.bias.data\n",
    "\n",
    "  # WEIGHTS SIMULATED QUANTISED\n",
    "\n",
    "  # quantise weights, activations are already quantised\n",
    "  if sym:\n",
    "    w = quantize_tensor_sym(layer.weight.data,num_bits=num_bits) \n",
    "    b = quantize_tensor_sym(layer.bias.data,num_bits=num_bits)\n",
    "  else:\n",
    "    w = quantize_tensor(layer.weight.data, num_bits=num_bits) \n",
    "    b = quantize_tensor(layer.bias.data, num_bits=num_bits)\n",
    "\n",
    "  layer.weight.data = w.tensor.float()\n",
    "  layer.bias.data = b.tensor.float()\n",
    "\n",
    "  ## END WEIGHTS QUANTISED SIMULATION\n",
    "\n",
    "\n",
    "  if vis:\n",
    "    axs[X,y].set_xlabel(\"Visualising weights of layer: \")\n",
    "    visualise(layer.weight.data, axs[X,y])\n",
    "\n",
    "  # QUANTISED OP, USES SCALE AND ZERO POINT TO DO LAYER FORWARD PASS. (How does backprop change here ?)\n",
    "  # This is Quantisation Arithmetic\n",
    "  scale_w = w.scale\n",
    "  zp_w = w.zero_point\n",
    "  scale_b = b.scale\n",
    "  zp_b = b.zero_point\n",
    "  \n",
    "  if sym:\n",
    "    scale_next, zero_point_next = calcScaleZeroPointSym(min_val=stat['min'], max_val=stat['max'])\n",
    "  else:\n",
    "    scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
    "\n",
    "  # Preparing input by saturating range to num_bits range.\n",
    "  if sym:\n",
    "    X = x.float()\n",
    "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data)\n",
    "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data)\n",
    "  else:\n",
    "    X = x.float() - zp_x\n",
    "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data - zp_w)\n",
    "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data + zp_b)\n",
    "\n",
    "  # All int computation\n",
    "  if sym:  \n",
    "    x = (layer(X)) \n",
    "  else:\n",
    "    x = (layer(X)) + zero_point_next \n",
    "  \n",
    "  # cast to int\n",
    "  x.round_()\n",
    "\n",
    "  # Perform relu too\n",
    "  x = F.relu(x)\n",
    "\n",
    "  # Reset weights for next forward pass\n",
    "  layer.weight.data = W\n",
    "  layer.bias.data = B\n",
    "  \n",
    "  return x, scale_next, zero_point_next\n",
    "\n",
    "# Get Min and max of x tensor, and stores it\n",
    "def updateStats(x, stats, key):\n",
    "  max_val, _ = torch.max(x, dim=1)\n",
    "  min_val, _ = torch.min(x, dim=1)\n",
    "\n",
    "  # add ema calculation\n",
    "\n",
    "  if key not in stats:\n",
    "    stats[key] = {'max': max_val.sum(), 'min': min_val.sum(), 'total': 1}\n",
    "  else:\n",
    "    stats[key]['max'] += max_val.sum().item()\n",
    "    stats[key]['min'] += min_val.sum().item()\n",
    "    if 'total' in stats[key]:\n",
    "        stats[key]['total'] += 1\n",
    "    else:\n",
    "        stats[key]['total'] = 1\n",
    "  \n",
    "  weighting = 2.0 / (stats[key]['total']) + 1\n",
    "\n",
    "  if 'ema_min' in stats[key]:\n",
    "    stats[key]['ema_min'] = weighting*(min_val.mean().item()) + (1- weighting) * stats[key]['ema_min']\n",
    "  else:\n",
    "    stats[key]['ema_min'] = weighting*(min_val.mean().item())\n",
    "\n",
    "  if 'ema_max' in stats[key]:\n",
    "    stats[key]['ema_max'] = weighting*(max_val.mean().item()) + (1- weighting) * stats[key]['ema_max']\n",
    "  else: \n",
    "    stats[key]['ema_max'] = weighting*(max_val.mean().item())\n",
    "\n",
    "  stats[key]['min_val'] = stats[key]['min']/ stats[key]['total']\n",
    "  stats[key]['max_val'] = stats[key]['max']/ stats[key]['total']\n",
    "  \n",
    "  return stats\n",
    "\n",
    "# Reworked Forward Pass to access activation Stats through updateStats function\n",
    "def gatherActivationStats(model, x, stats):\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "  x = model.conv1(x)\n",
    "  x = model.bn1(x)\n",
    "  x = model.relu(x)\n",
    "\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l10c1')\n",
    "  x = model.layer1[0].conv1(x)\n",
    "  x = model.layer1[0].bn1(x)\n",
    "  x = model.layer1[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l10c2')\n",
    "  x = model.layer1[0].conv2(x)\n",
    "  x = model.layer1[0].bn2(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l11c1')\n",
    "  x = model.layer1[1].conv1(x)\n",
    "  x = model.layer1[1].bn1(x)\n",
    "  x = model.layer1[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l11c2')\n",
    "  x = model.layer1[1].conv2(x)\n",
    "  x = model.layer1[1].bn2(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l20c1')\n",
    "  x = model.layer2[0].conv1(x)\n",
    "  x = model.layer2[0].bn1(x)\n",
    "  x = model.layer2[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l20c2')\n",
    "  x = model.layer2[0].conv2(x)\n",
    "  x = model.layer2[0].bn2(x)\n",
    "  #x = model.layer2[0].downsample[0](x)\n",
    "  #x = model.layer2[0].downsample[1](x)  \n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l21c1')\n",
    "  x = model.layer2[1].conv1(x)  \n",
    "  x = model.layer2[1].bn1(x)\n",
    "  x = model.layer2[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l21c2')\n",
    "  x = model.layer2[1].conv2(x)  \n",
    "  x = model.layer2[1].bn2(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l30c1')\n",
    "  x = model.layer3[0].conv1(x)\n",
    "  x = model.layer3[0].bn1(x)\n",
    "  x = model.layer3[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l30c2')\n",
    "  x = model.layer3[0].conv2(x)\n",
    "  x = model.layer3[0].bn2(x)\n",
    "  #x = model.layer3[0].downsample[0](x)\n",
    "  #x = model.layer3[0].downsample[1](x) \n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l31c1')\n",
    "  x = model.layer3[1].conv1(x)\n",
    "  x = model.layer3[1].bn1(x)\n",
    "  x = model.layer3[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l31c2')\n",
    "  x = model.layer3[1].conv2(x)   \n",
    "  x = model.layer3[1].bn2(x) \n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l40c1')\n",
    "  x = model.layer4[0].conv1(x)\n",
    "  x = model.layer4[0].bn1(x)\n",
    "  x = model.layer4[0].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l40c2')\n",
    "  x = model.layer4[0].conv2(x)\n",
    "  x = model.layer4[0].bn2(x)\n",
    "  #x = model.layer4[0].downsample[0](x)\n",
    "  #x = model.layer4[0].downsample[1](x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l41c1')\n",
    "  x = model.layer4[1].conv1(x)  \n",
    "  x = model.layer4[1].bn1(x)\n",
    "  x = model.layer4[1].relu(x)\n",
    "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l41c2')\n",
    "  x = model.layer4[1].conv2(x) \n",
    "  x = model.layer4[1].bn2(x) \n",
    "  x = x.view(-1, 512) \n",
    "  \n",
    "  stats = updateStats(x, stats, 'fc')\n",
    "\n",
    "  x = model.fc(x)\n",
    "\n",
    "\n",
    "  return stats\n",
    "\n",
    "# Entry function to get stats of all functions.\n",
    "def gatherStats(model, test_loader):\n",
    "    device = 'cpu'\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    stats = {}\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            stats = gatherActivationStats(model, data, stats)\n",
    "    \n",
    "    final_stats = {}\n",
    "    for key, value in stats.items():\n",
    "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
    "    return final_stats\n",
    "\n",
    "def quantForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8):\n",
    "  X = 0\n",
    "  y = 0\n",
    "  # Quantise before inputting into incoming layers\n",
    "  if sym:\n",
    "    x = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "  else:\n",
    "    x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.conv1, stats['l10c1'], x.scale, x.zero_point)\n",
    "  x = model.bn1(x)\n",
    "  x = model.relu(x)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].conv1, stats['l10c2'], scale_next, zero_point_next)\n",
    "  x = model.layer1[0].bn1(x)\n",
    "  x = model.layer1[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[0].conv2, stats['l11c1'], scale_next, zero_point_next)\n",
    "  x = model.layer1[0].bn2(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].conv1, stats['l11c2'], scale_next, zero_point_next)\n",
    "  x = model.layer1[1].bn1(x)\n",
    "  x = model.layer1[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer1[1].conv2, stats['l20c1'], scale_next, zero_point_next)\n",
    "  x = model.layer1[1].bn2(x)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].conv1, stats['l20c2'], scale_next, zero_point_next)\n",
    "  x = model.layer2[0].bn1(x)\n",
    "  x = model.layer2[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[0].conv2, stats['l21c1'], scale_next, zero_point_next)\n",
    "  x = model.layer2[0].bn2(x)\n",
    "  #x = model.layer2[0].downsample[0](x)\n",
    "  #x = model.layer2[0].downsample[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].conv1, stats['l21c2'], scale_next, zero_point_next)\n",
    "  x = model.layer2[1].bn1(x)\n",
    "  x = model.layer2[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer2[1].conv2, stats['l30c1'], scale_next, zero_point_next)\n",
    "  x = model.layer2[1].bn2(x)\n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].conv1, stats['l30c2'], scale_next, zero_point_next)\n",
    "  x = model.layer3[0].bn1(x)\n",
    "  x = model.layer3[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[0].conv2, stats['l31c1'], scale_next, zero_point_next)\n",
    "  x = model.layer3[0].bn2(x)\n",
    "  #x = model.layer3[0].downsample[0](x)\n",
    "  #x = model.layer3[0].downsample[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].conv1, stats['l31c2'], scale_next, zero_point_next)\n",
    "  x = model.layer3[1].bn1(x)\n",
    "  x = model.layer3[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer3[1].conv2, stats['l40c1'], scale_next, zero_point_next)\n",
    "  x = model.layer3[1].bn2(x)  \n",
    "\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[0].conv1, stats['l40c2'], scale_next, zero_point_next)\n",
    "  x = model.layer4[0].bn1(x)\n",
    "  x = model.layer4[0].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[0].conv2, stats['l41c1'], scale_next, zero_point_next)\n",
    "  x = model.layer4[0].bn2(x)\n",
    "  #x = model.layer4[0].downsample[0](x)\n",
    "  #x = model.layer4[0].downsample[1](x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[1].conv1, stats['l41c2'], scale_next, zero_point_next)\n",
    "  x = model.layer4[1].bn1(x)\n",
    "  x = model.layer4[1].relu(x)\n",
    "  x, scale_next, zero_point_next = quantizeLayer(x, model.layer4[1].conv2, stats['fc'], scale_next, zero_point_next)\n",
    "  x = model.layer4[1].bn2(x)  \n",
    "\n",
    "  x = x.view(-1, 512)   \n",
    "  \n",
    "  # Back to dequant for final layer\n",
    "  x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
    "   \n",
    "  x = model.fc(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "import torch\n",
    "\n",
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, num_bits=8, min_val=None, max_val=None):\n",
    "        x = quantize_tensor(x,num_bits=num_bits, min_val=min_val, max_val=max_val)\n",
    "        x = dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None\n",
    "\n",
    "def quantAwareTrainingForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8, act_quant=False):\n",
    "\n",
    "  ######## Outer layer #######\n",
    "  conv1weight = model.conv1.weight.data\n",
    "  model.conv1.weight.data = FakeQuantOp.apply(model.conv1.weight.data, num_bits)\n",
    "  x = model.conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['conv1']['ema_min'], stats['conv1']['ema_max'])\n",
    "  x = model.bn1(x)\n",
    "  x = model.relu(x)\n",
    "\n",
    "  ######## layer 1 #######\n",
    "  conv2weight = model.layer1[0].conv1.weight.data\n",
    "  model.layer1[0].conv1.weight.data = FakeQuantOp.apply(model.layer1[0].conv1.weight.data, num_bits)\n",
    "  x = model.layer1[0].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l10c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l10c1']['ema_min'], stats['l10c1']['ema_max'])\n",
    "  x = model.layer1[0].bn1(x)\n",
    "  x = model.layer1[0].relu(x)\n",
    "\n",
    "  conv3weight = model.layer1[0].conv2.weight.data\n",
    "  model.layer1[0].conv2.weight.data = FakeQuantOp.apply(model.layer1[0].conv2.weight.data, num_bits)\n",
    "  x = model.layer1[0].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l10c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l10c2']['ema_min'], stats['l10c2']['ema_max'])\n",
    "  x = model.layer1[0].bn2(x)\n",
    "\n",
    "\n",
    "  conv4weight = model.layer1[1].conv1.weight.data\n",
    "  model.layer1[1].conv1.weight.data = FakeQuantOp.apply(model.layer1[1].conv1.weight.data, num_bits)\n",
    "  x = model.layer1[1].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l11c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l11c1']['ema_min'], stats['l11c1']['ema_max'])\n",
    "  x = model.layer1[1].bn1(x)\n",
    "  x = model.layer1[1].relu(x)\n",
    "\n",
    "  conv5weight = model.layer1[1].conv2.weight.data\n",
    "  model.layer1[1].conv2.weight.data = FakeQuantOp.apply(model.layer1[1].conv2.weight.data, num_bits)\n",
    "  x = model.layer1[1].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l11c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l11c2']['ema_min'], stats['l11c2']['ema_max'])\n",
    "  x = model.layer1[1].bn2(x)\n",
    "\n",
    "  ######## layer 2 #######\n",
    "  conv6weight = model.layer2[0].conv1.weight.data\n",
    "  model.layer2[0].conv1.weight.data = FakeQuantOp.apply(model.layer2[0].conv1.weight.data, num_bits)\n",
    "  x = model.layer2[0].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l20c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l20c1']['ema_min'], stats['l20c1']['ema_max'])\n",
    "  x = model.layer2[0].bn1(x)\n",
    "  x = model.layer2[0].relu(x)\n",
    "\n",
    "  conv7weight = model.layer2[0].conv2.weight.data\n",
    "  model.layer2[0].conv2.weight.data = FakeQuantOp.apply(model.layer2[0].conv2.weight.data, num_bits)\n",
    "  x = model.layer2[0].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l20c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l20c2']['ema_min'], stats['l20c2']['ema_max'])\n",
    "  x = model.layer2[0].bn2(x)\n",
    "\n",
    "  conv8weight = model.layer2[1].conv1.weight.data\n",
    "  model.layer2[1].conv1.weight.data = FakeQuantOp.apply(model.layer2[1].conv1.weight.data, num_bits)\n",
    "  x = model.layer2[1].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l21c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l21c1']['ema_min'], stats['l21c1']['ema_max'])\n",
    "  x = model.layer2[1].bn1(x)\n",
    "  x = model.layer2[1].relu(x)\n",
    "\n",
    "  conv9weight = model.layer2[1].conv2.weight.data\n",
    "  model.layer2[1].conv2.weight.data = FakeQuantOp.apply(model.layer2[1].conv2.weight.data, num_bits)\n",
    "  x = model.layer2[1].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l21c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l21c2']['ema_min'], stats['l21c2']['ema_max'])\n",
    "  x = model.layer2[1].bn2(x)\n",
    "\n",
    "  ######## layer 3 #######\n",
    "  conv10weight = model.layer3[0].conv1.weight.data\n",
    "  model.layer3[0].conv1.weight.data = FakeQuantOp.apply(model.layer3[0].conv1.weight.data, num_bits)\n",
    "  x = model.layer3[0].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l30c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l30c1']['ema_min'], stats['l30c1']['ema_max'])\n",
    "  x = model.layer3[0].bn1(x)\n",
    "  x = model.layer3[0].relu(x)\n",
    "\n",
    "  conv11weight = model.layer3[0].conv2.weight.data\n",
    "  model.layer3[0].conv2.weight.data = FakeQuantOp.apply(model.layer3[0].conv2.weight.data, num_bits)\n",
    "  x = model.layer3[0].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l30c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l30c2']['ema_min'], stats['l30c2']['ema_max'])\n",
    "  x = model.layer3[0].bn2(x)\n",
    "\n",
    "  conv12weight = model.layer3[1].conv1.weight.data\n",
    "  model.layer3[1].conv1.weight.data = FakeQuantOp.apply(model.layer3[1].conv1.weight.data, num_bits)\n",
    "  x = model.layer3[1].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l31c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l31c1']['ema_min'], stats['l31c1']['ema_max'])\n",
    "  x = model.layer3[1].bn1(x)\n",
    "  x = model.layer3[1].relu(x)\n",
    "\n",
    "  conv13weight = model.layer3[1].conv2.weight.data\n",
    "  model.layer3[1].conv2.weight.data = FakeQuantOp.apply(model.layer3[1].conv2.weight.data, num_bits)\n",
    "  x = model.layer3[1].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l31c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l31c2']['ema_min'], stats['l31c2']['ema_max'])\n",
    "  x = model.layer3[1].bn2(x)\n",
    "  ######## layer 4 #######\n",
    "  conv14weight = model.layer4[0].conv1.weight.data\n",
    "  model.layer4[0].conv1.weight.data = FakeQuantOp.apply(model.layer4[0].conv1.weight.data, num_bits)\n",
    "  x = model.layer4[0].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l40c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l40c1']['ema_min'], stats['l40c1']['ema_max'])\n",
    "  x = model.layer4[0].bn1(x)\n",
    "  x = model.layer4[0].relu(x)\n",
    "\n",
    "  conv15weight = model.layer4[0].conv2.weight.data\n",
    "  model.layer4[0].conv2.weight.data = FakeQuantOp.apply(model.layer4[0].conv2.weight.data, num_bits)\n",
    "  x = model.layer4[0].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l40c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l40c2']['ema_min'], stats['l40c2']['ema_max'])\n",
    "  x = model.layer4[0].bn2(x)\n",
    "  conv16weight = model.layer4[1].conv1.weight.data\n",
    "  model.layer4[1].conv1.weight.data = FakeQuantOp.apply(model.layer4[1].conv1.weight.data, num_bits)\n",
    "  x = model.layer4[1].conv1(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l41c1')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l41c1']['ema_min'], stats['l41c1']['ema_max'])\n",
    "  x = model.layer4[1].bn1(x)\n",
    "  x = model.layer4[1].relu(x)\n",
    "\n",
    "  conv17weight = model.layer4[1].conv2.weight.data\n",
    "  model.layer4[1].conv2.weight.data = FakeQuantOp.apply(model.layer4[1].conv2.weight.data, num_bits)\n",
    "  x = model.layer4[1].conv2(x)\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'l41c2')\n",
    "  if act_quant:\n",
    "    x = FakeQuantOp.apply(x, num_bits, stats['l41c2']['ema_min'], stats['l41c2']['ema_max'])\n",
    "  x = model.layer4[1].bn2(x)\n",
    "  ######## layer ends  #######\n",
    "\n",
    "  x = x.view(-1, 512) \n",
    "  x = model.fc(x)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc')\n",
    "\n",
    "\n",
    "  return x, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, conv9weight, conv10weight, conv11weight, conv12weight, conv13weight, conv14weight, conv15weight, conv16weight,  conv17weight, stats\n",
    "# Training\n",
    "# Training\n",
    "def tinytrain(epoch, trainloader, optimizer, criterion, model, device, stats, act_quant=False, num_bits=8):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #outputs = net(inputs)\n",
    "        outputs, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, conv9weight, conv10weight, conv11weight, conv12weight, conv13weight, conv14weight, conv15weight, conv16weight,  conv17weight, stats = quantAwareTrainingForward(model, inputs, stats, num_bits=num_bits, act_quant=act_quant)\n",
    "        model.conv1.weight.data            = conv1weight\n",
    "        model.layer1[0].conv1.weight.data  = conv2weight\n",
    "        model.layer1[0].conv2.weight.data  = conv3weight\n",
    "        model.layer1[1].conv1.weight.data  = conv4weight\n",
    "        model.layer1[1].conv2.weight.data  = conv5weight\n",
    "        model.layer2[0].conv1.weight.data  = conv6weight\n",
    "        model.layer2[0].conv2.weight.data  = conv7weight\n",
    "        model.layer2[1].conv1.weight.data  = conv8weight\n",
    "        model.layer2[1].conv2.weight.data  = conv9weight\n",
    "        model.layer3[0].conv1.weight.data  = conv10weight\n",
    "        model.layer3[0].conv2.weight.data  = conv11weight\n",
    "        model.layer3[1].conv1.weight.data  = conv12weight\n",
    "        model.layer3[1].conv2.weight.data  = conv13weight\n",
    "        model.layer4[0].conv1.weight.data  = conv14weight\n",
    "        model.layer4[0].conv2.weight.data  = conv15weight\n",
    "        model.layer4[1].conv1.weight.data  = conv16weight\n",
    "        model.layer4[1].conv2.weight.data  = conv17weight\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test_qat(epoch, testloader, criterion, model, device, stats, act_quant, num_bits=8):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #outputs = net(inputs)\n",
    "            outputs, conv1weight, conv2weight, conv3weight, conv4weight, conv5weight, conv6weight, conv7weight, conv8weight, conv9weight, conv10weight, conv11weight, conv12weight, conv13weight, conv14weight, conv15weight, conv16weight,  conv17weight, stats = quantAwareTrainingForward(model, inputs, stats, num_bits=num_bits, act_quant=act_quant)\n",
    "            model.conv1.weight.data  = conv1weight\n",
    "            model.layer1[0].conv1.weight.data  = conv2weight\n",
    "            model.layer1[0].conv2.weight.data  = conv3weight\n",
    "            model.layer1[1].conv1.weight.data  = conv4weight\n",
    "            model.layer1[1].conv2.weight.data  = conv5weight\n",
    "            model.layer2[0].conv1.weight.data  = conv6weight\n",
    "            model.layer2[0].conv2.weight.data  = conv7weight\n",
    "            model.layer2[1].conv1.weight.data  = conv8weight\n",
    "            model.layer2[1].conv2.weight.data  = conv9weight\n",
    "            model.layer3[0].conv1.weight.data  = conv10weight\n",
    "            model.layer3[0].conv2.weight.data  = conv11weight\n",
    "            model.layer3[1].conv1.weight.data  = conv12weight\n",
    "            model.layer3[1].conv2.weight.data  = conv13weight\n",
    "            model.layer4[0].conv1.weight.data  = conv14weight\n",
    "            model.layer4[0].conv2.weight.data  = conv15weight\n",
    "            model.layer4[1].conv1.weight.data  = conv16weight\n",
    "            model.layer4[1].conv2.weight.data  = conv17weight\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a41c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we test the QAT trained model's inference accuracy\n",
    "pretrained_modelqat = \"./imgnet_qat.pt\"\n",
    "netqat = resnet18()\n",
    "sdqat = torch.load(pretrained_modelqat, map_location=torch.device('cpu'))\n",
    "netqat.load_state_dict(sdqat['net'])\n",
    "stats = gatherStats(netqat, trainloader)\n",
    "print(stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 1\n",
    "act_quant = True \n",
    "test_qat(epoch, testloader, criterion, netqat, device, stats, act_quant, num_bits=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ac204",
   "metadata": {},
   "source": [
    "### Ah! Now we get a decent performance with both activation and weight quantization. We will take this model to FPGA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ae971",
   "metadata": {},
   "source": [
    "## Section 3: DietCNN Inference - Multiplication Free\n",
    "### The main efficacy of this is in the FPGA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b010d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For training\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import faiss\n",
    "import sys\n",
    "sys.path.insert(1, '../core')\n",
    "from lut_utils_tiny import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from patchlib import *\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from evalutils_resnet_imgnet import *\n",
    "PARALLEL = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test method for the DietCNN Tiny Image Net ResNet Network. \n",
    "\n",
    "HOWDY = 20000000 \n",
    "\n",
    "# Test accuracy of symbolic inference\n",
    "def test_fullsym_acc(model, data_iter, bss=1):\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    for data in data_iter:\n",
    "        X, y = data\n",
    "        if counter > HOWDY:\n",
    "            break\n",
    "        output = model.forward(X)\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "            #if True:\n",
    "                correct += 1\n",
    "        \n",
    "        counter +=bss \n",
    "        total += bss\n",
    "        if(counter > 0 and counter % bss == 0):\n",
    "            print(\"Full symbolic model test accuracy DietCNN :{}% \".format(100*round(correct/total, 4)))\n",
    "    return round(correct/total, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c97ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DietCNN HyperParameters - 3 main \n",
    "# 1. Image  and all activation symbols \n",
    "# 2 & 3. Symbols for CONV and FC layers dictionary \n",
    "\n",
    "index = faiss.read_index(\"./kmeans_alexnet_c1_k1_s1_512_repeat2_v10.index\")\n",
    "\n",
    "n_clusters=512\n",
    "\n",
    "# using a single pixel patch as of now\n",
    "conv_patch_size = (1, 1)\n",
    "patch_size = (1, 1)\n",
    "all_patch_size = (1, 1)\n",
    "\n",
    "patch_stride = 1\n",
    "# Hyperparameters 2 & 3. Symbols for CONV and FC layers dictionary \n",
    "n_cluster_conv_filters = 256\n",
    "n_cluster_fc_filters = 128\n",
    "conv_stride = 1\n",
    "\n",
    "# this is the reverse dictionary - symbol to patch \n",
    "centroid_lut = index.reconstruct_n(0, n_clusters)\n",
    "\n",
    "import pickle\n",
    "# Load the CONV and FC dictionaries and the LUT that are created already\n",
    "with open('imgnet_conv_flt.index', \"rb\") as f:\n",
    "    filter_index_conv = pickle.load(f)\n",
    "with open('imgnet_fc_flt.index', \"rb\") as f:\n",
    "    filter_index_fc = pickle.load(f)\n",
    "fc_lut = np.genfromtxt('./imgnet_fc_lut.txt', delimiter=',',dtype=np.int16)\n",
    "conv_lut = np.genfromtxt('./imgnet_conv_lut.txt', delimiter=',',dtype=np.int16)\n",
    "add_lut = np.genfromtxt('./imgnet_add_lut.txt', delimiter=',',dtype=np.int16)\n",
    "relu_lut = np.genfromtxt('./imgnet_relu_lut.txt', delimiter=',',dtype=np.int16)\n",
    "\n",
    "# this is the creation of symbolic model.\n",
    "# All these steps are need in the desktop implementation to sync with PyTorch inference\n",
    "# For FPGA implementation the DietCNN models are quite simple \n",
    "\n",
    "print(\" Symbolic model loading started...\")\n",
    "t = time.process_time()\n",
    "# Sorry did not get time to change the name from VGG :(\n",
    "netsym = vgg_sym(net,sd, filter_index_conv, filter_index_fc, conv_lut, fc_lut, add_lut, \n",
    "                  relu_lut, n_clusters, index, centroid_lut, patch_size, patch_stride)\n",
    "\n",
    "elapsed_time3 = time.process_time() - t\n",
    "print(\"Symbolic model loading completed in:\",elapsed_time3)\n",
    "netsym.eval()\n",
    "start_t = time.time()  \n",
    "acc = test_fullsym_acc(netsym, val_loader)\n",
    "end = time.time()\n",
    "print(\"elapsed time for symbolic inference:\", end - start_t) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
